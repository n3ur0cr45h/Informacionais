

- Segurança Ofensiva é o processo de invadir sistemas, explorar bugs de softwares e encontrar loopholes em aplicações, para ganhar acessos não-autorizados, 

- Para lidar com um hacker, precisa se comportar como um hacker. 
 - Encontrar vulnerabilidades e recomendar patches antes que um criminoso explore,
 
- Na segurança defensiva, o foco é na proteção da rede da organização e dos sistemas computacionacionais,
 - analisando e protegendo de ameaças digitais, 
 - No papel defensivo, você investiga computadores infectados ou dispositivos, para entender como foi hackeado, rastrear criminosos e monitorar infraestrutura,
 
Penetration Tester - Responsible for testing technology products for finding exploitable security vulnerabilities.
Red Teamer - Plays the role of an adversary, attacking an organization and providing feedback from an enemy's perspective.
Security Engineer - Design, monitor, and maintain security controls, networks, and systems to help prevent cyberattacks.






Some of the tasks that are related to defensive security include:
User cyber security awareness: Training users about cyber security helps protect against various attacks that target their systems.

Documenting and managing assets: We need to know the types of systems and devices that we have to manage and protect properly.

Updating and patching systems: Ensuring that computers, servers, and network devices are correctly updated and patched against any known vulnerability (weakness).

Setting up preventative security devices: firewall and intrusion prevention systems (IPS) are critical components of preventative security. Firewalls control what network traffic can go inside and what can leave the system or network. IPS blocks any network traffic that matches present rules and attack signatures.

Setting up logging and monitoring devices: Without proper logging and monitoring of the network, it won’t be possible to detect malicious activities and intrusions. If a new unauthorized device appears on our network, we should be able to know.

Security Operations Center (SOC)
Threat Intelligence
Digital Forensics and Incident Response (DFIR)
Malware Analysis


A Security Operations Center (SOC) is a team of cyber security professionals that monitors the network and its systems to detect malicious cyber security events. Some of the main areas of interest for a SOC are:

Vulnerabilities: Whenever a system vulnerability (weakness) is discovered, it is essential to fix it by installing a proper update or patch. When a fix is not available, the necessary measures should be taken to prevent an attacker from exploiting it. Although remediating vulnerabilities is of vital interest to a SOC, it is not necessarily assigned to them.

Policy violations: We can think of a security policy as a set of rules required for the protection of the network and systems. For example, it might be a policy violation if users start uploading confidential company data to an online storage service.

Unauthorized activity: Consider the case where a user’s login name and password are stolen, and the attacker uses them to log into the network. A SOC needs to detect such an event and block it as soon as possible before further damage is done.

Network intrusions: No matter how good your security is, there is always a chance for an intrusion. An intrusion can occur when a user clicks on a malicious link or when an attacker exploits a public server. Either way, when an intrusion occurs, we must detect it as soon as possible to prevent further damage.

Threat intelligence aims to gather information to help the company better prepare against potential adversaries. The purpose would be to achieve a threat-informed defense. 
 
Different companies have different adversaries. 

Learning about your adversaries allows you to know their tactics, techniques, and procedures. As a result of threat intelligence, we identify the threat actor (adversary), predict their activity, and consequently, we will be able to mitigate their attacks and prepare a response strategy.

Forensics is the application of science to investigate crimes and establish facts. With the use and spread of digital systems, such as computers and smartphones, a new branch of forensics was born to investigate related crimes: computer forensics, which later evolved into, digital forensics.

In defensive security, the focus of digital forensics shifts to analyzing evidence of an attack and its perpetrators and other areas such as intellectual property theft, cyber espionage, and possession of unauthorized content. Consequently, digital forensics will focus on different areas such as:

File System: Analyzing a digital forensics image (low-level copy) of a system’s storage reveals much information, such as installed programs, created files, partially overwritten files, and deleted files.
System memory: If the attacker is running their malicious program in memory without saving it to the disk, taking a forensic image (low-level copy) of the system memory is the best way to analyze its contents and learn about the attack.
System logs: Each client and server computer maintains different log files about what is happening. Log files provide plenty of information about what happened on a system. Some traces will be left even if the attacker tries to clear their traces.
Network logs: Logs of the network packets that have traversed a network would help answer more questions about whether an attack is occurring and what it entails.


The four major phases of the incident response process are:

Preparation: This requires a team trained and ready to handle incidents. Ideally, various measures are put in place to prevent incidents from happening in the first place.

Detection and Analysis: The team has the necessary resources to detect any incident; moreover, it is essential to further analyze any detected incident to learn about its severity.

Containment, Eradication, and Recovery: Once an incident is detected, it is crucial to stop it from affecting other systems, eliminate it, and recover the affected systems. For instance, when we notice that a system is infected with a computer virus, we would like to stop (contain) the virus from spreading to other systems, clean (eradicate) the virus, and ensure proper system recovery.

Post-Incident Activity: After successful recovery, a report is produced, and the learned lesson is shared to prevent similar future incidents



Security Information and Event Management (SIEM) system.

 MTTD, MTTA, and MTTR - the meantime to detect, acknowledge, and recover
 
 
 

- Aplicação Web é um programa sendo executado em um servidor remoto, 
 - Banco de dados armazena dados de uma forma organizada,
 
- Programa de Bug Bounty permite a empresa oferecer uma recompensa para quem descobrer uma vulnerabilidade na segurança, 


- Insecure Direct Object References (IDOR)
 - IDOR falls under the category of Broken Access Control.
 - Broken access control means that an attacker can access information or perform actions not intended for them
 
 - The Operating System (OS) is the layer sitting between the hardware and the applications and programs you are running. 
 
- When we talk about security, we should think of protecting three things:
 - Confidentiality: You want to ensure that secret and private files and information are only available to intended persons.
 - Integrity: It is crucial that no one can tamper with the files stored on your system or while being transferred on the network.
 - Availability: You want your laptop or smartphone to be available to use anytime you decide to use it.
 
- Authentication can be achieved via three main ways:
 - Something you know, such as a password or a PIN code.
 - Something you are, such as a fingerprint.
 - Something you have, such as a phone number via which you can receive an SMS message.
 
- Quando se usa SSH, e digita a senha, nada é mostrado na tela.





- Network security refers to the devices, technologies, and processes to protect the confidentiality, integrity, and availability of a computer network and the data on it

- Firewall appliance: The firewall allows and blocks connections based on a predefined set of rules. It restricts what can enter and what can leave a network.

- Intrusion Detection System (IDS) appliance: An IDS detects system and network intrusions and intrusion attempts. It tries to detect attackers’ attempts to break into your network.

- Intrusion Prevention System (IPS) appliance: An IPS blocks detected intrusions and intrusion attempts. It aims to prevent attackers from breaking into your network.

- Virtual Private Network (VPN) concentrator appliance: A VPN ensures that the network traffic cannot be read nor altered by a third party. It protects the confidentiality (secrecy) and integrity of the sent data.



- Passos de Ataque:

1. Recon: Recon, short for reconnaissance, refers to the step where the attacker tries to learn as much as possible about the target. Information such as the types of servers, operating system, IP addresses, names of users, and email addresses, can help the attack’s success.

2. Weaponization: This step refers to preparing a file with a malicious component, for example, to provide the attacker with remote access.
Delivery: Delivery means delivering the “weaponized” file to the target via any feasible method, such as email or USB flash memory.

3. Exploitation: When the user opens the malicious file, their system executes the malicious component.

4. Installation: The previous step should install the malware on the target system.

5. Command & Control (C2): The successful installation of the malware provides the attacker with a command and control ability over the target system.

6. Actions on Objectives: After gaining control over one target system, the attacker has achieved their objectives. One example objective is Data Exfiltration (stealing target’s data).


- Nmap - Network Mapper 



- Processo de um Investigador Forense Digital:

1. Acquire the evidence: Collect the digital devices such as laptops, storage devices, and digital cameras. (Note that laptops and computers require special handling if they are turned on; however, this is outside the scope of this room.)

2. Establish a chain of custody: Fill out the related form appropriately (Sample form). The purpose is to ensure that only the authorized investigators had access to the evidence and no one could have tampered with it.

3. Place the evidence in a secure container: You want to ensure that the evidence does not get damaged. In the case of smartphones, you want to ensure that they cannot access the network, so they don’t get wiped remotely.

4. Transport the evidence to your digital forensics lab.


-- No laboratório:

5. Retrieve the digital evidence from the secure container.

6. Create a forensic copy of the evidence: The forensic copy requires advanced software to avoid modifying the original data.

7. Return the digital evidence to the secure container: You will be working on the copy. If you damage the copy, you can always create a new one.

8. Start processing the copy on your forensics workstation.


Proper search authority: Investigators cannot commence without the proper legal authority.

Chain of custody: This is necessary to keep track of who was holding the evidence at any time.

Validation with mathematics: Using a special kind of mathematical function, called a hash function, we can confirm that a file has not been modified.

Use of validated tools: The tools used in digital forensics should be validated to ensure that they work correctly. For example, if you are creating an image of a disk, you want to ensure that the forensic image is identical to the data on the disk.

Repeatability: The findings of digital forensics can be reproduced as long as the proper skills and tools are available.

Reporting: The digital forensics investigation is concluded with a report that shows the evidence related to the case that was discovered.


EXIF - Exchangeable Image File Format 




- Objetivos do SOC: 
 
 Find vulnerabilities on the network: A vulnerability is a weakness that an attacker can exploit to carry out things beyond their permission level. A vulnerability might be discovered in any device’s software (operating system and programs) on the network, such as a server or a computer. For instance, the SOC might discover a set of MS Windows computers that must be patched against a specific published vulnerability. Strictly speaking, vulnerabilities are not necessarily the SOC’s responsibility; however, unfixed vulnerabilities affect the security level of the entire company.

Detect unauthorized activity: Consider the case where an attacker discovered the username and password of one of the employees and used it to log in to the company system. It is crucial to detect this kind of unauthorized activity quickly before it causes any damage. Many clues can help us detect this, such as geographic location.

Discover policy violations: A security policy is a set of rules and procedures created to help protect a company against security threats and ensure compliance. What is considered a violation would vary from one company to another; examples include downloading pirated media files and sending confidential company files insecurely.

Detect intrusions: Intrusions refer to system and network intrusions. One example scenario would be an attacker successfully exploiting our web application. Another example scenario would be a user visiting a malicious site and getting their computer infected.

Support with the incident response: An incident can be an observation, a policy violation, an intrusion attempt, or something more damaging such as a major breach. Responding correctly to a severe incident is not an easy task. The SOC can support the incident response team handle the situation.

- Recursos de Dados que o SOC utiliza:
 1. Logs de Servidores
 2. Atividade de DNS
 3. Logs de Firewall
 4. Logs de DHCP 
 
- Security Information and Event Management (SIEM): Agrega dados de diferentes fontes

Network security monitoring (NSM): This focuses on monitoring the network data and analyzing the traffic to detect signs of intrusions.

Threat hunting: With threat hunting, the SOC assumes an intrusion has already taken place and begins its hunt to see if they can confirm this assumption.

Threat Intelligence: Threat intelligence focuses on learning about potential adversaries and their tactics and techniques to improve the company’s defences. The purpose would be to establish a threat-informed defence.






- MAC
 - The first six characters represent the company that made the network interface, and the last six is a unique number.
  - interesting thing with MAC addresses is that they can be faked or "spoofed" in a process known as spoofing.
  - This spoofing occurs when a networked device pretends to identify as another using its MAC address. 




- Redes

- when we refer to the term "topology", we are actually referring to the design or look of the network at hand. 

- Topologia Estrela: Possui um hub ou switch conectando os dispositivos
 - Qualquer informação enviada para um dispositivo nessa topologia é enviado via dispositivo central (switch ou hub) 


- Topologia Bus
 - Todos os dispositivos são conectados via 1 cabo - chamado de backbone,
 
- Topologia Ring / Token
 - Os dispositivos são conectados um no outro, formando um "loop", 
 - Os dados são passados por entre os computadores, até chegar ao destino  


- Switch
 - dedicated devices within a network that are designed to aggregate multiple other devices such as computers, printers, or any other networking-capable device using ethernet.
 - having ports of 4, 8, 16, 24, 32, and 64 for devices
  - Switches keep track of what device is connected to which port. This way, when they receive a packet, instead of repeating that packet to every port like a hub would do, it just sends it to the intended target, thus reducing network traffic.

- Roteador
 - Routing is the label given to the process of data travelling across networks. 
 - Routing involves creating a path between networks so that this data can be successfully delivered.
 
 
 
- Subnetting
 -  Subnetting is the term given to splitting up a network into smaller, miniature networks within itself.
 
 
- ARP 
 -  ARP protocol or Address Resolution Protocol for short, is the technology that is responsible for allowing devices to identify themselves on a network.  
 - the ARP protocol allows a device to associate its MAC address with an IP address on the network. 
 - Each device on a network will keep a log of the MAC addresses associated with other devices.
 
 
- DHCP (Dynamic Host Configuration Protocol) server
 - Discover - When a device connects to a network it sends out a request (DHCP Discover) to see if any DHCP servers are on the network
 - Offer - The DHCP server then replies back with an IP address the device could use (DHCP Offer)
 - Request - The device then sends a reply confirming it wants the offered IP Address (DHCP Request)
 - Ack - and then lastly, the DHCP server sends a reply acknowledging this has been completed, and the device can start using the IP Address (DHCP ACK).
 
 
 
 
-  OSI model (or Open Systems Interconnection Model)
 - 7 Aplicação
 - 6 Apresentação
 - 5 Sessão
 - 4 Transporte
 - 3 Redes
 - 2 Enlace
 - 1 Físico
  - Quando os pedaços de informação são adicionados, das camadas anteriores, é chamado "encapsulamento"
  
- 7 Aplicação - É a camada onde protocolos e regras determinam a interação com dados recebidos e enviados (DNS, FTP e etc...)

- GUID: Graphical Users Interface 

- 6 Apresentação - Onde os dados começam a ser traduzidos para / de. 
 - Atributos de segurança, como criptografia, ocorrem aqui. 
 
- 5 Sessão - Sincroniza os dois computadores para garantir que possam comunicar os dados de envio e recebimento, 
 - Sessões são únicas - e enquanto a conexão é ativa, a sessão também é, 
 
- 4 Transporte 
 - TCP - Transmission Control Protocol
 - UDP - User Datagram Protocol 
 
- 3 Rede 
 - Onde ocorre o Roteamento e a remontagem de pacotes
 - Inclui o OSPF (open Shortest Path First) e RIP (Routing Information Protocol) 
  - Nessa camada, tudo é lidado com o IP, 
  - Roteadores capaz de enviar pacotes usando IPs se chamam "Dispositivos Layer 3" - pois são capazes de  trabalhar na terceira camada do modelo OSI 
  
- 2 Enlace
 - Quando as informações são enviadas através da rede, o MAC é usado para identificar o destino (endereço físico) 
 - Essa camada recebe o pacote da camada de rede, e adiciona o MAC, 
 
- NIC - Network Interface Card

- 1 Physical 
 - Conexão física - hardware (cabos)
 
 
 
 
 
- Time to Live - Tempo de expiração do pacote, para não gargalar a rede,
- Checksum - Checador de integridade dos protocolos TCP / IP. Se for diferente, indica que o pacote esta corrompido 

- Quando o dado não possui IP, é um "quadro"
- Quando possui IP, é um "pacote"


- TCP / IP consiste de camadas:
 - Aplicação
 - Transporte
 - Internet
 - Interface de Rede

- TCP precisa estabelecer conexão 

- Pacotes TCP possuem 9 seções de cabeçalhos:
 - Source Port - Porta de Origem, sendo aleatório
 - Destination Port - Porta destino, não é aleatório
 - Source IP - IP do dispositivo que envia
 - Destionation IP - IP do dispositivo que recebe
 - Sequence Number - O primeiro dado transmitido possui um número aleatório
 - Acknowledgement Number - O número de sequência depois da primeira transmissão
 - Checksum - Valor de integridade, 
 - Data - Os dados - Byte do arquivo
 - Flag - Determina como o pacote deve ser lidado em ambos os dispositivos, no processo de aperto de mão
 


- Three-Way Handshake 
 - Processo de estabelecimento entre dois dispositivos
 - Flags:
  SYN - Inicia a conexão e sincroniza os dois dispositivos (CLIENTE)
  SYN / Ack - Responde com um "sim" para a sincronização (SERVIDOR)
  ACK - Usado por ambos para indicar que as mensagens / pacotes foram recebidos corretamente (AMBOS)
  DATA - Envio dos dados (AMBOS)
  FIN - Fecha a conexão depois de ter completado 
  RST - Encerra toda a comunicação abruptamente, indica que tem um erro no processo 
  
  
  
- Cabeçalhos UDP
 - TTL - Time to Live - Tempo de expiração do pacote
 - Source Address - IP da Origem
 - Destination Address - IP do Destino
 - Source Port - Porta de Origem (Aleatório)
 - Destination Port - Porta de destino (específico)
 - Data - Dados 
 
 - Quantidade de Portas: 65.535 
 


- Without port forwarding, applications and services such as web servers are only available to devices within the same direct network.
 - port forwarding opens specific ports
 - firewalls determine if traffic can travel across these ports
 
- A firewall is a device within a network responsible for determining what traffic is allowed to enter and exit. 
 - Think of a firewall as border security for a network.
  - Duas categorias primárias de firewall:
   - Stateful - Avalia o comportamento todo da conexão 
   - Stateless - Avalia os pacotes individualmente 
   
- VPN - allows devices on separate networks to communicate securely by creating a dedicated path between each other over the Internet (known as a tunnel)
 - Allows networks in different geographical locations to be connected.
 - Offers privacy.
 - Offers anonyminity.
 
- Tecnologias de VPN: 
 - PPP - This technology allow for authentication and provide encryption of data / This technology is not capable of leaving a network by itself (non-routable)
 
 - PPTP - The Point-to-Point Tunneling Protocol (PPTP) is the technology that allows the data from PPP to travel and leave a network. / weakly encrypted
 - IPSec - Internet Protocol Security (IPsec) encrypts data using the existing Internet Protocol (IP) framework. / boasts strong encryption
 
 
- Roteador
 - Routing is the label given to the process of data travelling across networks
 - Routers operate at Layer 3 of the OSI model
 
- Switch 
 - A switch is a dedicated networking device responsible for providing a means of connecting to multiple devices
 - Switches can operate at both layer 2 and layer 3
  - Camada 2: These switches will forward frames (remember these are no longer packets as the IP protocol has been stripped) onto the connected devices using their MAC address.
  - Camada 3: more sophisticated than layer 2, as they can perform some of the responsibilities of a router. send frames to devices and route packets to other devices using the IP 
  
- VLAN (Virtual Local Area Network) allows specific devices within a network to be virtually split up.


- DNS - Domain Name System
 - Traduz os endereços IP's em nomes de domínio, 
 - Existem 3 categorias:
  - Root: 
  - Top Level: .com, .edu. .br, .gov...
   - Podendo ser ainda: gTLD - Generico (governo, exercito) e ccTLD - Pais (uk. br. usa)
  - Second Level: google, bing, tryhackme
  - Subdominio: tryhackme.admin, google.user   
  
- Tipos de DNS:
 - A - Resolve os endereços IPv4
 - AAAA- Resolve os endereços IPv6
 - CNAME - Resolve os subdominios 
 - MX - Resolve os endereços de servidores de e-mail 
 - TXT - Podem ser usados para veracidade de dominio, listar servidores autenticos também 


- HTTP - HyperText Transfer Protocol 
- HTTPS - Hypertext Transfer Protocol Secure 



- URL - Uniform Resource Locator
 - URL é uma instrução de como acessar um recurso na internet
 
 http://user:password@tryhackme.com:80/view-room?id=1#task3
 
 http - scheme
 user:password - user
 tryhackme.com - Host / Domain
 80 - Porta
 view-room - path
 ?id=1 - query string
 #task3 - Fragmento 
 
 
 
- Requisição:

GET / HTTP/1.1
Host: tryhackme.com
User-Agent: Mozilla/5.0 Firefox/87.0
Referer: https://tryhackme.com/

Linha 1 - Método GET
linha 2 - O site requisitado
Linha 3 - O uso do navegador
Linha 4 - a página referenciada
linha 5 - linha vazia, para indicar que a requisição acabou


- Resposta

HTTP/1.1 200 OK
Server: nginx/1.15.8
Date: Fri, 09 Apr 2021 13:34:03 GMT
Content-Type: text/html
Content-Length: 98

<html>
<head>
    <title>TryHackMe</title>
</head>
<body>
    Welcome To TryHackMe.com
</body>
</html>

Linha 1 - Versão do procotolo 
linha 2 - o software do servidor web e a versão
linha 3 - a data atual, do servidor
linha 4 - O tipo de conteúdo que será enviado
linha 5 - O tamanho da resposta, para confirmar que nada está faltando
linha 6 - linha vazia para confirmar o fim da resposta
linha 7 - 14 - A informação requisitada 



- Métodos HTTP
 - GET - Pegar informações do servidor web 
 - POST - Inserir dados no servidor web
 - PUT - Atualizar informações
 - DELETE - Deletar registros
 
 
- Códigos de Status do HTTP 
 100 - 199 - Informativo
 200 - 299 - Sucesso
 300 - 3999 - Redirecionamento
 400 - 499 - Erros do CLIENTE
 500 - 599 - Erros do Servidor 
 
- Códigos Comuns do HTTP 
 200 - OK
 201 - Recurso Criado
 301 - Redirecionamento o browser do cliente
 302 - Redirecionamento temporário
 400 - A requisição está errada ou faltando algo
 401 - Não autorizado
 403 - Proibido
 405 - Método não permitido
 404 - Página não existe
 500 - Erro do SERVIDOR
 503 - Servidor não consegue lidar com a requisição 
 
 
- Headers / Cabeçalhos
 - Dados adicionais enviados ao servidor quando se está fazendo requisição
 
 host - o host que deseja (pois podem ter múltiplos)
 user-agent - o navegador, para que o servidor formate o o site corretamente para o seu navegador 
 content-lenght - quantidade de dados que se espera 
 accept-encoding - o tipo de compressão que o navegador suporta
 
 cookie - dados enviados ao servidor para ajudar a lembrar sua informação 
 
- Cabeçalhos de Resposta 
 set-cookie - Informação para armanzenar, que envia ao servidor a cada requisição
 cache-control - O tempo para armazenar o conteúdo da resposta no cache do naveador, até solicitar novamente
 content-type - Informa ao cliente o tipo de dado que é retornado 
 content-encoding - Método de compressão usado para comprimir os dados e deixar menor, para enviar através da internet 
 



- Website

 - Front End (Cliente) - A maneira que seu navegador renderiza o site
 - Back End (Servidor) - A maneira que o servidor processa sua requisição e retorna a resposta 
 
- Websites são criados utilizando, primariamente, 3 coisas: 
 - HTML - Para construir os sites e definir as estruturas / Formatar
 - CSS - para estilizar o site, deixando mais bonito
 - JavaScript - Inserindo características complexas de interatividade, como botões e etc. 
 
- HTML - HyperText Markup Language - linguagem em que os sites são construídos 
 - Elemtnos / Tags são os blocos de páginas HTML 
 
- Sensitive Data Exposure - Quando um site não protege os dados sensíveis ao usuário final, 
 - Normalmente esses dados são encontrados no código origem 
 
 
- HTML Injection 
 - Vulnerabilidade que ocorre quando não é sanitizado o que o usuário digita na página (não filtra o texto malicioso) 
 
 
1. Requisita o site usando DNS
2. Conversa com o servidor Web usando os comandos de HTTP
3. O servidor retorna o HTML, JavaScript e CSS, imagens e etc.. 


- Load Balancers / Balanceadores de Carga
 - Direciona o tráfego para servidores disponíveis
  - Robin - Envia o tráfego na ordem de servidor / sequência
  - weighted - Envia o tráfego para o servidor menos ocupado 
   - Tem uma avaliação periódica de performance, para saber se está tudo em ordem com os servidores: health heck 
   
- Content Delivery Networks (CDN)
 - Permite hospedar arquivos estáticos do site em servidores no mundo tudo - isso diminui a quantidade de tráfego 
 
- Web Application Firewall 
 - O WAF fica entre o a requisição do site e o servidor - protegendo o servidor de ataques, 



-  Web Server 
 - É um software que escuta por conexões, utilizando o protocolo HTTP para entregar conteúdo aos clientes
 - Os mais comuns são Apache, Nginx, IIS e NodeJS 
  - O Servidor Web entrega arquivos do diretório root, que é definido nas configurações de software 
  
- Virtual Hosts 
 - Múltiplos websites com diferentes nomes de domínio, no mesmo servidor web 
 
- Conteúdo Estático
 - Normalmente não muda, como imagens, javascript, CSS, e pode ser também HTML 
 
- Conteúdo Dinâmico
 - Conteúdo que pode mudar com diferentes requisições, como num blog, com postagens recentes e etc. 
 

- SSH - Secure Shell 
 - Permite o enviado de comandos para dispositivos remotos
 - É criptografado, através do uso do Shell 
 
- SCP - Secure Copy 

- Crontab - Um dos processos que inicia com o boot, responsável por facilitar e lidar com os jobs de cron 




- Encapsulamento - Quando os dados das camadas superiores são inseridos nas camadas posteriores

https://www.whois.com/whois/ 





- Ferramenta útil de rede: dig  
 - Dig (domínio) @(ip do servidor dns)
 
- Pesquisa de DNS:
 
 Local Cache -> DNS REcursivo -> Root Name Server -> Top Level Domain Servers -> Authoritative Name Servers 
 
 

- Nmap conecta em cada porta do alvo, 

- NSE - Nmap Scripting Engine 
 - Usa a linguagem LUA
 - Locais para pegar scripts:
  - https://nmap.org/nsedoc/
  - Diretório do Nmap: /usr/share/nmap/scripts
  
  
  
- SMB - Server Message Block Protocol 
 - A client-server communication protocol used for sharing access to files, printers, serial ports and other resources on a network.
 - The SMB protocol is known as a response-request protocol
  - it transmits multiple messages between the client and server to establish a connection. Clients connect to servers using TCP/IP (actually NetBIOS over TCP/IP as specified in RFC1001 and RFC1002), NetBEUI or IPX/SPX
  - Once they have established a connection, clients can then send commands (SMBs) to the server that allow them to access shares, open files, read and write files, and generally do all the sort of things that you want to do with a file system. However, in the case of SMB, these things are done over the network.
  
  - Samba, an open source server that supports the SMB protocol, was released for Unix systems.
  
- Enumeration is the process of gathering information on a target in order to find potential attack vectors and aid in exploitation.

- Enum4linux is a tool used to enumerate SMB shares on both Windows and Linux systems.
 - sudo apt install enum4linux





- Telnet 
 - Telnet is an application protocol which allows you, with the use of a telnet client, to connect to and execute commands on a remote machine that's hosting a telnet server.
 
 - Lacks encryption 
 - Poor access Control 
  - A CVE, short for Common Vulnerabilities and Exposures, is a list of publicly disclosed computer security flaws. 
  - When someone refers to a CVE, they usually mean the CVE ID number assigned to a security flaw.
  - A reverse shell is a type of shell in which the target machine communicates back to the attacking machine.
  
  
  
  
 - FTP - File Transfer Protocol 
  - Porta 20 e 21
  - Possible to log in as "anonymous" and without password
  
  
- NFS - Network File System 
 - Share files and directories through network 
  - https://docs.oracle.com/cd/E19683-01/816-4882/6mb2ipq7l/index.html
  - https://www.datto.com/library/what-is-nfs-file-share
  - http://nfs.sourceforge.net/
  - https://wiki.archlinux.org/index.php/NFS
   - For example, WIndows server can act as a n NFS File Server for other non-windows client 
   - Necessary to mount the directory on the server
    - To attack NFS, its necessary to install the package 'NFS-Common' -> https://packages.ubuntu.com/jammy/nfs-common

  - Root_Squash - prevent users from having root access to the NFS volume
   - SUID files - files that can be run with the permissions of the owner / group 
   
- Enumeration is a process which establishes connection to the target, to discovert attack vectors 
 - The information from this stage will inform the possible attacks 
 
 
 


- SMTP - Simple Mail Transfer Protocol  
 - Verifica quem está enviando e-mails através do servidor SMTP
 - Envia e-mails para foram
 - Se o e-mail não pode ser enviado, ele manda uma mensagem de volta ao remetente
  - Existe o SMTP Handshake e o SMTP Queu
  
- POP - Post Office Protocol 
- IMAP - Internet Message Access Protocol 
 - Ambos servem para transferir e-mails entre um cliente e servidor 
 - Com sincronização entre ambos (POP sendo mais simples)
 
https://computer.howstuffworks.com/e-mail-messaging/email3.htm
https://www.afternerd.com/blog/smtp/




- RDBMS - Relational Database Management System 
 - Software ou serviço usado para criar e gerenciar bancos de dados no modelo "Relacional" 
 - "relacional" significa que os dados são armazenados em tabelas 
  - Cada tabela se relaciona de algum jeito com cada chave primária ou outras chaves 
   - SQL é a comunicação entre servidor e cliente 
   
https://dev.mysql.com/doc/dev/mysql-server/latest/PAGE_SQL_EXECUTION.html 

https://www.w3schools.com/php/php_mysql_intro.asp






- Burp Suite
 - Framework escrito em Java, providencia tudo para uma invasão em aplicações web 
 - Pode capturar e manipular todo o tráfego entre o atacante e o webserver 
  - Proxy - Intercepta e modifica requisições / respostas de aplicações web
  - Repeater - Captura, modifica e reenvia a mesma requisição diversas vezes
  - Intruder - Spray um endpoint com requisições 
  - Decoder - Decodifica a informação receptada ou codifica um payload antes de enviar 
  - Comparer - Compara dados 
  - Sequencer - Para aleatoriedades de tokens 
  
- XSS - Cross Site Scripting 
 - Injeção de script do lado cliente, na página web, de maneira que a página executa.
 - Reflected XSS afeta apenas a pessoa fazendo a requisição web 
 


- Broken Access Control 
 - Se um visitante consegue visualizar páginas de acesso, que não deveriam ser vistas, os controles de acesso estão "quebrados"
 
 - IDOR - Insecure Direct Object Reference 
  - Vulnerabilidade que permite acessar recursos que não estariam disponíveis para visualizar. 
  
  
- Cryptographic Failure 
 - Vulnerabilidade ocasionada pelo misuso ou falta de uso de algoritmos criptográficos 
 - Aplicações web utilizam criptografia para providenciam confidencialidade para os usuários 
  - Criptografia de Dados em Trânsito - Dados em movimento 
  - Criptografia de Dados em Descanso - Dados armazenados 
   - Flat-File Database - Consiste em dados de uma tabela só (Excel, SQLite e etc) 


- Injeções 
 - SQL Injections - Inputs inseridos em queries SQL
 - Command Injections - Input inserido em comandos sistêmicos 
  - Formas de Mitigar esse problema: 
   - Lista de Permissão de Usuários 
   - Remover caracteres "perigosos" (que permitem a injeção de comandos) 
  
- Insecure Design 
 - Arquitetura mal feita (Exemplo: não ter quantidade limite de tentativa de logins, e etc) 
 
- Security Misconfiguration
 - Senhas padrões, má configuração de segurança em geral 
 
 
- Identification and Authentication Failures
 - Algumas falhas: 
  - Ataques de força bruta
  - Credenciais fracas
  - Cookies de Sessão Fracos 
   - Algumas mitigações: 
    - Política de senha forte
	- Trava automática depois de certa quantidade de tentativas
	- Autenticação Multi-Fator 
	
- Integrity 
 - Utiliza hash para validar autenticidade dos arquivos 
 - Hash é um número resultante de uma aplicação de algoritmo
 
- Cookies podem ser tokens de sessão gerados pelo site, para uso de requisições de acesso 
- JWT - JSON Web Token 


- Server-Side Request Forgery (SSRF)
  - Finalidades: 
   - Enumerate internal networks, including IP addresses and ports.
   - Abuse trust relationships between servers and gain access to otherwise restricted services.
   - Interact with some non-HTTP services to get remote code execution (RCE).
   
   
   
- SQL Injection 
 - ' or 1=1-- deve ser usado quando não se sabe o usuário ou é inválido 
 
- Horizontal Privilege Escalation
 - Acessar com outro usuário que possuem as mesmas permissões que o atual
 
- Vertical Privilege Escalation
 - Acessar com um usuário que possui permissões maiores
 
- Websites that allow the user to modify the iframe or other DOM elements will most likely be vulnerable to XSS.   
 
- www-data é uma conta de usuário de poucos privilégios 

- RCE via upload pode ser explorado realizando upload de um programa escrito na mesma linguagem que o back-end o website.
 - Ou outra linguagem que o servidor entenda e possa executar 
 (tradicionalmente, seria PHP. Mas, atualmente, pode ser Python, Django e JS / Node.js) 

- There are two basic ways to achieve RCE on a webserver when exploiting a file upload vulnerability: webshells, and reverse/bind shells
 
- Client Side Script: roda no cliente 
 - Mais fácil de explorar 
 
- Server Side: roda no servidor 
 - Mais difícil, por não ter o código na sua tela 


- File extensions are used (in theory) to identify the contents of a file. In practice they are very easy to change, so actually don't mean much
 - They either blacklist extensions (i.e. have a list of extensions which are not allowed) 
 - they whitelist extensions (i.e. have a list of extensions which are allowed, and reject everything else).
 
- File type filtering looks, once again, to verify that the contents of a file are acceptable to upload. 
 - MIME validation: MIME (Multipurpose Internet Mail Extension) types are used as an identifier for files -- originally when transfered as attachments over email, but now also when files are being transferred over HTTP(S).
  - The MIME type for a file upload is attached in the header of the request
  - As MIME is based on the extension of the file, this is extremely easy to bypass.
 
 - Magic Number validation (Esses magic numbers são os números que aparecem nos pacotes, no wireshark)
  - Magic numbers are the more accurate way of determining the contents of a file; although, they are by no means impossible to fake.
  - The "magic number" of a file is a string of bytes at the very beginning of the file content which identify the content
  
 - File Length Filtering:
  - File length filters are used to prevent huge files from being uploaded to the server via an upload form  (as this can potentially starve the server of resources).
  
 - File Name Filtering:
  - files uploaded to a server should be unique. Usually this would mean adding a random aspect to the file name
  - file names should be sanitised on upload to ensure that they don't contain any "bad characters", which could potentially cause problems on the file system when uploaded
  
 - File Content Filtering:
  - More complicated filtering systems may scan the full contents of an uploaded file to ensure that it's not spoofing its extension
  


There are four easy ways to bypass your average client-side file upload filter:

  - Turn off Javascript in your browser -- this will work provided the site doesn't require Javascript in order to provide basic functionality. If turning off Javascript completely will prevent the site from working at all then one of the other methods would be more desirable; otherwise, this can be an effective way of completely bypassing the client-side filter.
  
  - Intercept and modify the incoming page. Using Burpsuite, we can intercept the incoming web page and strip out the Javascript filter before it has a chance to run. The process for this will be covered below.
  
  - Intercept and modify the file upload. Where the previous method works before the webpage is loaded, this method allows the web page to load as normal, but intercepts the file upload after it's already passed (and been accepted by the filter). Again, we will cover the process for using this method in the course of the task.
  
  -Send the file directly to the upload point. Why use the webpage with the filter, when you can send the file directly using a tool like curl? Posting the data directly to the page which contains the code for handling the file upload is another effective method for completely bypassing a client side filter.
   
   



- Plaintext - Data before encryption or hashing, often text but not always as it could be a photograph or other file instead.

- Encoding - This is NOT a form of encryption, just a form of data representation like base64 or hexadecimal. Immediately reversible.

- Hash - A hash is the output of a hash function. Hashing can also be used as a verb, "to hash", meaning to produce the hash value of some data.

- Brute force - Attacking cryptography by trying every different password or every different key

- Cryptanalysis - Attacking cryptography by finding a weakness in the underlying maths
 
 
 - Hashing is used very often in cyber security. When you logged into TryHackMe, that used hashing to verify your password. 
  - When you logged into your computer, that also used hashing to verify your password. You interact indirectly with hashing more than you would think, mostly in the context of passwords. 
  
- A hash collision is when 2 different inputs give the same output. 
 - Due to the pigeonhole effect, collisions are not avoidable.
 - The pigeonhole effect is basically, there are a set number of different output values for the hash function, but you can give it any size input. 
 
- Hashing is used for 2 main purposes in Cyber Security. To verify integrity of data (More on that later), or for verifying passwords.
 - You can't encrypt the passwords, as the key has to be stored somewhere. If someone gets the key, they can just decrypt the passwords.
 - A rainbow table is a lookup table of hashes to plaintexts, so you can quickly find out what password a user had just from the hash.
  - To protect against rainbow tables, we add a salt to the passwords.
  - The salt is randomly generated and stored in the database, unique to each user
   - The salt is added to either the start or the end of the password before it’s hashed, and this means that every user will have a different password hash even if they have the same password.
   
- You can't "decrypt" password hashes. They're not encrypted. You have to crack the hashes by hashing a large number of different inputs

- Graphics cards have thousands of cores. Although they can’t do the same sort of work that a CPU can, they are very good at some of the maths involved in hash functions.
 - This means you can use a graphics card to crack most hash types much more quickly
 
- Hashing can be used to check that files haven't been changed. If you put the same data in, you always get the same data out.

- Dictionay Attack
 - . You can then compare these hashes to the one you're trying to crack, to see if any of them match. If they do, you now know what word corresponds to that hash- you've cracked it
 
- NThash is the hash format that modern Windows Operating System machines will store user and service passwords in
 - It's also commonly referred to as "NTLM" which references the previous version of Windows format for hashing passwords known as "LM", thus "NT/LM".
 
- Word Mangling
 - pegar uma palavra, por exemplo "usuário", e tentar senhas como "usuário1", "usuário2"...
 
 
 
 

Ciphertext - The result of encrypting a plaintext, encrypted data

Cipher - A method of encrypting or decrypting data. Modern ciphers are cryptographic, but there are many non cryptographic ciphers like Caesar.

Plaintext - Data before encryption, often text but not always. Could be a photograph or other file

Encryption - Transforming data into ciphertext, using a cipher.

Encoding - NOT a form of encryption, just a form of data representation like base64. Immediately reversible.

Key - Some information that is needed to correctly decrypt the ciphertext and obtain the plaintext.

Passphrase - Separate to the key, a passphrase is similar to a password and used to protect a key.

Asymmetric encryption - Uses different keys to encrypt and decrypt.

Symmetric encryption - Uses the same key to encrypt and decrypt

Brute force - Attacking cryptography by trying every different password or every different key

Cryptanalysis - Attacking cryptography by finding a weakness in the underlying maths


- Cryptography is used to protect confidentiality, ensure integrity, ensure authenticity. 

- DO NOT encrypt passwords unless you’re doing something like a password manager. Passwords should not be stored in plaintext, and you should use hashing to manage them safely.


There's a little bit of math(s) that comes up relatively often in cryptography. The Modulo operator. 



- Symmetric encryption uses the same key to encrypt and decrypt the data. 
 - Examples of Symmetric encryption are DES (Broken) and AES
 - tend to be faster than asymmetric cryptography,
 - use smaller keys (128 or 256 bit keys are common for AES, DES keys are 56 bits long).
 
- Asymmetric encryption uses a pair of keys, one to encrypt and the other in the pair to decrypt.
 - Examples are RSA and Elliptic Curve Cryptography.  
 - Normally these keys are referred to as a public key and a private key. 
  - Data encrypted with the private key can be decrypted with the public key, and vice versa. 
  - Asymmetric encryption tends to be slower and uses larger keys, for example RSA typically uses 2048 to 4096 bit keys.
  
- RSA - Rivest Shamir Adleman
 - The key variables that you need to know about for RSA in CTFs are p, q, m, n, e, d, and c.
 - “p” and “q” are large prime numbers, “n” is the product of p and q.
 - The public key is n and e, the private key is n and d.
 - “m” is used to represent the message (in plaintext) and “c” represents the ciphertext (encrypted text).
 
- Digital signatures are a way to prove the authenticity of files, to prove who created or modified them.

- By default, SSH keys are RSA keys.
 - You can choose which algorithm to generate, and/or add a passphrase to encrypt the SSH key.
 - key authentication is enabled as it is more secure than using a password to authenticate. Normally for the root user, only key authentication is enabled.
 
- PGP stands for Pretty Good Privacy
 - It’s a software that implements encryption for encrypting files, performing digital signing and
 - With PGP/GPG, private keys can be protected with passphrases in a similar way to SSH private keys. 
 
- AES - Advanced Encryption Standard
 - It was a replacement for DES which had short keys and other cryptographic flaws.
 - AES and DES both operate on blocks of data (a block is a fixed size series of bits).
 - The NSA recommends using RSA-3072 or better for asymmetric encryption and AES-256 or better for symmetric encryption.
 
 
 
- a Windows domain is a group of users and computers under the administration of a given business.
 - The main idea behind a domain is to centralise the administration of common components of a Windows computer network in a single repository called Active Directory (AD).
 - The server that runs the Active Directory services is known as a Domain Controller (DC).
 
The main advantages of having a configured Windows domain are:

Centralised identity management: All users across the network can be configured from Active Directory with minimum effort.

Managing security policies: You can configure security policies directly from Active Directory and apply them to users and computers across the network as needed.



- The core of any Windows Domain is the Active Directory Domain Service (AD DS). 
 -  This service acts as a catalogue that holds the information of all of the "objects" that exist on your network.
 
- Users can be used to represent two types of entities:
 - People: users will generally represent persons in your organisation that need to access the network, like employees.
 
 - Services: you can also define users to be used by services like IIS or MSSQL. Every single service requires a user to run, but service users are different from regular users as they will only have the privileges needed to run their specific service.
 
- objects are organised in Organizational Units (OUs) which are container objects that allow you to classify users and machines
 -  OUs are mainly used to define sets of users with similar policing requirements. 
 -  The people in the Sales department of your organisation are likely to have a different set of policies applied than the people in IT
 
- Builtin: Contains default groups available to any Windows host.
- Computers: Any machine joining the network will be put here by default. You can move them if needed.
- Domain Controllers: Default OU that contains the DCs in your network.
- Users: Default users and groups that apply to a domain-wide context.
- Managed Service Accounts: Holds accounts used by services in your Windows domain.


- OUs are handy for applying policies to users and computers, which include specific configurations that pertain to sets of users depending on their particular role in the enterprise. 

- Security Groups, on the other hand, are used to grant permissions over resources. 

- Windows manages such policies through Group Policy Objects (GPO)
 - GPOs are simply a collection of settings that can be applied to OUs.
 - GPOs can contain policies aimed at either users or computers, allowing you to set a baseline on specific machines and identities.
 
- When using Windows domains, all credentials are stored in the Domain Controllers
 - Two protocols can be used for network authentication in windows domains:
  - Kerberos: Used by any recent version of Windows. This is the default protocol in any recent domain.
   Cliente -> Requisição TGT -> KDC (Controlador de Domínio) -> Responde ao Cliente (Para Serviços) 
   TGT + Cliente -> Requisição TGS -> KDC (Controlador de Domínio) -> Responde ao Cliente (Para compartilhamentos, bancos de dados...)
   
  - NetNTLM: Legacy authentication protocol kept for compatibility purposes.
  
- If you have two domains that share the same namespace (thm.local in our example), those domains can be joined into a Tree.

- The union of several trees with different namespaces into the same network is known as a forest.
 - domains arranged in trees and forests are joined together by trust relationships.
 


- The Metasploit Framework is a set of tools that allow information gathering, scanning, exploitation, exploit development, post-exploitation, and more.
 - While the primary usage of the Metasploit Framework focuses on the penetration testing domain, it is also useful for vulnerability research and exploit development.
 
- The main components of the Metasploit Framework can be summarized as follows;
 - msfconsole: The main command-line interface.
 - Modules: supporting modules such as exploits, scanners, payloads, etc.
 - Tools: Stand-alone tools that will help vulnerability research, vulnerability assessment, or penetration testing. Some of these tools are msfvenom, pattern_create and pattern_offset. We will cover msfvenom within this module, but pattern_create and pattern_offset are tools useful in exploit development which is beyond the scope of this module.
 
- Auxiliary: Any supporting module, such as scanners, crawlers and fuzzers, can be found here.
- Encoders: Encoders will allow you to encode the exploit and payload in the hope that a signature-based antivirus solution may miss them.
- Evasion: “evasion” modules will try to evade antivirus 
- Exploits: Exploits, neatly organized by target system.
- NOPs: NOPs (No OPeration) do nothing, literally.
- Payloads: Payloads are codes that will run on the target system.
 - Adapters: An adapter wraps single payloads to convert them into different formats
 - Singles: Self-contained payloads (add user, launch notepad.exe, etc.) that do not need to download an additional component to run.
 - Stagers: Responsible for setting up a connection channel between Metasploit and the target system. 
 - Stages: Downloaded by the stager. This will allow you to use larger sized payloads.
- Post modules will be useful on the final stage of the penetration testing process listed above, post-exploitation.



HTTP: Could potentially host a web application where you can find vulnerabilities like SQL injection or Remote Code Execution (RCE). 
FTP: Could allow anonymous login and provide access to interesting files. 
SMB: Could be vulnerable to SMB exploits like MS17-010
SSH: Could have default or easy to guess credentials
RDP: Could be vulnerable to Bluekeep or allow desktop access if weak credentials were used. 

- The term “low hanging fruit” usually refers to easily identifiable and exploitable vulnerabilities that could potentially allow you to gain a foothold on a system



- Meterpreter is a Metasploit payload that supports the penetration testing process with many valuable components. 
 - Meterpreter will run on the target system and act as an agent within a command and control architecture. 
 - You will interact with the target operating system and files and use Meterpreter's specialized commands.
  - Meterpreter runs on the target system but is not installed on it. It runs in memory and does not write itself to the disk on the target.
  - Meterpreter runs in memory (RAM - Random Access Memory) to avoid having a file, so, it will be seen as a process 
   -  If the target organization does not decrypt and inspect encrypted traffic (e.g. HTTPS) coming to and going out of the local network, IPS and IDS solutions will not be able to detect its activities.
   
   - staged payloads are sent to the target in two steps. An initial part is installed (the stager) and requests the rest of the payload
   - he inline payloads are sent in a single step. 
   
   - Your decision on which version of Meterpreter to use will be mostly based on three factors;
    - The target operating system (Is the target operating system Linux or Windows? Is it a Mac device? Is it an Android phone? etc.)
    - Components available on the target system (Is Python installed? Is this a PHP website? etc.)
    - Network connection types you can have with the target system (Do they allow raw TCP connections? Can you only have an HTTPS reverse connection? Are IPv6 addresses not as closely monitored as IPv4 addresses? etc.) 
	
	
	
- In the simplest possible terms, shells are what we use when interfacing with a Command Line environment (CLI).
 - the common bash or sh programs in Linux are examples of shells, as are cmd.exe and Powershell on Windows.
  - When targeting remote systems it is sometimes possible to force an application running on the server (such as a webserver, for example) to execute arbitrary code.
  -  When this happens, we want to use this initial access to obtain a shell running on the target.
   - we can force the remote server to either send us command line access to the server (a reverse shell)
   - to open up a port on the server which we can connect to in order to execute further commands (a bind shell).
   
   
- Netcat is the traditional "Swiss Army Knife" of networking. 
 -  It is used to manually perform all kinds of network interactions
 - it can be used to receive reverse shells and connect to remote ports attached to bind shells on a target
 - Netcat shells are very unstable (easy to lose) by default, but can be improved by techniques
 
- Socat is like netcat on steroids. 
 - It can do all of the same things, and many more
 - Socat shells are usually more stable than netcat shells out of the box
  -  there are two big catches:
   - The syntax is more difficult
   - Netcat is installed on virtually every Linux distribution by default. Socat is very rarely installed by default.
    - Both Socat and Netcat have .exe versions for use on Windows.
	
- The exploit/multi/handler module of the Metasploit framework is, like socat and netcat, used to receive reverse shells. 
 -  It's also the only way to interact with a meterpreter shell
 
- Like multi/handler, msfvenom is technically part of the Metasploit Framework, however, it is shipped as a standalone tool.
 - Msfvenom is used to generate payloads on the fly.
 
-  we are interested in two kinds of shell when it comes to exploiting a target: Reverse shells, and bind shells.

 - Reverse shells are when the target is forced to execute code that connects back to your computer. 
  -  On your own computer you would use one of the tools mentioned in the previous task to set up a listener which would be used to receive the connection.
  -  Reverse shells are a good way to bypass firewall rules that may prevent you from connecting to arbitrary ports on the target;
  -  the drawback is that, when receiving a shell from a machine across the internet, you would need to configure your own network to accept the shell.
  
 - Bind shells are when the code executed on the target is used to start a listener attached to a shell directly on the target
  - This would then be opened up to the internet, meaning you can connect to the port that the code has opened and obtain remote code execution that way
  - This has the advantage of not requiring any configuration on your own network, but may be prevented by firewalls protecting the target.
 
- reverse shells are easier to execute and debug

- Shells can be either interactive or non-interactive.
 - Interactive 
  - f you've used Powershell, Bash, Zsh, sh, or any other standard CLI environment then you will be used to
interactive shells. 
  - These allow you to interact with programs after executing them.
   - Exemplo: SSH / programas que pedem credenciais 
  
  - Non-Interactive
   - In a non-interactive shell you are limited to using programs which do not require user interaction
   -  the majority of simple reverse and bind shells are non-interactive, 
    - comandos - tipo whoami 
	
- Listener 
 - Realistically you could use any port you like, as long as there isn't already a service using it
 - if you choose to use a port below 1024, precisa ser ADM. 
 - Porta 80 / 443 ou 53 são boas opções, para fugir do Firewall 
 
- If we are looking to obtain a bind shell on a target then we can assume that there is already a listener waiting for us on a chosen port of the target

- Formas de estabilizar os Shells: 
 - Usando Python
 - Usando rlwrap 
 - Usando Socat 
 
- Socat 
 -  socat is as a connector between two points 
  - One of the many great things about socat is that it's capable of creating encrypted shells - both bind and reverse
  - Encrypted shells cannot be spied on unless you have the decryption key, and are often able to bypass an IDS as a result.
  - We first need to generate a certificate in order to use encrypted shells
  


- Tipos de Reverse Shell Payloads 
 
  - Staged 
   - are sent in two parts. 
   - The first part is called the stager. This is a piece of code which is executed directly on the server itself. It connects back to a waiting listener, but doesn't actually contain any reverse shell code by itself.
   - 
  
  - Stageless 
   - They are entirely self-contained in that there is one piece of code which, when executed, sends a shell back immediately to the waiting listener.
   -  easier to use and catch; however, they are also bulkier, and are easier for an antivirus or intrusion detection program to discover and remove
   
-  Anti-Malware Scan Interface (AMSI)


- Multi/Handler is a superb tool for catching reverse shells. It's essential if you want to use Meterpreter shells, and is the go-to when using staged payloads.
 - Open Metasploit with msfconsole
 - Type use multi/handler, and press enter
 
- "Webshell" is a colloquial term for a script that runs inside a webserver (usually in a language such as PHP or ASP) 
 - which executes code on the server. 
 
 
 
- Privesc - Privilege Escalation 
 - Privilege Escalation usually involves going from a lower permission to a higher permission
 - it's the exploitation of a vulnerability, design flaw or configuration oversight in an operating system or application to gain unauthorized access to resources that are usually restricted from the users.
 - Rarely when doing a CTF or real-world penetration test, will you be able to gain a foothold (initial access) that affords you administrator access.
  - This allow you to do many things, including:
   Reset passwords
   Bypass access controls to compromise protected data
   Edit software configurations
   Enable persistence, so you can access the machine again later.
   Change privilege of users
   Get that cheeky root flag ;)
   
  - Horizontal privilege escalation: This is where you expand your reach over the compromised system by taking over a different user who is on the same privilege level as you.
  - Vertical privilege escalation (privilege elevation): This is where you attempt to gain higher privileges or access
 
- LinEnum is a simple bash script that performs common commands related to privilege escalation

- The first step in Linux privilege escalation exploitation is to check for files with the SUID/GUID bit set 
 - This means that the file or files can be run with the permissions of the file(s) owner/group. In this case, as the super-user. 
 - The maximum number of bit that can be used to set permission for each user is 7
 - which is a combination of read (4) write (2) and execute (1) operation
  - But when special permission is given to each user it becomes SUID or SGID. 
   - When extra bit “4” is set to user(Owner) it becomes SUID
   - when bit “2” is set to group it becomes SGID (Set Group ID).
   
Usuário / Grupo / Outros    
   
SUID:
rws-rwx-rwx

GUID:
rwx-rws-rwx

- GTFOBins is a curated list of Unix binaries that can be exploited by an attacker to bypass local security restrictions. 

- The Cron daemon is a long-running process that executes commands at specific dates and times.
 - You can use this to schedule activities, either as one-time events or as recurring tasks.
 
 
- PATH is an environmental variable in Linux and Unix-like operating systems which specifies directories that hold executable programs
 - When the user runs any command in the terminal, it searches for executable files with the help of the PATH Variable in response to commands executed by a user.
 
 
- User Defined Functions (UDFs)
 
- The /etc/passwd file contains information about user accounts. It is world-readable, but usually only writable by the root user.
- The /etc/shadow file contains user password hashes and is usually readable only by the root user.


- Burp Suite captures and enables manipulation of all the HTTP/HTTPS traffic between a browser and a web server.


- Samba is the standard Windows interoperability suite of programs for Linux and Unix. 
 - It allows end users to access and use files, printers and other commonly shared resources on a companies intranet or internet.
 - Its often referred to as a network file system.
  - Samba is based on the common client/server protocol of Server Message Block (SMB)
   - SMB is developed only for Windows, without Samba, other computer platforms would be isolated from Windows machines, even if they were part of the same network.
   
 - Samba possui 2 portas 
  - 139 - Netbios 
  - 443 - TCP 
  
  
- ProFtpd is a free and open-source FTP server, compatible with Unix and Windows systems.

- Searchsploit is basically just a command line search tool for exploit-db.com.


Permission	On Files	On Directories
SUID Bit	User executes the file with permissions of the file owner									-
SGID Bit	User executes the file with the permission of the group owner. File created in directory gets the same group owner.
Sticky Bit	No meaning														Users are prevented from deleting files from other users.


- A framework is a collection of premade code that easily allows a developer to include common features that a website would require, such as blogs, user management, form processing, and much more, saving the developers hours or days of development.


- Content can be many things, a file, video, picture, backup, a website feature.

- There are three main ways of discovering content on a website
 - Manually, Automated and OSINT (Open-Source Intelligence).
 
- The robots.txt file is a document that tells search engines which pages they are and aren't allowed to show on their search engine results
 - or ban specific search engines from crawling the website altogether.
 - It can be common practice to restrict certain website areas so they aren't displayed in search engine results. 
 - These pages may be areas such as administration portals or files meant for the website's customers.
  - This file gives us a great list of locations on the website that the owners don't want us to discover as penetration testers.
  
- The favicon is a small icon displayed in the browser's address bar or tab used for branding a website.
 - Sometimes when frameworks are used to build a website, a favicon that is part of the installation gets leftover
 -  and if the website developer doesn't replace this with a custom one, this can give us a clue on what framework is in use. 
 - OWASP host a database of common framework icons that you can use to check against the targets favicon https://wiki.owasp.org/index.php/OWASP_favicon_database. 
 
- the sitemap.xml file gives a list of every file the website owner wishes to be listed on a search engine.
 - These can sometimes contain areas of the website that are a bit more difficult to navigate to
 - or even list some old webpages that the current site no longer uses but are still working behind the scenes.
 
- When we make requests to the web server, the server returns various HTTP headers. 
 - These headers can sometimes contain useful information such as the webserver software and possibly the programming/scripting language in use.
 
- Wappalyzer (https://www.wappalyzer.com/) is an online tool and browser extension that helps identify what technologies a website uses

- Google Dork 
 - Pesquisas customizadas do Google

- The Wayback Machine (https://archive.org/web/) is a historical archive of websites that dates back to the late 90s. 
 - This service can help uncover old pages that may still be active on the current website.
 
- Git is a version control system that tracks changes to files in a project. 
 - GitHub is a hosted version of Git on the internet.
 - You can use GitHub's search feature to look for company names or website names to try and locate repositories belonging to your target
 
- S3 Buckets are a storage service provided by Amazon AWS
 - allowing people to save files and even static website content in the cloud accessible over HTTP and HTTPS.
 - The owner of the files can set access permissions to either make files public, private and even writable. 
  - The format of the S3 buckets is http(s)://{name}.s3.amazonaws.com where {name} is decided by the owner
  - S3 buckets can be discovered in many ways, such as finding the URLs in the website's page source, GitHub repositories, or even automating the process. 
  
  
- Subdomain enumeration is the process of finding valid subdomains for a domain
 -  We do this to expand our attack surface to try and discover more potential points of vulnerability.
 -  subdomain enumeration methods: Brute Force, OSINT (Open-Source Intelligence) and Virtual Host.
 
-  SSL/TLS (Secure Sockets Layer/Transport Layer Security) 

- certificate is created for a domain by a CA (Certificate Authority)
 - CA's take part in what's called "Certificate Transparency (CT) logs".
 - These are publicly accessible logs of every SSL/TLS certificate created for a domain name.
 - The purpose of Certificate Transparency logs is to stop malicious and accidentally made certificates from being used. 
  - We can use this service to our advantage to discover subdomains belonging to a domain, sites like https://crt.sh and https://ui.ctsearch.entrust.com/ui/ctsearchui 
  

-  helpful exercise to complete when trying to find authentication vulnerabilities is creating a list of valid usernames

- A brute force attack is an automated process that tries a list of commonly used passwords against either a single username or, like in our case, a list of usernames.

- Sometimes authentication processes contain logic flaws.
 - A logic flaw is when the typical logical path of an application is either bypassed, circumvented or manipulated by a hacker.
 
 
 
- IDOR stands for Insecure Direct Object Reference and is a type of access control vulnerability.
 -  can occur when a web server receives user-supplied input to retrieve objects (files, data, documents)
 -  too much trust has been placed on the input data, and it is not validated on the server-side to confirm the requested object belongs to the user requesting it.
  - Por exemplo, na URL tem www.amazon.com/leandro. Se alterar para www.amazon.com/admin, e abrir as coisas de admin, seria uma IDOR 
  
- When passing data from page to page either by post data, query strings, or cookies, web developers will often first take the raw data and encode it
 -  Encoding changes binary data into an ASCII string commonly using the a-z, A-Z, 0-9 and =
 - The most common encoding technique on the web is base64 encoding and can usually be pretty easy to spot. 
 
- Hashed IDs are a little bit more complicated to deal with than encoded ones

- If the Id cannot be detected using the above methods, an excellent method 
 - is to create two accounts and swap the Id numbers between them.
 -  If you can view the other users' content using their Id number while still being logged in with a different account you've found a valid IDOR vulnerability.
 
 
 
 
- Local File Inclusion (LFI)
- Remote File Inclusion (RFI)

- Parameters are query parameter strings attached to the URL that could be used to retrieve data or perform actions based on user input.

- Path Traversal / Directory Transversal
 -  a web security vulnerability allows an attacker to read operating system resources, such as local files on the server running an application
 - The attacker exploits this vulnerability by manipulating and abusing the web application's URL to locate and access files or directories stored outside the application's root directory.
  - Often poor input validation or filtering is the cause of the vulnerability.
  
  
  
  
- SSRF stands for Server-Side Request Forgery.
 - It's a vulnerability that allows a malicious user to cause the webserver to make an additional or edited HTTP request to the resource of the attacker's choosing.
 
 - There are two types of SSRF vulnerability
  - the first is a regular SSRF where data is returned to the attacker's screen.
  - The second is a Blind SSRF vulnerability where an SSRF occurs, but no information is returned to the attacker's screen.
  
  - A successful SSRF attack can result in any of the following: 
   - Access to unauthorised areas.
   - Access to customer/organisational data.
   - Ability to Scale to internal networks.
   - Reveal authentication tokens/credentials.
 
 
- A Deny List is where all requests are accepted apart from resources specified in a list or matching a particular pattern.
 - A Web Application may employ a deny list to protect sensitive endpoints, IP addresses or domains from being accessed by the public while still allowing access to other locations
 
 -  in a cloud environment, it would be beneficial to block access to the IP address 169.254.169.254
 
- An allow list is where all requests get denied unless they appear on a list or match a particular pattern




-  XSS is based on JavaScript, it would be helpful to have a basic understanding of the language (Cross Site Scripting) 
 - also, a basic understanding of Client-Server requests and responses.
 - injection attack where malicious JavaScript gets injected into a web application with the intention of being executed by other users. 
 
- In XSS, the payload is the JavaScript code we wish to be executed on the targets computer. 
 - There are two parts to the payload, the intention and the modification.
  - The intention is what you wish the JavaScript to actually do 
  -  the modification is the changes to the code we need to make it execute as every scenario is different 
  
- Details of a user's session, such as login tokens, are often kept in cookies on the targets machine


- Reflected XSS happens when user-supplied data in an HTTP request is included in the webpage source without any validation.

- Stored XSS payload is stored on the web application (in a database, for example) and then gets run when other users visit the site or web page.

- DOM stands for Document Object Model and is a programming interface for HTML and XML documents.
 - A web page is a document, and this document can be either displayed in the browser window or as the HTML source
 - DOM Based XSS is where the JavaScript execution happens directly in the browser without any new pages being loaded or data submitted to backend code.
 - You'd need to look for parts of the code that access certain variables that an attacker can have control over, such as "window.location.x" parameters.
 
 
- Blind XSS is similar to a stored XSS 
 - in that your payload gets stored on the website for another user to view, but in this instance, you can't see the payload working or be able to test it against yourself first.



- How your JavaScript payload gets reflected in a target website's code will determine the payload you need to use.


- An XSS polyglot is a string of text which can escape attributes, tags and bypass filters all in one.


- Command injection is the abuse of an application's behaviour to execute commands on the operating system, 
 - using the same privileges that the application on a device is running with.
 
- Command injection is also often known as “Remote Code Execution” (RCE) because of the ability to remotely execute code within an application.

- This vulnerability exists because applications often use functions in programming languages such as PHP, Python and NodeJS to pass data to and to make system calls on the machine’s operating system. 

- Command Injection can be detected in mostly one of two ways:
 - Blind command injection - This type of injection is where there is no direct output from the application when testing payloads.
 - Verbose command injection - This type of injection is where there is direct feedback from the application once you have tested a payload.
 
 
- SQL (Structured Query Language) Injection, mostly referred to as SQLi
 - attack on a web application database server that causes malicious queries to be executed.
  - When a web application communicates with a database using input from a user that hasn't been properly validated, there runs the potential of an attacker being able to steal, delete or alter private and customer data and also attack the web applications authentication
  
  
- A database is a way of electronically storing collections of data in an organised manner.
 - A database is controlled by a DBMS which is an acronym for  Database Management System
 - DBMS's fall into two camps Relational or Non-Relational
 
- In-Band SQL Injection is the easiest type to detect and exploit;
 - In-Band just refers to the same method of communication being used to exploit the vulnerability and also receive the results
 
- Error-Based SQL Injection
 - This type of SQL Injection is the most useful for easily obtaining information about the database structure as error messages from the database are printed directly to the browser screen.
 
- Union-Based SQL Injection
 - This type of Injection utilises the SQL UNION operator alongside a SELECT statement to return additional results to the page.
 
- Blind SQLi
 - Unlike In-Band SQL injection, where we can see the results of our attack directly on the screen, blind SQLi is when we get little to no feedback to confirm whether our injected queries were
  - Tipos de Blind SQLi
   - Authentication Bypass
   - Boolean Based 
   - Time Based 
   
- remediações:
 
Prepared Statements (With Parameterized Queries):
 In a prepared query, the first thing a developer writes is the SQL query and then any user inputs are added as a parameter afterwards. Writing prepared statements ensures that the SQL code structure doesn't change and the database can distinguish between the query and the data. As a benefit, it also makes your code look a lot cleaner and easier to read.

Input Validation:
 Input validation can go a long way to protecting what gets put into an SQL query. Employing an allow list can restrict input to only certain strings, or a string replacement method in the programming language can filter the characters you wish to allow or disallow. 

Escaping User Input:
 Allowing user input containing characters such as ' " $ \ can cause SQL Queries to break or, even worse, as we've learnt, open them up for injection attacks. Escaping user input is the method of prepending a backslash (\) to these characters, which then causes them to be parsed just as a regular string and not a special character.
 
 
 
- Intruder is Burp Suite's built-in fuzzing tool that allows for automated request modification and repetitive testing with variations in input values
 -  Intruder can send multiple requests with slightly altered values based on user-defined configurations.
  - It serves various purposes, such as brute-forcing login forms by substituting username and password fields with values from a wordlist 
  - performing fuzzing attacks using wordlists to test subdirectories, endpoints, or virtual hosts.
   - Intruder's functionality is comparable to command-line tools like Wfuzz or ffuf.
   
- The term "fuzzing" refers to the process of testing functionality or existence by applying a set of data to a parameter. 
 - Fuzzing for endpoints in a web application involves taking each word in a wordlist and appending it to a request URL (e.g., http://10.10.1.37/WORD_GOES_HERE) to observe the server's response.
 

- Insecure Direct Object References (IDOR)

- Macro: Repeated set of actions

- Hashing is a one-way process that transforms data into a unique signature.
 - For a function to qualify as a hashing algorithm, the output it generates must be irreversible. 
 - hashes are commonly used to verify the integrity of files and documents, as even a tiny alteration to the file significantly changes the hashsum.
 
- Tokens are strings used to identify something and should ideally be generated in a cryptographically secure manner.



- In Burp Suite, the BApp Store (Burp App Store) allows us to easily discover and integrate official extensions seamlessly into the tool.
 - Extensions can be written in various languages, with Java and Python being the most common choices.
  - To use Python modules in Burp Suite, we need to include the Jython Interpreter JAR file, which is a Java implementation of Python.
  
  
  
  
  
- In the Junior Security Analyst role, you will be a Triage Specialist. 
 - You will spend a lot of time triaging or monitoring the event logs and alerts.
 - The core function of a SOC (Security Operations Center) is to investigate, monitor, prevent, and respond to threats in the cyber realm 24/7 or around the clock.
 
- Prevention methods include gathering intelligence data on the latest threats, threat actors, and their TTPs (Tactics, Techniques, and Procedures)

- A SOC team proactively uses SIEM (Security information and event management) and EDR (Endpoint Detection and Response) tools to monitor suspicious and malicious network activities.


- Pyramid of Pain 
 - This well-renowned concept is being applied to cybersecurity solutions like Cisco Security, SentinelOne, and SOCRadar 
 - to improve the effectiveness of CTI (Cyber Threat Intelligence), threat hunting, and incident response exercises.
 
 
 
A hash value is the result of a hashing algorithm.
- A hash is not considered to be cryptographically secure if two files have the same hash value or digest.

- Security professionals usually use the hash values to gain insight into a specific malware sample, a malicious or a suspicious file
- as a way to uniquely identify and reference the malicious artifact.

- Hash Lookups: 
https://www.virustotal.com/gui/
https://metadefender.opswat.com/?lang=en


- As an attacker, modifying a file by even a single bit is trivial, which would produce a different hash value
- With so many variations and instances of known malware or ransomware, threat hunting using file hashes as the IOC (Indicators of Compromise) can become difficult.




- An IP address is used to identify any device connected to a network.
- We rely on IP addresses to send and receive the information over the network. 
- From a defense standpoint, knowledge of the IP addresses an adversary uses can be valuable. 
  - A common defense tactic is to block, drop, or deny inbound requests from IP addresses on your parameter or external firewall.

   - One of the ways an adversary can make it challenging to successfully carry out IP blocking is by using Fast Flux.
   - Fast Flux is a DNS technique used by botnets to hide phishing, web proxying, malware delivery, and malware communication activities behind compromised hosts acting as proxies.
   - The purpose of using the Fast Flux network is to make the communication between malware and its command and control server (C&C) challenging to be discovered by security professionals. 
    - the primary concept of a Fast Flux network is having multiple IP addresses associated with a domain name, which is constantly changing.
		- https://unit42.paloaltonetworks.com/fast-flux-101/


https://app.any.run/tasks/a66178de-7596-4a05-945d-704dbf6b3b90 -  sandboxing service that executes the sample, we can review any connections such as HTTP requests, DNS requests or processes communicating with an IP address. (Any.run)

 

 

- Domain Names can be thought as simply mapping an IP address to a string of text.
- A domain name can contain a domain and a top-level domain (evilcorp.com) or a sub-domain followed by a domain and top-level domain (tryhackme.evilcorp.com).

- Domain Names can be a little more of a pain for the attacker to change as they would most likely need to purchase the domain, register it and modify DNS records.

- "Punycode is a way of converting words that cannot be written in ASCII, into a Unicode ASCII encoding."
  - Muito comum para enganar e fazer com que os sites pareçam legítimos. Com caracteres de diferentes tipos de codificação 
  - Attackers usually hide the malicious domains under URL Shorteners.
   - A URL Shortener is a tool that creates a short and unique URL that will redirect to the specific website specified during the initial step of setting up the URL Shortener link
   - attackers use the following URL Shortening services to generate malicious links: 
	bit.ly
	goo.gl
	ow.ly
	s.id
	smarturl.it
	tiny.pl
	tinyurl.com
	x.co
	- (colocar um "+" no final do link mostra pra onde direciona: http://goo.gl/sXloW2+)


- Host artifacts are the traces or observables that attackers leave on the system, such as registry values, suspicious process execution, attack patterns or IOCs (Indicators of Compromise), files dropped by malicious applications, or anything exclusive to the current threat.


- A network artifact can be a user-agent string, C2 information, or URI patterns followed by the HTTP POST requests.
- Network artifacts can be detected in Wireshark PCAPs (file that contains the packet data of a network) by using a network protocol analyzer such as TShark or exploring IDS (Intrusion Detection System) logging from a source such as Snort.

- MalwareBazaar and Malshare are good resources to provide you with access to the samples, malicious feeds, and YARA results - these all can be very helpful when it comes to threat hunting and incident response.

- For detection rules, SOC Prime Threat Detection Marketplace is a great platform, where security professionals share their detection rules for different kinds of threats including the latest CVE's that are being exploited in the wild by adversaries.

- Fuzzy hashing is also a strong weapon against the attacker's tools. Fuzzy hashing helps you to perform similarity analysis - match two files with minor differences based on the fuzzy hash values. 
- One of the examples of fuzzy hashing is the usage of SSDeep; on the SSDeep official website, you can also find the complete explanation for fuzzy hashing. 
- https://ssdeep-project.github.io/ssdeep/index.html

- TTPs stands for Tactics, Techniques & Procedures.
- https://attack.mitre.org/

- https://www.fireeye.com/current-threats/apt-groups.html

- As David Blanco states, "the amount of pain you cause an adversary depends on the types of indicators you are able to make use of".





- The term kill chain is a military concept related to the structure of an attack.
 - It consists of target identification, decision and order to attack the target, and finally the target destruction.
 
- Advanced Persistent Threats (APTs)

- You can use the Cyber Kill Chain to assess your network and system security by identifying missing security controls and closing certain security gaps based on your company's infrastructure.

- Reconnaissance is discovering and collecting information on the system and the victim.
 - The reconnaissance phase is the planning phase for the adversaries.
  - OSINT (Open-Source Intelligence) also falls under reconnaissance
  - OSINT is the first step an attacker needs to complete to carry out the further phases of an attack.
   - The attacker needs to study the victim by collecting every available piece of information on the company and its employees, such as the company's size, email addresses, phone numbers from publicly available resources to determine the best target for the attack. 
   https://www.varonis.com/blog/what-is-osint/
   
- Email harvesting is the process of obtaining email addressesfrom public, paid, or free services.
 - An attacker would also use social media websites such as LinkedIn, Facebook, Twitter, and Instagram to collect information on a specific victim

theHarvester - other than gathering emails, this tool is also capable of gathering names, subdomains, IPs, and URLs using multiple public data sources 
Hunter.io - this is  an email hunting tool that will let you obtain contact information associated with the domain
OSINT Framework - OSINT Framework provides the collection of OSINT tools based on various categories



- "weaponizer" that, according to Lockheed Martin, combines malware and exploit into a deliverable payload.
 - Most attackers usually use automated tools to generate the malware or refer to the DarkWeb to purchase the malware.
 
 - Malware is a program or software that is designed to damage, disrupt, or gain unauthorized access to a computer.
 - An exploit is a program or a code that takes advantage of the vulnerability or flaw in the application or system.
 - A payload is a malicious code that the attacker runs on the system.
 
 
- The Delivery phase is when the attacker choose the method for transmitting the payload or the malware
 - Phishing Email
 - Distributing infected USB Drives
 - Watering Hole - Direciona os usuários rotineiros de sites específicos a acessarem um site falso, instalado coisas falsas (Drive-By Download) 
 
- lateral movement refers to the techniques that a malicious actor uses after gaining initial access to the victim's machine to move deeper into a network to obtain sensitive data. 
 
- the zero-day exploit or a zero-day vulnerability is an unknown exploit in the wild that exposes a vulnerability
 - A zero-day exploit leaves NO opportunity for detection at the beginning."
 
 
- the backdoor lets an attacker bypass security measures and hide the access.
 - A backdoor is also known as an access point. 
 - Once the attacker gets access to the system, he would want to reaccess the system if he loses the connection to it
 - or if he got detected and got the initial access removed, or if the system is later patched.
  - That is when the attacker needs to install a persistent backdoor
  - A persistent backdoor will let the attacker access the system he compromised in the past. 
  
  - The persistence can be achieved through:
   - Installing a web shell on the webserver. A web shell is a malicious script written in web development programming languages such as ASP, PHP, or JSP used by an attacker to maintain access to the compromised system.
   - Installing a backdoor on the victim's machine. For example, the attacker can use Meterpreter to install a backdoor on the victim's machine.
   - Creating or modifying Windows services. This technique is known as T1543.003. An attacker can create or modify the Windows services to execute the malicious scripts or payloads regularly as a part of the persistence. Adding the entry to the "run keys" for the malicious payload in the Registry or the Startup Folder. 
  
-  the attacker can also use the Timestomping technique to avoid detection by the forensic investigator and also to make the malware appear as a part of a legitimate program. 
 - The Timestomping technique lets an attacker modify the file's timestamps, including the modify, access, create and change times. 
  
-  MITRE ATT&CK (MITRE ATT&CK® is a knowledge base of adversary tactics and techniques based on real-world scenarios).



-  C&C or C2 Beaconing as a type of malicious communication between a C&C server and malware on the infected host.
 - The infected host will consistently communicate with the C2 server; that is also where the beaconing term came from. 
 - Until recently, IRC (Internet Relay Chat) was the traditional C2 channel used by attackers. This is no longer the case
  - The most common C2 channels used by adversaries nowadays:
   - The protocols HTTP on port 80 and HTTPS on port 443 - this type of beaconing blends the malicious traffic with the legitimate traffic and can help the attacker evade firewalls.  
   - DNS (Domain Name Server). The infected machine makes constant DNS requests to the DNS server that belongs to an attacker, this type of C2 communication is also known as DNS Tunneling.
   
- Depois das 6 etapas de ataque, o atacante pode conseguir o seguinte: 
 - Collect the credentials from users.
 - Perform privilege escalation (gaining elevated access like domain administrator access from a workstation by exploiting the misconfiguration).
 - Internal reconnaissance (for example, an attacker gets to interact with internal software to find its vulnerabilities).
 - Lateral movement through the company's environment.
 - Collect and exfiltrate sensitive data.
 - Deleting the backups and shadow copies.
    (Shadow Copy is a Microsoft technology that can create backup copies, snapshots of computer files, or volumes. )
 - Overwrite or corrupt data.
 
- Adversaries are capable of defeating threat intelligence by modifying the file hashes and IP addresses.
 -  Security solutions companies are developing technologies like AI (Artificial Intelligence) and different algorithms to detect even slight and suspicious changes. 
 
- Insider Threats.
 - "The Insider Threat is the potential for an insider to use their authorized access or understanding of an organization to harm that organization."
 
- We recommend not only relying on the traditional Cyber Kill Chain model but also referring to MITRE ATT&CK as well as Unified Kill Chain to apply a more comprehensive approach to your defence methodologies. 





-  UKC (Unified Kill Chain) framework that is used to help understand how cyber attacks occur.
 - “Kill Chain” is a term used to explain the various stages of an attack.
 -  In the realm of cybersecurity, a “Kill Chain” is used to describe the methodology/path attackers such as hackers or APTs use to approach and intrude a target 
  - For example, an attacker scanning, exploiting a web vulnerability, and escalating privileges will be a “Kill Chain”
  
- Threat modelling, in a cybersecurity context, is a series of steps to ultimately improve the security of a system
 - Threat modelling is about identifying risk and essentially boils down to:
  - Identifying what systems and applications need to be secured and what function they serve in the environment.
  - Assessing what vulnerabilities and weaknesses these systems and applications may have and how they could be potentially exploited
  - Creating a plan of action to secure these systems and applications from the vulnerabilities highlighted
  - Putting in policies to prevent these vulnerabilities from occurring again where possible
  
- Threat modelling is an important procedure in reducing the risk within a system or application
 - as it creates a high-level overview of an organisation's IT assets (an asset in IT is a piece of software or hardware) and the procedures to resolve vulnerabilities.
 
- STRIDE, DREAD and CVSS (to name a few) are all frameworks specifically used in threat modelling. 
 
- The UKC states that there are 18 phases to an attack
 1 Reconnaissance
 2 Weaponization
 3 Delivery
 4 Social Engineering
 5 Exploitation
 6 Persistence
 7 Defense Evasion
 8 Command & Control
 9 Pivoting
 -- Initial Foothold --
 10 Discovery
 11 Privilege Escalation
 12 Execution
 13 Credential Access
 14 Lateral Movement
 -- Network Propagation -- 
 15 Collection
 16 Exfiltration
 17 Impact 
 18 Objectives 
 -- Action on Objectives -- 
 
- persistence (such as files or a process that allows the attacker to connect to the machine at any time)





- the Diamond Model is composed of four core features: adversary, infrastructure, capability, and victim
 - and establishes the fundamental atomic element of any intrusion activity.
 - Why is it called a "Diamond Model"? The four core features are edge-connected, representing their underlying relationships and arranged in the shape of a diamond. 
 
- Advanced Persistent Threat (APT).
 
- An adversary is also known as an attacker, enemy, cyber threat actor, or hacker. 
 - The adversary is the person who stands behind the cyberattack.
 - Cyberattacks can be an instruction or a breach.
 
- adversary is an actor or organization responsible for utilizing a capability against the victim to achieve their intent.
 - Adversary knowledge can generally be mysterious, and this core feature is likely to be empty for most events – at least at the time of discovery. 
 
- It is essential to know the distinction between adversary operator and adversary customer because it will help you understand intent, attribution, adaptability, and persistence by helping to frame the relationship between an adversary and victim pair.  
 - Adversary Operator is the “hacker” or person(s) conducting the intrusion activity.
 - Adversary Customer is the entity that stands to benefit from the activity conducted in the intrusion. 
 
- Victim – is a target of the adversary. A victim can be an organization, person, target email address, IP address, domain, etc.
 - A victim can be an opportunity for the attackers to get a foothold on the organization they are trying to attack.
 
- Victim Personae are the people and organizations being targeted and whose assets are being attacked and exploited. 
- Victim Assets are the attack surface and include the set of systems, networks, email addresses, hosts, IP addresses, social networking accounts, etc., 

- Capability – is also known as the skill, tools, and techniques used by the adversary in the event. The capability highlights the adversary’s tactics, techniques, and procedures (TTPs). 
 - The capability can include all techniques used to attack the victims, from the less sophisticated methods, such as manual password guessing, to the most sophisticated techniques, like developing malware or a malicious tool. 
  - Capability Capacity is all of the vulnerabilities and exposures that the individual capability can use.
  - An Adversary Arsenal is a set of capabilities that belong to an adversary. The combined capacities of an adversary's capabilities make it the adversary's arsenal.



- Infrastructure – is also known as software or hardware.
 - Infrastructure is the physical or logical interconnections that the adversary uses to deliver a capability or maintain control of capabilities
 - For example, a command and control centre (C2) and the results from the victim (data exfiltration). 
 
- Type 1 Infrastructure is the infrastructure controlled or owned by the adversary. 
- Type 2 Infrastructure is the infrastructure controlled by an intermediary. Sometimes the intermediary might or might not be aware of it.
 -  Type 2 Infrastructure has the purpose of obfuscating the source and attribution of the activity
 
- Service Providers are organizations that provide services considered critical for the adversary availability of Type 1 and Type 2 Infrastructures, for example, Internet Service Providers, domain registrars, and webmail providers.



- Six possible meta-features can be added to the Diamond Model.
 - Meta-features are not required, but they can add some valuable information or intelligence to the Diamond Model.
	- Timestamp - is the date and time of the event. 
	- Phase - these are the phases of an intrusion, attack, or breach.
	 - Every malicious activity contains two or more phases which must be successfully executed in succession to achieve the desired result
	 - Malicious activities do not occur as single events, but rather as a sequence of events.
	- Result - While the results and post-conditions of an adversary’s operations will not always be known or have a high confidence value when they are known, they are helpful to capture
	- Direction - This meta-feature helps describe host-based and network-based events and represents the direction of the intrusion attack.
	 - Analysis defines seven potential values for this meta-feature: Victim-to-Infrastructure, Infrastructure-to-Victim, Infrastructure-to-Infrastructure, Adversary-to-Infrastructure, Infrastructure-to-Adversary, Bidirectional or Unknown.
	- Methodology - This meta-feature will allow an analyst to describe the general classification of intrusion, for example, phishing, DDoS, breach, port scan, etc. 
	- Resources - According to the Diamond Model, every intrusion event needs one or more external resources to be satisfied to succeed. Examples of the resources can include the following: software (e.g., operating systems, virtualization software, or Metasploit framework), knowledge (e.g., how to use Metasploit to execute the attack and run the exploit), information (e.g., a username/password to masquerade), hardware (e.g., servers, workstations, routers), funds (e.g., money to purchase domains), facilities (e.g., electricity or shelter), access (e.g., a network path from the source host to the victim and vice versa, network access from an Internet Service Provider (ISP)).
	
- The social-political component describes the needs and intent of the adversary, for example, financial gain, gaining acceptance in the hacker community, hacktivism, or espionage. 

- Technology – the technology meta-feature or component highlights the relationship between the core features: capability and infrastructure.








- MITRE researches in many areas, outside of cybersecurity, for the 'safety, stability, and well-being of our nation.'
 - These areas include artificial intelligence, health informatics, space security, to name a few.
  - "At MITRE, we solve problems for a safer world. Through our federally funded R&D centers and public-private partnerships, we work across government to tackle challenges to the safety, stability, and well-being of our nation."
  
- ATT&CK® (Adversarial Tactics, Techniques, and Common Knowledge) Framework
- CAR (Cyber Analytics Repository) Knowledge Base
- ENGAGE (sorry, not a fancy acronym)
- D3FEND (Detection, Denial, and Disruption Framework Empowering Network Defense)
- AEP (ATT&CK Emulation Plans)

- APT is an acronym for Advanced Persistent Threat.
 - This can be considered a team/group (threat group), or even country (nation-state group), that engages in long-term attacks against organizations and/or countries. 
 - The techniques these APT groups use are quite common and can be detected with the right implementations in place.
 - You can view FireEye's current list of APT groups: https://www.fireeye.com/current-threats/apt-groups.html
 
- TTP - Tactics, Techniques and Procedures 
 - The Tactic is the adversary's goal or objective.
 - The Technique is how the adversary achieves the goal or objective.
 - The Procedure is how the technique is executed.
 
- MITRE ATT&CK® is a globally-accessible knowledge base of adversary tactics and techniques based on real-world observations.

- In 2013, MITRE began to address the need to record and document common TTPs (Tactics, Techniques, and Procedures) that APT (Advanced Persistent Threat) groups used against enterprise Windows networks.
 -  This started with an internal project known as FMX (Fort Meade Experiment). Within this project, selected security professionals were tasked to emulated adversarial TTPs against a network, and data was collected from the attacks on this network. 
 -  The gathered data helped construct the beginning pieces of what we know today as the ATT&CK® framework.
 
- The ATT&CK® framework has grown and expanded throughout the years.
 - One notable expansion was that the framework focused solely on the Windows platform but has expanded to cover other platforms, such as macOS and Linux. 
 - https://attack.mitre.org/



-  CAR - Cyber Analytics Repository (CAR)
 - knowledge base of analytics developed by MITRE based on the MITRE ATT&CK® adversary model. CAR defines a data model that is leveraged in its pseudocode representations but also includes implementations directly targeted at specific tools (e.g., Splunk, EQL) in its analytics.
 - https://car.mitre.org/analytics/CAR-2020-09-001/
 - https://mitre-attack.github.io/attack-navigator/#layerURL=https://raw.githubusercontent.com/mitre-attack/car/master/docs/coverage/car_analytic_coverage_04_05_2022.json
  - To summarize, CAR is a great place for finding analytics that takes us further than the Mitigation and Detection summaries in the ATT&CK® framework. This tool is not a replacement for ATT&CK® but an added resource.
  
-  "MITRE Engage is a framework for planning and discussing adversary engagement operations that empowers you to engage your adversaries and achieve your cybersecurity goals."
 - MITRE Engage is considered an Adversary Engagement Approach.  
 - This is accomplished by the implementation of Cyber Denial and Cyber Deception.
 - With Cyber Denial we prevent the adversary's ability to conduct their operations and with Cyber Deception we intentionally plant artifacts to mislead the adversary. 



- D3FEND website, this resource is "A knowledge graph of cybersecurity countermeasures."
 - D3FEND is still in beta and is funded by the Cybersecurity Directorate of the NSA. 
 - D3FEND stands for Detection, Denial, and Disruption Framework Empowering Network Defense. 
 
 
 
- under MITRE ENGENUITY, we have CTID, the Adversary Emulation Library, and ATT&CK® Emulation Plans.
 - MITRE formed an organization named The Center of Threat-Informed Defense (CTID)
 - This organization consists of various companies and vendors from around the globe. 
 - Their objective is to conduct research on cyber threats and their TTPs and share this research to improve cyber defense for all. 
  - Some of the companies and vendors who are participants of CTID:
	- AttackIQ (founder)
	- Verizon
	- Microsoft (founder)
	- Red Canary (founder)
	- Splunk	
	 - The Adversary Emulation Library is a public library making adversary emulation plans a free resource for blue/red teamers.
	 - There are several ATT&CK® Emulation Plans currently available: APT3, APT29, and FIN6.
	 - The emulation plans are a step-by-step guide on how to mimic the specific threat group.
	 
	 
	 
	
- Threat Intelligence (TI) or Cyber Threat Intelligence (CTI) is the information, or TTPs, attributed to the adversary.
 - By using threat intelligence, as defenders, we can make better decisions regarding the defensive strategy. 
 - Some of this threat intel can be open source or through a subscription with a vendor, such as CrowdStrike.
  - In contrast, many defenders wear multiple hats (roles) within some organizations, and they need to take time from their other tasks to focus on threat intelligence. To cater to the latter, we'll work on a scenario of using ATT&CK® for threat intelligence. The goal of threat intelligence is to make the information actionable. 
 
 
- Cyber Defence Frameworks 
 - Pyramid of Pain
 - Cyber Kill Chain
 - Unified Kill Chain
 - Diamond Model
 - MITRE 
 
 
 
 
- Cyber Threat Intelligence (CTI) can be defined as evidence-based knowledge about adversaries, including their indicators, tactics, motivations, and actionable advice against them.
 - These can be utilised to protect critical assets and inform cyber security teams and management business decisions.
  - Data: Discrete indicators associated with an adversary, such as IP addresses, URLs or hashes.
  - Information: A combination of multiple data points that answer questions such as “How many times have employees accessed tryhackme.com within the month?”
  - Intelligence: The correlation of data and information to extract patterns of actions based on contextual analysis.
  
 - cyber threat context by trying to answer the following questions:
	Who’s attacking you?
	What are their motivations?
	What are their capabilities?
	What artefacts and indicators of compromise (IOCs) should you look out for?
	
 - Internal:
	Corporate security events such as vulnerability assessments and incident response reports.
	Cyber awareness training reports.
	System logs and events.
	
 - Community:
	Open web forums.
	Dark web communities for cybercriminals.

 - External
	Threat intel feeds (Commercial & Open-source)
	Online marketplaces.
	Public sources include government data, publications, social media, financial and industrial assessments.
	
	
- Threat Intelligence Classifications
 - Strategic Intel: High-level intel that looks into the organisation’s threat landscape and maps out the risk areas based on trends, patterns and emerging threats that may impact business decisions.
 
 - Technical Intel: Looks into evidence and artefacts of attack used by an adversary. Incident Response teams can use this intel to create a baseline attack surface to analyse and develop defence mechanisms.
 
 - Tactical Intel: Assesses adversaries’ tactics, techniques, and procedures (TTPs). This intel can strengthen security controls and address vulnerabilities through real-time investigations.
 
 - Operational Intel: Looks into an adversary’s specific motives and intent to perform an attack. Security teams may use this intel to understand the critical assets available in the organisation (people, processes and technologies) that may be targeted.
 

- Threat intel is obtained from a data-churning process that transforms raw data into contextualised and action-oriented insights geared towards triaging security incidents.
 - The transformational process follows a six-phase:
 
  - Direction - Definition of objectives and goals
  
  - Collection - Gather required data to address to (Normally, this phase is automated because the higher volume 
  
  - Processing - This phase ensures that the data is extracted, sorted, organised, correlated with appropriate tags and presented visually in a usable and understandable format to the analysts.
  
  - Analysis - Once the information aggregation is complete, security analysts must derive insights. Decisions to be made may involve:
	Investigating a potential threat through uncovering indicators and attack patterns.
	Defining an action plan to avert an attack and defend the infrastructure.
	Strengthening security controls or justifying investment for additional resources.
	
  - Dissemination
   - analysts will more likely inform the technical team about the threat IOCs, adversary TTPs and tactical action plans.
  
  - Feedback 
   - Feedback should be regular interaction between teams to keep the lifecycle working.
   
- The ATT&CK framework is a knowledge base of adversary behaviour, focusing on the indicators and tactics.

- The Trusted Automated eXchange of Indicator Information (TAXII) defines protocols for securely exchanging threat intel to have near real-time detection, prevention and mitigation of threats.
 - The protocol supports two sharing models:
  - Collection: Threat intel is collected and hosted by a producer upon request by users using a request-response model.
  - Channel: Threat intel is pushed to users from a central server through a publish-subscribe model.
  
- Structured Threat Information Expression (STIX) is a language developed for the "specification, capture, characterisation and communication of standardised cyber threat information".
 - It provides defined relationships between sets of threat info such as observables, indicators, adversary TTPs, attack campaigns, and more.
 
- Developed by Lockheed Martin, the Cyber Kill Chain breaks down adversary actions into steps. 
 - This breakdown helps analysts and defenders identify which stage-specific activities occurred when investigating an attack. 
 
- The diamond model looks at intrusion analysis and tracking attack groups over time.
 - It focuses on four key areas, each representing a different point on the diamond
  - Adversary 
  - Victim
  - Infrastructure
  - Capabilities 
  
  
- Reports 
 - https://www.mandiant.com/resources
 - https://www.recordedfuture.com/resources/global-issues
 - https://cybersecurity.att.com/
 
 
 
 
 
- Threat Intel is geared towards understanding the relationship between your operational environment and your adversary.
 - Strategic Intel: High-level intel that looks into the organisation's threat landscape and maps out the risk areas based on trends, patterns and emerging threats that may impact business decisions.
 
 - Technical Intel: Looks into evidence and artefacts of attack used by an adversary.
 
 - Tactical Intel: Assesses adversaries' tactics, techniques, and procedures (TTPs). This intel can strengthen security controls and address vulnerabilities through real-time investigations.
 
 - Operational Intel: Looks into an adversary's specific motives and intent to perform an attack. Security teams may use this intel to understand the critical assets available in the organisation (people, processes, and technologies) that may be targeted.
 
- Urlscan.io is a free service developed to assist in scanning and analysing websites.
 - It is used to automate the process of browsing and crawling through websites to record activities and interactions.
 
 
- Abuse.ch is a research project hosted by the Institue for Cybersecurity and Engineering at the Bern University of Applied Sciences in Switzerland.
 - It was developed to identify and track malware and botnets through several operational platforms developed under the project.
 - These platforms are:
  - Malware Bazaar:  this project is an all in one malware collection and analysis database
  - Feodo Tracker:  A resource used to track botnet command and control (C2) infrastructure linked with Emotet, Dridex and TrickBot.
  - SSL Blacklist:  developed this tool to identify and detect malicious SSL connections.
  - URL Haus:  this tool focuses on sharing malicious URLs used for malware distribution.
  - Threat Fox:  security analysts can search for, share and export indicators of compromise associated with malware.
  
  
- PhishTool seeks to elevate the perception of phishing as a severe form of attack and provide a responsive means of email security.
 - Through email analysis, security analysts can uncover email IOCs, prevent breaches and provide forensic reports that could be used in phishing containment and training engagements.
  - Perform email analysis: PhishTool retrieves metadata from phishing emails and provides analysts with the relevant explanations and capabilities to follow the email’s actions, attachments, and URLs to triage the situation.
  
  - Heuristic intelligence: OSINT is baked into the tool to provide analysts with the intelligence needed to stay ahead of persistent attacks and understand what TTPs were used to evade security controls and allow the adversary to social engineer a target.
  
  - Classification and reporting: Phishing email classifications are conducted to allow analysts to take action quickly. Additionally, reports can be generated to provide a forensic record that can be shared.
  
  
  
  
  
- Cisco assembled a large team of security practitioners called Cisco Talos to provide actionable intelligence, visibility on indicators, and protection against emerging threats through data collected from their products. 
 - https://talosintelligence.com/
 - Cisco Talos encompasses six key teams:
  - Threat Intelligence & Interdiction: Quick correlation and tracking of threats provide a means to turn simple IOCs into context-rich intel.
  - Detection Research: Vulnerability and malware analysis is performed to create rules and content for threat detection.
  - Engineering & Development: Provides the maintenance support for the inspection engines and keeps them up-to-date to identify and triage emerging threats.
  - Vulnerability Research & Discovery: Working with service and software vendors to develop repeatable means of identifying and reporting security vulnerabilities.
  - Communities: Maintains the image of the team and the open-source solutions.
  - Global Outreach: Disseminates intelligence to customers and the security community through publications.
  
  
  
  
  
- Yara (Yet Another Ridiculous Acronym) 
 - https://github.com/virustotal/yara
 - "The pattern matching swiss knife for malware researchers (and everyone else)" 
  - Yara can identify information based on both binary and textual patterns, such as hexadecimal and strings contained within a file.
  - For example, Yara rules are frequently written to determine if a file is malicious or not, based upon the features - or patterns - it presents.
  - Malware, just like our "Hello World" application, uses strings to store textual data
   - Frameworks such as the Cuckoo Sandbox or Python's PE Module allow you to improve the technicality of your Yara rules ten-fold.
   
  - Cuckoo Sandbox is an automated malware analysis environment. 
   - This module allows you to generate Yara rules based upon the behaviours discovered from Cuckoo Sandbox.
   
  - Python's PE module allows you to create Yara rules from the various sections and elements of the Windows Portable Executable (PE) structure.

 - Knowing how to create custom Yara rules is useful, but luckily you don't have to create many rules from scratch to begin using Yara to search for evil. 
  - There are plenty of GitHub resources and open-source tools (along with commercial products) that can be utilized to leverage Yara in hunt operations and/or incident response engagements. 
  
- LOKI is a free open-source IOC (Indicator of Compromise) scanner created/written by Florian Roth.
 - Based on the GitHub page, detection is based on 4 methods:
  - File Name IOC Check
  - Yara Rule Check (we are here)
  - Hash Check
  - C2 Back Connect Check
   - https://github.com/Neo23x0/Loki/blob/master/README.md
   
- THOR Lite is Florian's newest multi-platform IOC AND YARA scanner.
 -  A nice feature with THOR Lite is its scan throttling to limit exhausting CPU resources.
 - https://www.nextron-systems.com/thor-lite/
 
- FENRIR
 - The updated version was created to address the issue from its predecessors, where requirements must be met for them to function
 - Fenrir is a bash script; it will run on any system capable of running bash (nowadays even Windows). 
 
- YAYA was created by the EFF (Electronic Frontier Foundation) and released in September 2020.
 - "YAYA is a new open-source tool to help researchers manage multiple YARA rule repositories. YAYA starts by importing a set of high-quality YARA rules and then lets researchers add their own rules, disable specific rulesets, and run scans of files."
 
 
- As a security analyst, you may need to research various threat intelligence reports, blog postings, etc. and gather information on the latest tactics and techniques used in the wild, past or present.
 -  Typically in these readings, IOCs (hashes, IP addresses, domain names, etc.) will be shared so rules can be created to detect these threats in your environment, along with Yara rules.
 - On the flip side, you might find yourself in a situation where you've encountered something unknown, that your security stack of tools can't/didn't detect.
  - Using tools such as Loki, you will need to add your own rules based on your threat intelligence gathers or findings from an incident response engagement (forensics). 
  
  
- yarGen is a generator for YARA rules.
 - Use yarGen to generate a Yara rule for files 
 
 
- Valhalla is an online Yara feed created and hosted by Nextron-Systems (erm, Florian Roth)
 - https://www.nextron-systems.com/valhalla/
 
 
 
 
 
- OpenCTI, an open-source threat intelligence platform
- OpenCTI is another open-sourced platform designed to provide organisations with the means to manage CTI through the storage, analysis, visualisation and presentation of threat campaigns, malware and IOCs.
- the platform's main objective is to create a comprehensive tool that allows users to capitalise on technical and non-technical information while developing relationships between each piece of information and its primary source.

- OpenCTI uses a variety of knowledge schemas in structuring data, the main one being the Structured Threat Information Expression (STIX2) standards.
- TIX is a serialised and standardised language format used in threat intelligence exchange.
- It allows for the data to be implemented as entities and relationships, effectively tracing the origin of the provided information.

- According to OpenCTI, connectors fall under the following classes:
- External Input Connector       Ingests information from external sources                               CVE, MISP, TheHive, MITRE
- Stream Connector               Consumes platform data stream                                           History, Tanium
- Internal Enrichment Connecto   Takes in new OpenCTI entities from user requests                        Observables enrichment
- Internal Import File Connector  Extracts information from uploaded reports                             PDFs, STIX2 Import
- Internal Export File Connector  Exports information from OpenCTI into different file formats           CSV, STIX2 export, PDF

- https://github.com/OpenCTI-Platform/connectors
- https://luatix.notion.site/Data-model-4427344d93a74fe194d5a52ce4a41a8d



- MISP Malware & Threat Sharing Platform
 - https://www.misp-project.org/
 
- open-source threat information platform that facilitates the collection, storage and distribution of threat intelligence and Indicators of Compromise (IOCs) related to malware, cyber attacks, financial fraud or any intelligence within a community of trusted members. 

- MISP is effectively useful for the following use cases:
 - Malware Reverse Engineering: Sharing of malware indicators to understand how different malware families function.
 - Security Investigations: Searching, validating and using indicators in investigating security breaches.
 - Intelligence Analysis: Gathering information about adversary groups and their capabilities.
 - Law Enforcement: Using Indicators to support forensic investigations.
 - Risk Analysis: Researching new threats, their likelihood and occurrences.
 - Fraud Analysis: Sharing of financial indicators to detect financial fraud.
 
 - MISP provides the following core functionalities: 
  - IOC database: This allows for the storage of technical and non-technical information about malware samples, incidents, attackers and intelligence.
  - Automatic Correlation: Identification of relationships between attributes and indicators from malware, attack campaigns or analysis.
  - Data Sharing: This allows for sharing of information using different models of distributions and among different MISP instances.
  - Import & Export Features: This allows the import and export of events in different formats to integrate other systems such as NIDS, HIDS, and OpenIOC.
  - Event Graph: Showcases the relationships between objects and attributes identified from events.
  - API support: Supports integration with own systems to fetch and export events and intelligence.
  
- The following terms are commonly used within MISP and are related to the functionalities described above and the general usage of the platform:
 - Events: Collection of contextually linked information.
 - Attributes: Individual data points associated with an event, such as network or system indicators.
 - Objects: Custom attribute compositions.
 - Object References: Relationships between different objects.
 - Sightings: Time-specific occurrences of a given data point or attribute detected to provide more credibility.
 - Tags: Labels attached to events/attributes.
 - Taxonomies: Classification libraries are used to tag, classify and organise information.
 - Galaxies: Knowledge base items used to label events/attributes.
 - Indicators: Pieces of information that can detect suspicious or malicious cyber activity.
 
- A taxonomy is a means of classifying information based on standard features or attributes.
 - Set events for further processing by external tools such as VirusTotal.
 - Ensure events are classified appropriately before the Organisation Admin publishes them.
 - Enrich intrusion detection systems' export values with tags that fit specific deployments.
 
- Taxonomies are expressed in machine tags, which comprise three vital parts:
 - Namespace: Defines the tag's property to be used.
 - Predicate: Specifies the property attached to the data.
 - Value: Numerical or text details to map the property.
 
- The following tags can be considered a must-have to provide a well-defined event for distribution:
 - Traffic Light Protocol: Provides a colour schema to guide how intelligence can be shared.
 - Confidence: Provides an indication as to whether or not the data being shared is of high quality and has been vetted so that it can be trusted to be good for immediate usage.
 - Origin: Describes the source of information and whether it was from automation or manual investigation.
 - Permissible Actions Protocol: An advanced classification that indicates how the data can be used to search for compromises within the organisation.
  - https://www.circl.lu/
  







  
  
  
  
- Typical Network Security Management Operation is explained in the given table:
 - 

- Network Security is a set of operations for protecting data, applications, devices and systems connected to the network.
 - It focuses on the system design, operation and management of the architecture/infrastructure to provide network accessibility, integrity, continuity and reliability
 - Traffic analysis (often called Network Traffic Analysis) is a subdomain of the Network Security domain, and its primary focus is investigating the network data to identify problems and anomalies. 
 
- The essential concern of Network Security focuses on two core concepts: 
 - Authentication 
 - Authorisation.
 
- Network security operations contain three base control levels to ensure the maximum available security management.
 - Physical - Physical security controls prevent unauthorised physical access to networking devices, cable boards, locks, and all linked components.
 - Technical - Data security controls prevent unauthorised access to network data, like installing tunnels and implementing security layers.
 - Administrative - Administrative security controls provide consistency in security operations like creating policies, access levels and authentication processes.
 
- There are two main approaches and multiple elements under these control levels.
 - Access Control - The starting point of Network Security. It is a set of controls to ensure authentication and authorisation. 
 - Threat Control - Detecting and preventing anomalous/malicious activities on the network. It contains both internal (trusted) and external traffic data probes.
 
 
 
 
 
- The key elements of Access Control:

- Firewall Protection
 - Controls incoming and outgoing network traffic with predetermined security rules. 
 - Designed to block suspicious/malicious traffic and application-layer threats while allowing legitimate and expected traffic.
 
- Network Access Control (NAC)
 - Controls the devices' suitability before access to the network. 
 - Designed to verify device specifications and conditions are compliant with the predetermined profile before connecting to the network.

- Identity and Access Management (IAM)	
 - Controls and manages the asset identities and user access to data systems and resources over the network.
 
- Load Balancing	
 - Controls the resource usage to distribute (based on metrics) tasks over a set of resources and improve overall data processing flow.
 
- Network Segmentation
 - Creates and controls network ranges and segmentation to isolate the users' access levels, group assets with common functionalities, and improve the protection of sensitive/internal devices/data in a safer network.
 
- Virtual Private Networks (VPN)
 - Creates and controls encrypted communication between devices (typically for secure remote access) over the network (including communications over the internet).
 
- Zero Trust Model	
 - Suggests configuring and implementing the access and permissions at a minimum level (providing access required to fulfil the assigned role). The mindset is focused on: "Never trust, always verify".
 
 
 
 
 
- The key elements of Threat Control:
 - Intrusion Detection and Prevention (IDS/IPS) - Inspects the traffic and creates alerts (IDS) or resets the connection (IPS) when detecting an anomaly/threat.
 
 - Data Loss Prevention (DLP) - Inspects the traffic (performs content inspection and contextual analysis of the data on the wire) and blocks the extraction of sensitive data.
 
 - Endpoint Protection - Protecting all kinds of endpoints and appliances that connect to the network by using a multi-layered approach like encryption, antivirus, antimalware, DLP, and IDS/IPS.
 
 - Cloud Security - Protecting cloud/online-based systems resources from threats and data leakage by applying suitable countermeasures like VPN and data encryption.
 
 - Security Information and Event Management (SIEM) - Technology that helps threat detection, compliance, and security incident management, through available data (logs and traffic statistics) by using event and context analysis to identify anomalies, threats, and vulnerabilities.
 
 - Security Orchestration Automation and Response (SOAR) - Technology that helps coordinate and automates tasks between various people, tools, and data within a single platform to identify anomalies, threats, and vulnerabilities. It also supports vulnerability management, incident response, and security operations.
 
 - Network Traffic Analysis & Network Detection and Response
  - Inspecting network traffic or traffic capture to identify anomalies and threats.
  
  
  
  
- Typical Network Security Management Operation is explained in the given table:
 - Deployment
  - Device and software installation
  - Initial configuration
  - Automation
  
 - Configuration
  - Feature configuration
  - Initial network access configuration
  
 - Management
  - Security policy implementation
  - NAT and VPN implementation
  - Threat mitigation
  
 - Monitoring
  - System monitoring
  - User activity monitoring
  - Threat monitoring
  - Log and traffic sample capturing
  
 - Maintenance
  - Upgrades
  - Security updates
  - Rule adjustments
  - Licence management
  - Configuration updates
  
  
- Managed Security Services (MSS)
 - MSS are services that have been outsourced to service providers.
 - These service providers are called Managed Security Service Providers (MSSPs)
  - There are various elements of MSS, and the most common ones are explained below.
   
  - Network Penetration Testing - Assessing network security by simulating external/internal attacker techniques to breach the network.
  - Vulnerability Assessment - Assessing network security by discovering and analysing vulnerabilities in the environment.
  - Incident Response - An organised approach to addressing and managing a security breach. It contains a set of actions to identify, contain, and eliminate incidents.
  - Behavioural Analysis - An organised approach to addressing system and user behaviours, creating baselines and traffic profiles for specific patterns to detect anomalies, threats, vulnerabilities, and attacks.
  
  
  
- Traffic Analysis is a method of intercepting, recording/monitoring, and analysing network data and communication patterns to detect and respond to system health issues, network anomalies, and threats.
 - The network is a rich data source, so traffic analysis is useful for security and operational matters. 


- Traffic analysis is one of the essential approaches used in network security, and it is part of multiple disciplines of network security operations listed below:
 - Network Sniffing and Packet Analysis
 - Network Monitoring
 - Intrusion Detection and Prevention
 - Network Forensics
 - Threat Hunting
 
- Flow Analysis
 - Collecting data/evidence from the networking devices.
 - This type of analysis aims to provide statistical results through the data summary without applying in-depth packet-level investigation.
  - Advantage: Easy to collect and analyse.
  - Challenge: Doesn't provide full packet details to get the root cause of a case.
  
- Packet Analysis
 - Collecting all available network data.
 - Applying in-depth packet-level investigation (often called Deep Packet Inspection (DPI) ) to detect and block anomalous and malicious packets.
  - Advantage: Provides full packet details to get the root cause of a case.
  - Challenge: Requires time and skillset to analyse.
  
- Benefits of the Traffic Analysis:
 - Provides full network visibility.
 - Helps comprehensive baselining for asset tracking.
 - Helps to detect/respond to anomalies and threats.
  - Even if it is encoded/encrypted, it still provides a value by pointing to an odd, weird or unexpected pattern/situation.
  - Therefore traffic analysis is still a must-to-have skill for any security analyst who wants to detect and respond to advanced threats.
  
  
  
  
  
  
  
  
  
  
  
  
  
- SNORT is an open-source, rule-based Network Intrusion Detection and Prevention System (NIDS/NIPS)
 - It was developed and still maintained by Martin Roesch, open-source contributors, and the Cisco Talos team. 
 
- Intrusion Detection System (IDS) 
 - IDS is a passive monitoring solution for detecting possible malicious activities/patterns, abnormal incidents, and policy violations.
 - It is responsible for generating alerts for each suspicious event. 
  - There are two main types of IDS systems;
  
   - Network Intrusion Detection System (NIDS)
    - NIDS monitors the traffic flow from various areas of the network.
	- The aim is to investigate the traffic on the entire subnet. 
	- If a signature is identified, an alert is created.
	
   - Host-based Intrusion Detection System (HIDS)
    - HIDS monitors the traffic flow from a single endpoint device.
	- The aim is to investigate the traffic on a particular device. 
	- If a signature is identified, an alert is created.
 
- Intrusion Prevention System (IPS)
 - IPS is an active protecting solution for preventing possible malicious activities/patterns, abnormal incidents, and policy violations. 
 - It is responsible for stopping/preventing/terminating the suspicious event as soon as the detection is performed.
  -  There are four main types of IPS systems;
  
  - Network Intrusion Prevention System (NIPS) 
   - NIPS monitors the traffic flow from various areas of the network. The aim is to protect the traffic on the entire subnet.
   - If a signature is identified, the connection is terminated.
   
  - Behaviour-based Intrusion Prevention System (Network Behaviour Analysis - NBA)
   - Behaviour-based systems monitor the traffic flow from various areas of the network.
   - The aim is to protect the traffic on the entire subnet. If a signature is identified, the connection is terminated.
  
  - Wireless Intrusion Prevention System (WIPS)
   - WIPS monitors the traffic flow from of wireless network.
   - The aim is to protect the wireless traffic and stop possible attacks launched from there.
   - If a signature is identified, the connection is terminated.
  
  - Host-based Intrusion Prevention System (HIPS)
   - HIPS actively protects the traffic flow from a single endpoint device.
   - The aim is to investigate the traffic on a particular device.
   - If a signature is identified, the connection is terminated.
   
- while HIDS creates alerts for threats, HIPS stops the threats by terminating the connection.

- There are three main detection and prevention techniques used in IDS and IPS solutions;

  - Signature-Based - This technique relies on rules that identify the specific patterns of the known malicious behaviour. This model helps detect known threats. 
  
  - Behaviour-Based - This technique identifies new threats with new patterns that pass through signatures. The model compares the known/normal with unknown/abnormal behaviours.
  
  - Policy-Based - This technique compares detected activities with system configuration and security policies. This model helps detect policy violations.
  
- IDS can identify threats but require user assistance to stop them.
- IPS can identify and block the threats with less user assistance at the detection time.

- SNORT is an open-source, rule-based Network Intrusion Detection and Prevention System (NIDS/NIPS). 


- Snort has three main use models;
 - Sniffer Mode - Read IP packets and prompt them in the console application.
 
 - Packet Logger Mode - Log all IP packets (inbound and outbound) that visit the network.
 
 - NIDS (Network Intrusion Detection System)  and NIPS (Network Intrusion Prevention System) Modes 
  - Log/drop the packets that are deemed as malicious according to the user-defined rules.
  
  
  
  









- NetworkMiner is an open-source traffic sniffer, pcap handler and protocol analyser.
- NetworkMiner is an open source Network Forensic Analysis Tool (NFAT) for Windows (but also works in Linux / Mac OS X / FreeBSD)
- NetworkMiner can be used as a passive network sniffer/packet capturing tool to detect operating systems, sessions, hostnames, open ports etc. 
  - without putting any traffic on the network. NetworkMiner can also parse PCAP files for off-line analysis and to regenerate/reassemble transmitted files and certificates from PCAP files.
  
- Network Traffic Analysis (NTA)
 
 
- Network Forensics is a specific subdomain of the Forensics domain, and it focuses on network traffic investigation.
-  Network Forensics discipline covers the work done to access information transmitted by listening and investigating live and recorded traffic, gathering evidence/artefacts and understanding potential problems. 
- The ultimate goal is to provide sufficient information to detect malicious activities, security breaches, policy/regulation compliance, system health and user behaviour.

- The investigation tries to answer the 5W;
- Who (Source IP and port)
- What (Data/payload)
- Where (Destination IP and port)
- When (Time and data)
- Why (How/What happened)

- Having enough data and the right timeline capture for a successful network forensics investigation is crucial.
 
- The most common network forensics use cases are explained below;
- Network discovery: Discovering the network to overview connected devices, rogue hosts and network load. 
- Packets reassembling: Reassembling the packets to investigate the traffic flow. This use case is helpful in unencrypted traffic flows.
- Data leakage detection: Reviewing packet transfer rates for each host and destination address helps detect possible data leakage. 
- Anomaly and malicious activity detection: Reviewing overall network load by focusing on used ports, source and destination addresses, and data helps detect possible malicious activities along with vulnerabilities
- Policy/Regulation compliance control: Reviewing overall network behaviour helps detect policy/regulation compliance.


- General advantages of network forensics are explained below;
- Availability of network-based evidence in the wild: Capturing network traffic is collecting evidence, so it is easier than other types of evidence collections such as logs and IOCs.
- Ease of data/evidence collection without creating noise: Capturing and working with network traffic is easier than investigating unfiltered events by EDRs, EPPs and log systems.
- It is hard to destroy the network evidence, as it is the transferred data: Since the evidence is the traffic itself, it is impossible to do anything without creating network noise.
- Availability of log sources: Logs provide valuable information which helps to correlate the chain of events and support the investigation hypothesis.
- It is possible to gather evidence for memory and non-residential malicious activities: The malware/threat might reside in the memory to avoid detection.

- There are multiple evidence resources to gather network forensics data.
- TAPS
- InLine Devices
- SPAN Ports
- Hubs
- Switches
- Routers
- DHCP Servers
- Name Servers
- Authentication Servers
- Firewalls
- Web Proxies
- Central Log Servers
- Logs (IDS/IPS, Application, OS, Device)

- Primary Purposes of Network Forensics 
- Security Operations (SOC): Daily security monitoring activities on system performance and health, user behaviour, and security issues.
- Incident Handling/Response and Threat Hunting: During/Post-incident investigation activities on understanding the reason for the incident, detecting malicious and suspicious activity, and investigating the data flow content.


- There are three main data types investigated in Network Forensics
- Live Traffic
- Traffic Captures (full packet captures and network flows)
- Log Files

- There are two main operating modes;
- Sniffer Mode: Although it has a sniffing feature, it is not intended to use as a sniffer.
- Packet Parsing/Processing: NetworkMiner can parse traffic captures to have a quick overview and information on the investigated capture.



- Pros
- OS fingerprinting
- Easy file extraction
- Credential grabbing
- Clear text keyword parsing
- Overall overview


- Cons
- Not useful in active sniffing
- Not useful for large pcap investigation
- Limited filtering
- Not built for manual traffic investigation






- Zeek (formerly Bro) is an open-source and commercial network monitoring tool (traffic analyser).
 - Zeek is a passive, open-source network traffic analyser. 
 - Many operators use Zeek as a network security monitor (NSM) to support suspicious or malicious activity investigations. 

- Network monitoring is highly focused on IT assets like uptime (availability), device health and connection quality (performance), and network traffic balance and management (configuration). 
 - Monitoring and visualising the network traffic, troubleshooting, and root cause analysis are also part of the Network Monitoring process.
 
- Network Security Monitoring is focused on network anomalies like rogue hosts, encrypted traffic, suspicious service and port usage, and malicious/suspicious traffic patterns in an intrusion/anomaly detection and response approach.
 - This model is helpful for security analysts/incident responders, security engineers and threat hunters and covers identifying threats, vulnerabilities and security issues with a set of rules, signatures and patterns.

- Network Security Monitoring is part of the SOC, and the actions are separated between tier 1-2-3 analyst levels


- Zeek differs from known monitoring and IDS/IPS tools by providing a wide range of detailed logs ready to investigate both for forensics and data analysis actions.
 - Currently, Zeek provides 50+ logs in 7 categories.
 
- Zeek has two primary layers; "Event Engine" and "Policy Script Interpreter"
 -  The Event Engine layer is where the
 - It is where the packages are divided into parts such as source and destination addresses, protocol identification, session analysis and file extraction
 
 - The Policy Script Interpreter layer is where the semantic analysis is conducted. 
  - It is responsible for describing the event correlations by using Zeek scripts.

- Zeek has several frameworks to provide extended functionality in the scripting layer. 
 - These frameworks enhance Zeek's flexibility and compatibility with other network components.
 - Each framework focuses on the specific use case and easily runs with Zeek installation.
 
- Frameworks:

Logging	          Notice	          Input	     Configuration	Intelligence
Cluster	    Broker Communication	Supervisor	 GeoLocation	File Analysis
Signature	Summary	NetControl	Packet Analysis	 TLS Decryption


- Once you run Zeek, it will automatically start investigating the traffic or the given pcap file and generate logs automatically.


- There are two operation options for Zeek.
 - The first one is running it as a service

 - The second option is running the Zeek against a pcap
 
- Zeek generates log files according to the traffic data.
 - You will have logs for every connection in the wire, including the application level protocols and fields. 
 
 
- Zeek Logs: 
 - Network - Network protocol logs.
 - Files - File analysis result logs.
 - NetControl - Network control and flow logs.
 - Detection - Detection and possible indicator logs.
 - Network Observations - Network flow logs.
 - Miscellaneous - Additional logs cover external alerts, inputs and failures.
 - Zeek Diagnostic - Zeek diagnostic logs cover system messages, actions and some statistics.
 
 
- Zeek has its own event-driven scripting language, which is as powerful as high-level languages and allows us to investigate and correlate the detected events.
 - Since it is as capable as high-level programming languages, you will need to spend time on Zeek scripting language in order to become proficient. 

- Zeek has base scripts installed by default, and these are not intended to be modified -> "/opt/zeek/share/zeek/base".
- User-generated or modified scripts should be located in a specific path.  -> "/opt/zeek/share/zeek/site".
- Policy scripts are located in a specific path.  -> "/opt/zeek/share/zeek/policy".
- To automatically load/use a script in live sniffing mode, you must identify the script in the Zeek configuration file. -> "/opt/zeek/share/zeek/site/local.zeek".

- Zeek scripts use the ".zeek" extension.
 - Do not modify anything under the "zeek/base" directory. 
 - User-generated and modified scripts should be in the "zeek/site" directory.
  - Zeek is event-oriented, not packet-oriented! We need to use/write scripts to handle the event of interest.
  
- You can learn and practice the Zeek scripting language by using -> https://try.bro.org/#/?example=hello




- Scripts contain operators, types, attributes, declarations and statements, and directives.


- Zeek has 15+ frameworks that help analysts to discover the different events of interest.
 - You can find and read more on the prebuilt scripts and frameworks by visiting Zeek's online book -> 	https://docs.zeek.org/en/master/frameworks/index.html
 
 
 
 
 
 
 
 
- BRIM is an open-source desktop application that processes pcap files and logs files.
- Its primary focus is providing search and analytics.
  - It uses the Zeek log processing format.
  - It also supports Zeek signatures and Suricata Rules for detection.
  
- It can handle two types of data as an input;
- Packet Capture Files: Pcap files created with tcpdump, tshark and Wireshark like applications.
- Log Files: Structured log files like Zeek logs.
 
 
- Brim is built on open-source platforms:
- Zeek: Log generating engine.
- Zed Language: Log querying language that allows performing keywoırd searches with filters and pipelines.
- ZNG Data Format: Data storage format that supports saving data streams.
- Electron and React: Cross-platform UI.

- Brim reduces the time and effort spent processing pcap files and investigating the log files by providing a simple and powerful GUI application.
 
- The common best practice is handling medium-sized pcaps with Wireshark, creating logs and correlating events with Zeek, and processing multiple logs in Brim.








- Wireshark is one of the most potent traffic analyser tools available in the wild. There are multiple purposes for its use:
 - Detecting and troubleshooting network problems, such as network load failure points and congestion.
 - Detecting security anomalies, such as rogue hosts, abnormal port usage, and suspicious traffic.
 - Investigating and learning protocol details, such as response codes and payload data. 
  - Wireshark is not an Intrusion Detection System (IDS). It only allows analysts to discover and investigate the packets in depth.
  
  
- Packet dissection is also known as protocol dissection, which investigates packet details by decoding available protocols and fields.
 - Wireshark supports a long list of protocols for dissection, and you can also write your dissection scripts. 
 - https://github.com/boundary/wireshark/blob/master/doc/README.dissector
 
- Wireshark uses OSI layers to break down packets

- Wireshark calculates the number of investigated packets and assigns a unique number for each packet. 

- Wireshark also detects specific states of protocols to help analysts easily spot possible anomalies and problems. 
 -  Chat	Blue	   Information on usual workflow.
    Note	Cyan	   Notable events like application error codes.
    Warn	Yellow	   Warnings like unusual error codes or problem statements.
    Error	Red	       Problems like malformed packets.








- ARP protocol, or Address Resolution Protocol (ARP), is the technology responsible for allowing devices to identify themselves on a network. 

- Address Resolution Protocol Poisoning (also known as ARP Spoofing or Man In The Middle (MITM) attack) 
 - type of attack that involves network jamming/manipulating by sending malicious ARP packets to the default gateway.
 - The ultimate aim is to manipulate the "IP to MAC address table" and sniff the traffic of the target host.
 
 
- ARP analysis in a nutshell:
 - Works on the local network
 - Enables the communication between MAC addresses
 - Not a secure protocol
 - Not a routable protocol
 - It doesn't have an authentication function
 - Common patterns are request & response, announcement and gratuitous packets.





- When investigating a compromise or malware infection activity, a security analyst should know how to identify the hosts on the network apart from IP to MAC address match.
 - One of the best methods is identifying the hosts and users on the network to decide the investigation's starting point and list the hosts and users associated with the malicious traffic/activity.
 - For a security analyst, it is still essential to have host and user identification skills. 

- Protocols that can be used in Host and User identification:
 - Dynamic Host Configuration Protocol (DHCP) traffic
 - NetBIOS (NBNS) traffic 
 - Kerberos traffic
 
- Dynamic Host Configuration Protocol (DHCP), is the technology responsible for managing automatic IP address and required communication parameters assignment.



- NetBIOS or Network Basic Input/Output System is the technology responsible for allowing applications on different hosts to communicate with each other. 

- Kerberos is the default authentication service for Microsoft Windows domains.
 - It is responsible for authenticating service requests between two or more computers over the untrusted network. The ultimate aim is to prove identity securely.
 
 
 
 
 
- Traffic tunnelling is (also known as "port forwarding") transferring the data/resources in a secure method to network segments and zones.
 - It can be used for "internet to private networks" and "private networks to internet" flow/direction.
 - There is an encapsulation process to hide the data, it contains private data packets and transfers them to the final destination securely.
 
- Tunnelling provides anonymity and traffic security. 
 - attackers use tunnelling to bypass security perimeters using the standard and trusted protocols used in everyday traffic like ICMP and DNS.
 - for a security analyst, it is crucial to have the ability to spot ICMP and DNS anomalies.

- Internet Control Message Protocol (ICMP) is designed for diagnosing and reporting network communication issues.
 - It is highly used in error reporting and testing.
  - As it is a trusted network layer protocol, sometimes it is used for denial of service (DoS) attacks; 
  - Also, adversaries use it in data exfiltration and C2 tunnelling activities.
  
- Usually, ICMP tunnelling attacks are anomalies appearing/starting after a malware execution or vulnerability exploitation. 
 - As the ICMP packets can transfer an additional data payload, adversaries use this section to exfiltrate data and establish a C2 connection
 - It could be a TCP, HTTP or SSH data
  - Most enterprise networks block custom packets or require administrator privileges to create custom ICMP packets.


- Domain Name System (DNS) is designed to translate/convert IP domain addresses to IP addresses.
 - As it is the essential part of web services, it is commonly used and trusted, and therefore often ignored.
 - Due to that, adversaries use it in data exfiltration and C2 activities.












- To learn more about Core Windows Processes, a built-in Windows tool named Task Manager may aid us in understanding the underlying processes inside a Windows machine.  
 - Task Manager is a built-in GUI-based Windows utility that allows users to see what is running on the Windows system.


- The Sysinternals tools are a compilation of over 70+ Windows-based tools. Each of the tools falls into one of the following categories:
 - File and Disk Utilities
 - Networking Utilities
 - Process Utilities
 - Security Utilities
 - System Information
 - Miscellaneous


- two of the most used Sysinternals tools for endpoint investigation for this task.
 - TCPView - Networking Utility tool.
 - Process Explorer - Process Utility tool.
 
- "TCPView is a Windows program that will show you detailed listings of all TCP and UDP endpoints on your system, including the local and remote addresses and state of TCP connections.

- Process Explorer enables you to inspect the details of a running process, such as:
 - Associated services
 - Invoked network traffic
 - Handles such as files or directories opened
 - DLLs and memory-mapped files loaded
 
- the importance of endpoint logging, which enables us to audit significant events across different endpoints, collect and aggregate them for searching capabilities, and better automate the detection of anomalies.

- The Windows Event Logs are not text files that can be viewed using a text editor. However, the raw data can be translated into XML using the Windows API. 
 - The events in these log files are stored in a proprietary binary format with a .evt or .evtx extension.
 - The log files with the .evtx file extension typically reside in C:\Windows\System32\winevt\Logs.
 
- There are three main ways of accessing these event logs within a Windows system:
 - Event Viewer (GUI-based application)
 - Wevtutil.exe (command-line tool)
 - Get-WinEvent (PowerShell cmdlet)
 
 
 
- Sysmon, a tool used to monitor and log events on Windows, is commonly used by enterprises as part of their monitoring and logging solutions. 
 - As part of the Windows Sysinternals package, Sysmon is similar to Windows Event Logs with further detail and granular control. 
 
- Sysmon gathers detailed and high-quality logs as well as event tracing that assists in identifying anomalies in your environment.
 - It is commonly used with a security information and event management (SIEM) system or other log parsing solutions that aggregate, filter, and visualize events. 
 
 
- Osquery is an open-source tool created by Facebook.
 - With Osquery, Security Analysts, Incident Responders, and Threat Hunters can query an endpoint (or multiple endpoints) using SQL syntax. 
 - Osquery only allows you to query events inside the machine.
  - with Kolide Fleet, you can query multiple endpoints from the Kolide Fleet UI instead of using Osquery locally to query an endpoint. 
  
- ﻿Wazuh is an open-source, freely available, and extensive EDR solution, which Security Engineers can deploy in all scales of environments.
 - Wazuh operates on a management and agent model where a dedicated manager device is responsible for managing agents installed on the devices you'd like to monitor.
 
- Endpoint detection and response (EDR) 
 - are tools and applications that monitor devices for an activity that could indicate a threat or security breach. These tools and applications have features that include:
 
 - Auditing a device for common vulnerabilities
 - Proactively monitoring a device for suspicious activity such as unauthorized logins, brute-force attacks, or privilege escalations.
 - Visualizing complex data and events into neat and trendy graphs
 - Recording a device's normal operating behaviour to help with detecting anomalies
 
 
- Event correlation identifies significant relationships from multiple log sources such as application logs, endpoint logs, and network logs.
 - Event correlation deals with identifying significant artefacts co-existing from different log sources and connecting each related artefact. 
 - Event correlation can build the puzzle pieces to complete the exact scenario from an investigation.
 
- Baselining is the process of knowing what is expected to be normal.











- The Sysinternals tools is a compilation of over 70+ Windows-based tools. Each of the tools falls into one of the following categories:
 - File and Disk Utilities
 - Networking Utilities
 - Process Utilities
 - Security Utilities
 - System Information
 - Miscellaneous
 
- The Sysinternals tools and its website (sysinternals.com) were created by Mark Russinovich in the late '90s, along Bryce Cogswell under the company Wininternals Software.

- add the folder path to the environment variables. By doing so, you can launch the tools via the command line without navigating to the directory the tools reside in. 
 - The System Properties can be launched via the command line by running sysdm.cpl
 
 
- Alternatively, a PowerShell module can download and install all of the Sysinternals tools. 
 - PowerShell command: Download-SysInternalsTools C:\Tools\Sysint 
 
 
- Sysinternals Live is a service that enables you to execute Sysinternals tools directly from the Web without hunting for and manually downloading them.
 - Simply enter a tool's Sysinternals Live path into Windows Explorer or a command prompt as live.sysinternals.com/<toolname> or \\live.sysinternals.com\tools\<toolname>."
  - the WebDAV client must be installed and running on the machine.
  - The WebDAV protocol is what allows a local machine to access a remote machine running a WebDAV share and perform actions in it.
   - Instalar com powershell: Install-WindowsFeature WebDAV-Redirector –Restart
  
  - Pode ser necessário verificar se o serviço está rodando: get-service webclient 
  - Network Discovery needs to be enabled as well. This setting can be enabled in the Network and Sharing Center.
  
  
- Sigcheck is a command-line utility that shows file version number, timestamp information, and digital signature details, including certificate chains. It also includes an option to check a file’s status on VirusTotal, a site that performs automated file scanning against over 40 antivirus engines, and an option to upload a file for scanning

- Stream - "The NTFS file system provides applications the ability to create alternate data streams of information. By default, all data is stored in a file's main unnamed data stream, but by using the syntax 'file:stream', you are able to read and write to alternates."

- SDelete is a command line utility that takes a number of options. In any given use, it allows you to delete one or more files and/or directories, or to cleanse the free space on a logical disk
 - SDelete has been used by adversaries and is associated with MITRE techniques T1485 (Data Destruction) and T1070.004 (Indicator Removal on Host: File Deletion)
 
https://docs.microsoft.com/en-us/sysinternals/downloads/file-and-disk-utilities





- TCPView is a Windows program that will show you detailed listings of all TCP and UDP endpoints on your system, including the local and remote addresses and state of TCP connections. 
 - The TCPView download includes Tcpvcon, a command-line version with the same functionality.
 
- This is a good time to mention that Windows has a built-in utility that provides the same functionality. This tool is called Resource Monitor. There are many ways to open this tool. From the command line use resmon.

https://docs.microsoft.com/en-us/sysinternals/downloads/networking-utilities





- Startup - This utility, which has the most comprehensive knowledge of auto-starting locations of any startup monitor, shows you what programs are configured to run during system bootup or login, and when you start various built-in Windows applications like Internet Explorer, Explorer and media players. 
 - This is a good tool to search for any malicious entries created in the local machine to establish Persistence.
 
 
- ProcDump is a command-line utility whose primary purpose is monitoring an application for CPU spikes and generating crash dumps during a spike that an administrator or developer can use to determine the cause of the spike
 - https://docs.microsoft.com/en-us/sysinternals/downloads/procdump


- The Process Explorer display consists of two sub-windows. The top window always shows a list of the currently active processes, including the names of their owning accounts, whereas the information displayed in the bottom window depends on the mode that Process Explorer is in: if it is in handle mode you'll see the handles that the process selected in the top window has opened; if Process Explorer is in DLL mode you'll see the DLLs and memory-mapped files that the process has loaded.


- Process Monitor is an advanced monitoring tool for Windows that shows real-time file system, Registry and process/thread activity. It combines the features of two legacy Sysinternals utilities, Filemon and Regmon, and adds an extensive list of enhancements including rich and non-destructive filtering, comprehensive event properties such as session IDs and user names, reliable process information, full thread stacks with integrated symbol support for each operation, simultaneous logging to a file, and much more. Its uniquely powerful features will make Process Monitor a core utility in your system troubleshooting and malware hunting toolkit.
 - https://adamtheautomator.com/procmon/
 
 
 
- PsExec is a light-weight telnet-replacement that lets you execute processes on other systems, complete with full interactivity for console applications, without having to manually install client software. PsExec's most powerful uses include launching interactive command-prompts on remote systems and remote-enabling tools like IpConfig that otherwise do not have the ability to show information about remote systems
 - PsExec is another tool that is utilized by adversaries. This tool is associated with MITRE techniques T1570 (Lateral Tool Transfer), T1021.002 (Remote Services: SMB/Windows Admin Shares), and T1569.002 (System Services: Service Execution). It's MITRE ID is S0029.
 - https://docs.microsoft.com/en-us/sysinternals/downloads/psexec
 
 - https://docs.microsoft.com/en-us/sysinternals/downloads/process-utilities


- System Monitor (Sysmon) is a Windows system service and device driver that, once installed on a system, remains resident across system reboots to monitor and log system activity to the Windows event log. It provides detailed information about process creations, network connections, and changes to file creation time. 
 - By collecting the events it generates using Windows Event Collection or SIEM agents and subsequently analyzing them, you can identify malicious or anomalous activity and understand how intruders and malware operate on your network.
 
 https://docs.microsoft.com/en-us/sysinternals/downloads/security-utilities
 
 
 
- "WinObj is a 32-bit Windows NT program that uses the native Windows NT API (provided by NTDLL.DLL) to access and display information on the NT Object Manager's name space."

https://docs.microsoft.com/en-us/sysinternals/downloads/system-information




- BgInfo - It automatically displays relevant information about a Windows computer on the desktop's background, such as the computer name, IP address, service pack version, and more.

- RegJump - "This little command-line applet takes a registry path and makes Regedit open to that path. It accepts root keys in standard (e.g. HKEY_LOCAL_MACHINE) and abbreviated form (e.g. HKLM)." (official definition)
 - There are multiple ways to query the Windows Registry without using the Registry Editor, such as via the command line (reg query) and PowerShell (Get-Item/Get-ItemProperty).
 
 
 
- Strings - "Strings just scans the file you pass it for UNICODE (or ASCII) strings of a default length of 3 or more UNICODE (or ASCII) characters. Note that it works under Windows 95 as well." (official definition)


- https://docs.microsoft.com/en-us/sysinternals/downloads/system-information







- Event logs record events taking place in the execution of a system to provide an audit trail that can be used to understand the activity of the system and to diagnose problems. They are essential to understand the activities of complex systems, particularly in applications with little user interaction (such as server applications)

- SIEMs (Security information and event management) such as Splunk and Elastic come into play.

- logging system on Linux systems is known as Syslog
-  Windows logging system, Windows Event Logs

- The events in these log files are stored in a proprietary binary format with a .evt or .evtx extension.
 - The log files with the .evtx file extension typically reside in C:\Windows\System32\winevt\Logs
 
 
- we need to know what elements form event logs in Windows systems. These elements are:

 - System Logs: Records events associated with the Operating System segments. They may include information about hardware changes, device drivers, system changes, and other activities related to the device.
 
 - Security Logs: Records events connected to logon and logoff activities on a device. The system's audit policy specifies the events. The logs are an excellent
 source for analysts to investigate attempted or successful unauthorized activity.
 - Application Logs: Records events related to applications installed on a system. The main pieces of information include application errors, events, and warnings.
 
 - Directory Service Events: Active Directory changes and activities are recorded in these logs, mainly on domain controllers.
 
 - File Replication Service Events: Records events associated with Windows Servers during the sharing of Group Policies and logon scripts to domain controllers, from where they may be accessed by the users through the client servers.
 
 - DNS Event Logs: DNS servers use these logs to record domain events and to map out
 
 - Custom Logs: Events are logged by applications that require custom data storage. This allows applications to control the log size or attach other parameters, such as ACLs, for security purposes.
 
- There are three main ways of accessing these event logs within a Windows system:
 - Event Viewer (GUI-based application)
 - Wevtutil.exe (command-line tool)
 - Get-WinEvent (PowerShell cmdlet)
 
 
- Microsoft Management Console (MMC) snap-in



- Event Viewer - eventvwr.msc
 
- wevtutil.exe - enables you to retrieve information about event logs and publishers. You can also use this command to install and uninstall event manifests, to run queries, and to export, archive, and clear logs.




- Get-WinEvent 
 - gets events from event logs and event tracing log files on local and remote computers.
 - It provides information on event logs and event log providers.
  - Note: The Get-WinEvent cmdlet replaces the Get-EventLog cmdlet. 
  
  
  
- W3C created XPath, or XML Path Language in full, to provide a standard syntax and semantics for addressing parts of an XML document and manipulating strings, numbers, and booleans. 
 -  The Windows Event Log supports a subset of XPath 1.0.  
 - an XPath event query starts with '*' or 'Event'. 
  - The rest of the XPath query can be created with wevtutil and get-winevent 

- https://docs.microsoft.com/en-us/windows/win32/wes/consuming-events#xpath-10-limitations
- https://docs.microsoft.com/en-us/previous-versions/dotnet/netframework-4.0/ms256115(v=vs.100)









- Sysmon, a tool used to monitor and log events on Windows
 - commonly used by enterprises as part of their monitoring and logging solutions.
 - Sysmon is similar to Windows Event Logs with further detail and granular control.

- System Monitor (Sysmon) is a Windows system service and device driver that, once installed on a system, remains resident across system reboots to monitor and log system activity to the Windows event log. 
 -  By collecting the events it generates using Windows Event Collection or SIEM agents and subsequently analyzing them, you can identify malicious or anomalous activity and understand how intruders and malware operate on your network.
 - Sysmon is most commonly used in conjunction with security information and event management (SIEM) system or other log parsing solutions that aggregate, filter, and visualize events.
 
- Events within Sysmon are stored in Applications and Services Logs/Microsoft/Windows/Sysmon/Operational
 - Sysmon requires a config file in order to tell the binary how to analyze the events that it is receiving. 
 - You can create your own Sysmon config or you can download a config. 
  - Here is an example of a high-quality config that works well for identifying anomalies created by SwiftOnSecurity: https://github.com/SwiftOnSecurity/sysmon-config
  https://github.com/ion-storm/sysmon-config/blob/develop/sysmonconfig-export.xml
  
- Sysmon includes 29 different types of Event IDs, all of which can be used within the config to specify how the events should be handled and analyzed. 

-  If a DLL is loaded in /Temp/ it can be considered an anomaly and should be further investigateded. 
-  any files created in an alternate data stream. This is a common technique used by adversaries to hide malware.
-  "Windows\System\Scripts" directory as this is a common directory for adversaries to place scripts to establish

- https://docs.google.com/spreadsheets/d/17pSTDNpa0sf6pHeRhusvWG6rThciE8CsXTSlDUAZDyo -- Commonly Open Ports Malware 

- Mimikatz is well known and commonly used to dump credentials from memory along with other Windows post-exploitation activity. 
 - The first method of hunting for Mimikatz is just looking for files created with the name Mimikatz. 

- LSASS - Local Security Authority Subsystem Service 
 - Ele verifica os usuários que se conectam a um computador ou servidor Windows, lida com alterações de senha e cria tokens de acesso.[1] Ele também ecreve no log de segurança do Windows.
 
 
- RATs or Remote Access Trojans are used similar to any other payload to gain remote access to a machine
 - Examples of RATs are Xeexe and Quasar
 
- To help detect and hunt malware we will need to first identify the malware that we want to hunt or detect and identify ways that we can modify configuration files, this is known as hypothesis-based hunting

- Persistence is used by attackers to maintain access to a machine once it is compromised. 

- Some examples of evasion techniques are Alternate Data Streams, Injections, Masquerading, Packing/Compression, Recompiling, Obfuscation, Anti-Reversing Techniques.
 - Alternate Data Streams are used by malware to hide its files from normal inspection by saving the file in a different stream apart from $DATA
 
- Injection techniques come in many different types: Thread Hijacking, PE Injection, DLL Injection, and more.







- Osquery is an open-source agent created by Facebook in 2014  
 - It converts the operating system into a relational database
 - It allows us to ask questions from the tables using SQL queries, like returning the list of running processes, a user account created on the host, and the process of communicating with certain suspicious domains.
  - It is widely used by Security Analysts, Incident Responders, Threat Hunters, etc
  - Knowledge of columns and types (known as a schema )
   - https://osquery.io/schema/5.5.1/
  
- The SQL language implemented in Osquery is not an entire SQL language that you might be accustomed to, but rather it's a superset of SQLite. 





- SIEM stands for Security Information and Event Management system. 
 -  tool that collects data from various endpoints/network devices across the network, stores them at a centralized place, and performs correlation on them. 
 
- We can divide our network log sources into two logical parts:

 - Host-Centric Log Sources 
  - These are log sources that capture events that occurred within or related to the host.
  - Some log sources that generate host-centric logs are Windows Event logs, Sysmon, Osquery,
   - Some examples of host-centric logs are:
    - A user accessing a file
    - A user attempting to authenticate.
    - A process Execution Activity
    - A process adding/editing/deleting a registry key or value.
    - Powershell execution
	
 - Network-Centric Log Sources
  - Network-related logs are generated when the hosts communicate with each other or access the internet to visit a website.
  - Some network-based protocols are SSH, VPN, HTTP/s, FTP, etc.
   - Examples of such events are:
    - SSH connection
    - A file being accessed via FTP
    - Web traffic
    - A user accessing company's resources through VPN.
    - Network file sharing Activity
	
- Some key features provided by SIEM are:
 - Real-time log Ingestion
 - Alerting against abnormal activities
 - 24/7 Monitoring and visibility
 - Protection against the latest threats through early detection
 - Data Insights and visualization
 - Ability to investigate past incidents.
 
- common locations where Linux store logs are:
 - /var/log/httpd : Contains HTTP Request  / Response and error logs.
 - /var/log/cron   : Events related to cron jobs are stored in this location.
 - /var/log/auth.log and /var/log/secure : Stores authentication related logs.
 - /var/log/kern : This file stores kernel related events.
 
- In Linux, common locations to write all apache related logs are /var/log/apache or /var/log/httpd.

- For WIndows, Logs are in Event Viewer

- Some of the common capabilities of SIEM are:
 - Correlation between events from different log sources.
 - Provide visibility on both Host-centric and Network-centric activities.
 - Allow analysts to investigate the latest threats and timely responses.
 - Hunt for threats that are not detected by the rules in place.
 
 
- SOC Analysts utilize SIEM solutions in order to have better visibility of what is happening within the network. Some of their responsibilities include:
 - Monitoring and Investigating.
 - Identifying False positives.
 - Tuning Rules which are causing the noise or False positives.
 - Reporting and Compliance.
 - Identifying blind spots in the network visibility and covering them.
 
- Once the logs are ingested, SIEM looks for unwanted behavior or suspicious pattern within the logs with the help of the conditions set in the rules by the analysts. 
 - If the condition is met, a rule gets triggered, and the incident is investigated.
 
 
-  A few examples of correlation rules are:
 - If a User gets 5 failed Login Attempts in 10 seconds - Raise an alert for Multiple Failed Login Attempts
 - If login is successful after multiple failed login attempts - Raise an alert for Successful Login After multiple Login Attempts
 - A rule is set to alert every time a user plugs in a USB (Useful if USB is restricted as per the company policy)
 - If outbound traffic is > 25 MB - Raise an alert to potential Data exfiltration Attempt (Usually, it depends on the company policy)
 
 
 
 
 
- Endpoint detection and response (EDR) are a series of tools and applications that monitor devices for an activity that could indicate a threat or security breach.
 - These tools and applications have features that include:
  - Auditing a device for common vulnerabilities
  - Proactively monitoring a device for suspicious activity such as unauthorised logins, brute-force attacks or privilege escalations
  - Visualising complex data and events into neat and trendy graphs
  - Recording a device's normal operating behaviour to help with detecting anomalies
 
 
- Created in 2015, Wazuh is an open-source, freely available and extensive EDR solution.
 - Wazuh operates on a management and agent module.
 
- Devices that record the events and processes of a system are called agents.
 - Agents monitor the processes and events that take place on the device, such as authentication and user management.
 - Agents will offload these logs to a designated collector for processing, such as Wazuh
 
 
 
 
 
 
- Elastic stack is the collection of different open source components linked together to help users take the data from any source and in any format and perform a search, analyze and visualize the data in real-time.

 - Elasticsearch is a full-text search and analytics engine used to store JSON-formated documents. 
 
 - Logstash is a data processing engine used to take the data from different sources, apply the filter on it or normalize it, and then send it to the destination which could be Kibana or a listening port. 
  - The input part is where the user defines the source from which the data is being ingested. 
  - The filter part is where the user specifies the filter options to normalize the log ingested above.
  - The Output part is where the user wants the filtered data to send.
  
 - Beats is a host-based agent known as Data-shippers that is used to ship/transfer data from the endpoints to elasticsearch. 

 - Kibana is a web-based data visualization that works with elasticsearch to analyze, investigate and visualize the data stream in real-time.
 
 Beats -> Logstash -> Elastic Search -> Kibana 
 
 
- KQL (Kibana Query Language) is a search query language used to search the ingested logs/documents in the elasticsearch.
 - Apart from the KQL language, Kibana also supports Lucene Query Language.




	- Splunk is one of the leading SIEM solutions in the market that provides the ability to collect, analyze and correlate the network and machine logs in real-time
	- Splunk has three main components, namely Forwarder, Indexer, and Search Head. 
	
	- Splunk Forwarder is a lightweight agent installed on the endpoint intended to be monitored, and its main task is to collect the data and send it to the Splunk instance.
	- It does not affect the endpoint's performance as it takes very few resources to process.
	- Key Data Sources:
	- Web server generating web traffic.
	- Windows machine generating Windows Event Logs, PowerShell, and Sysmon data.
	- Linux host generating host-centric logs.
	- Database generating DB connection requests, responses, and errors.
	
	- Splunk Indexer plays the main role in processing the data it receives from forwarders.
	- It takes the data, normalizes it into field-value pairs, determines the datatype of the data, and stores them as events.
	- Processed data is easy to search and analyze.
	
	- Splunk Search Head is the place within the Search & Reporting App where users can search the indexed logs
	- When the user searches for a term or uses a Search language known as Splunk Search Processing Language, the request is sent to the indexer and the relevant events are returned in the form of field-value pairs.
	- Search Head also provides the ability to transform the results into presentable tables, visualizations like pie-chart, bar-chart and column-chart
	
	- Documentação: https://docs.splunk.com/Documentation/Splunk/8.1.2/SearchTutorial/NavigatingSplunk

	- Splunk can ingest any data. 
	- As per the Splunk documentation, when data is added to Splunk, the data is processed and transformed into a series of individual events. 
	- The data sources can be event logs, website logs, firewall logs, etc.
	- Below is a chart listing from the Splunk documentation detailing each data source category.
	
	
	
	
	- An incident from a security perspective is "Any event or action, that has a negative consequence on the security of a user/computer or an organization is considered a security incident."
	- Below are a few of the events that would negatively affect the environment when they occurred:
	• Crashing the system
	• Execution of an unwanted program
	• Access to sensitive information from an unauthorized user
	• A Website being defaced by the attacker
	• The use of USB devices when there is a restriction in usage is against the company's policy
	
	
	
	- As an Incident Handler / SOC Analyst, we would aim to know the attackers' tactics, techniques, and procedures. Then we can stop/defend/prevent against the attack in a better way.

	- The preparation phase covers the readiness of an organization against an attack. 
	- That means documenting the requirements, defining the policies, incorporating the security controls to monitor like EDR / SIEM / IDS / IPS, etc. It also includes hiring/training the staff.

	- The detection phase covers everything related to detecting an incident and the analysis process of the incident.
	- This phase covers getting alerts from the security controls like SIEM/EDR investigating the alert to find the root cause.
	- This phase also covers hunting for the unknown threat within the organization.

	- This phase covers the actions needed to prevent the incident from spreading and securing the network.
	- It involves steps taken to avoid an attack from spreading into the network, isolating the infected host, clearing the network from the infection traces, and gaining control back from the attack.

	- This phase includes identifying the loopholes in the organization's security posture, which led to an intrusion, and improving so that the attack does not happen next time.
	- The steps involve identifying weaknesses that led to the attack, adding detection rules so that similar breach does not happen again, and most importantly, training the staff if required.
	
	
	
	
	
	
	
	
     -  DFIR stands for Digital Forensics and Incident Response. 
	- This field covers the collection of forensic artifacts from digital devices such as computers, media devices, and smartphones to investigate an incident.
	- This field helps Security Professionals identify footprints left by an attacker when a security incident occurs, use them to determine the extent of compromise in an environment, and restore the environment to the state it was before the incident occurred. 
	- DFIR helps security professionals in various ways, some of which are summarized below:
		• Finding evidence of attacker activity in the network and sifting false alarms from actual incidents.
		• Robustly removing the attacker,  so their foothold from the network no longer remains.
		• Identifying the extent and timeframe of a breach. This helps in communicating with relevant stakeholders.
		• Finding the loopholes that led to the breach. What needs to be changed to avoid the breach in the future?
		• Understanding attacker behavior to pre-emptively block further intrusion attempts by the attacker.
		• Sharing information about the attacker with the community.

	
	- DFIR requires expertise in both Digital Forensics and Incident Response.
	- The following skillset is needed to become a DFIR professional:
	- Digital Forensics: These professionals are experts in identifying forensic artifacts or evidence of human activity in digital devices. 
	- Incident Response: Incident responders are experts in cybersecurity and leverage forensic information to identify the activity of interest from a security perspective. 
	
	
	- Artifacts are pieces of evidence that point to an activity performed on a system.
	- When performing DFIR, artifacts are collected to support a hypothesis or claim about attacker activity.
	- Artifacts can be collected from the Endpoint or Server's file system, memory, or network activity.
	
	- When performing DFIR, we must maintain the integrity of the evidence we are collecting.
	- For this reason, certain best practices are established in the industry.
	-  We must note that any forensic analysis contaminates the evidence.
	- Therefore, the evidence is first collected and write-protected.
	- Then, a copy of the write-protected evidence is used for analysis. 
	
	- Another critical aspect of maintaining the integrity of evidence is the chain of custody.
	- When the evidence is collected, it must be made sure that it is kept in secure custody. 
	- Any person not related to the investigation must not possess the evidence, or it will contaminate the chain of custody of the evidence.
	
	- Digital evidence is often volatile, i.e., it can be lost forever if not captured in time. 
	- Data in a computer system's memory (RAM) will be lost when the computer is shut down since the RAM keeps data only as long as it remains powered on.
	- While performing DFIR, it is vital to understand the order of volatility of the different evidence sources to capture and preserve accordingly.

	- Once we have collected the artifacts and maintained their integrity, we need to present them understandably to fully use the information contained in them
	- A timeline of events needs to be created for efficient and accurate analysis.
	- This timeline of events puts all the activities in chronological order. 
	- This activity is called timeline creation. 
	- Timeline creation provides perspective to the investigation and helps collate information from various sources to create a story of how things happened.

	- Tools:
	- KAPE
	- Autopsy
	- Volatility
	- Redline
	- Velociraptor

	- Different organizations have published standardized methods to perform Incident Response.
	
	- NIST has defined a process in their SP-800-61 Incident Handling guide, which has the following steps:
		1. Preparation
		2. Detection and Analysis
		3. Containment, Eradication, and Recovery
		4. Post-incident Activity
	
	- Similarly, SANS has published an Incident Handler's handbook. The handbook defines the steps as follows (PICERL):
		1. Preparation
		2. Identification
		3. Containment
		4. Eradication
		5. Recovery
		6. Lessons Learned

	1. Preparation: Before an incident happens, preparation needs to be done so that everyone is ready in case of an incident. Preparation includes having the required people, processes, and technology to prevent and respond to incidents.
	2. Identification: An incident is identified through some indicators in the identification phase. These indicators are then analyzed for False Positives, documented, and communicated to the relevant stakeholders.
	3. Containment: In this phase, the incident is contained, and efforts are made to limit its effects. There can be short-term and long-term fixes for containing the threat based on forensic analysis of the incident that will be a part of this phase.
	4. Eradication: Next, the threat is eradicated from the network. It has to be ensured that a proper forensic analysis is performed and the threat is effectively contained before eradication. For example, if the entry point of the threat actor into the network is not plugged, the threat will not be effectively eradicated, and the actor can gain a foothold again.
	5. Recovery: Once the threat is removed from the network, the services that had been disrupted are brought back as they were before the incident happened.
	6. Lessons Learned: Finally, a review of the incident is performed, the incident is documented, and steps are taken based on the findings from the incident to make sure that the team is better prepared for the next time an incident occurs.




	- Computer forensics is an essential field of cyber security that involves gathering evidence of activities performed on computers.
	- It is a part of the wider Digital Forensics field, which deals with forensic analysis of all types of digital devices, including recovering, examining, and analyzing data found in digital devices.
	- The applications of digital and computer forensics are wide-ranging, from the legal sphere, where it is used to support or refute a hypothesis in a civil or criminal case
	-  to the private sphere, where it helps in internal corporate investigations and incident and intrusion analysis.

	- Forensic artifacts are essential pieces of information that provide evidence of human activity.
	- On a Windows system, a person's actions can be traced back quite accurately using computer forensics because of the various artifacts a Windows system creates for a given activity.
	- These artifacts often reside in locations 'normal' users won't typically venture to.
	- we'll see that Windows stores these artifacts in different locations throughout the file system such as in the registry, a user's profile directory, in application-specific files, etc. 
	
	
	- The Windows Registry is a collection of databases that contains the system's configuration data. 
	- This configuration data can be about the hardware, the software, or the user's information.
	- It also includes data about the recently used files, programs used, or devices connected to the system.
	- A Registry Hive is a group of Keys, subkeys, and values stored in a single file on the disk.

	
	
	
	
	- The majority of these hives are located in the C:\Windows\System32\Config directory and are:
		1. DEFAULT (mounted on HKEY_USERS\DEFAULT)
		2. SAM (mounted on HKEY_LOCAL_MACHINE\SAM)
		3. SECURITY (mounted on HKEY_LOCAL_MACHINE\Security)
		4. SOFTWARE (mounted on HKEY_LOCAL_MACHINE\Software)
		5. SYSTEM (mounted on HKEY_LOCAL_MACHINE\System)
		
	- Apart from these hives, two other hives containing user information can be found in the User profile directory.
	- For Windows 7 and above, a user’s profile directory is located in C:\Users\<username>\ where the hives are:
		1. NTUSER.DAT (mounted on HKEY_CURRENT_USER when a user logs in)
		2. USRCLASS.DAT (mounted on HKEY_CURRENT_USER\Software\CLASSES)

	- The USRCLASS.DAT hive is located in the directory C:\Users\<username>\AppData\Local\Microsoft\Windows
	- The NTUSER.DAT hive is located in the directory C:\Users\<username>\
	- Remember that NTUSER.DAT and USRCLASS.DAT are hidden files.
	
	- There is another very important hive called the AmCache hive.
	- This hive is located in C:\Windows\AppCompat\Programs\Amcache.hve
	- Windows creates this hive to save information on programs that were recently run on the system.
	
	- Some other very vital sources of forensic data are the registry transaction logs and backups.
	- The transaction logs can be considered as the journal of the changelog of the registry hive. Windows often uses transaction logs when writing data to registry hives.
	- The transaction log for each hive is stored as a .LOG file in the same directory as the hive itself. 
	- It has the same name as the registry hive, but the extension is .LOG. For example, the transaction log for the SAM hive will be located in C:\Windows\System32\Config in the filename SAM.LOG.
	- Sometimes there can be multiple transaction logs as well. In that case, they will have .LOG1, .LOG2 etc.

	- Registry backups are the opposite of Transaction logs.
	- These are the backups of the registry hives located in the C:\Windows\System32\Config directory. 
	- These hives are copied to the C:\Windows\System32\Config\RegBack directory every ten days.
	- It might be an excellent place to look if you suspect that some registry keys might have been deleted/modified recently.
	
	- For the sake of accuracy, it is recommended practice to image the system or make a copy of the required data and perform forensics on it.
	- This process is called data acquisition.
	- Though we can view the registry through the registry editor, the forensically correct method is to acquire a copy of this data and perform analysis on that.
	- However, when we go to copy the registry hives from %WINDIR%\System32\Config, we cannot because it is a restricted file.
	- To copy these registries, use: KAPE, AUTOPSY, FTK Imager
	- We can only see these files in the Registry Editor. Since it only works in live systems, we can use some tools specific to this:
	- Registry Viewer
	- Zimmerman's Registry Explorer
	- RegRipper

	- When we start performing forensic analysis, the first step is to find out about the system information.
	- This task will cover gathering information related to a machine's System and Account information.
	- OS Version: SOFTWARE\Microsoft\Windows NT\CurrentVersion

	- The hives containing the machine’s configuration data used for controlling system startup are called Control Sets.
- In most cases, ControlSet001 will point to the Control Set that the machine booted with
- ControlSet002 will be the last known good configuration.
- Their locations will be:
 - SYSTEM\ControlSet001
 - SYSTEM\ControlSet002
 
- Windows creates a volatile Control Set when the machine is live, called the CurrentControlSet (HKLM\SYSTEM\CurrentControlSet)
 - For getting the most accurate system information, this is the hive that we will refer to: SYSTEM\Select\Current
 - Similarly, the last known good configuration can be found using the following registry value: SYSTEM\Select\LastKnownGood
  - Many forensic artifacts we collect will be collected from the Control Sets.
  
- We can find the Computer Name from the following location: SYSTEM\CurrentControlSet\Control\ComputerName\ComputerName 

- It is important to establish what time zone the computer is located in
 - This will help us understand the chronology of the events as they happened: SYSTEM\CurrentControlSet\Control\TimeZoneInformation
 - The following registry key will give a list of network interfaces on the machine we are investigating: SYSTEM\CurrentControlSet\Services\Tcpip\Parameters\Interfaces
 
 - The past networks a given machine was connected to can be found in the following locations:
  - SOFTWARE\Microsoft\Windows NT\CurrentVersion\NetworkList\Signatures\Unmanaged
  - SOFTWARE\Microsoft\Windows NT\CurrentVersion\NetworkList\Signatures\Managed
  
- The following registry keys include information about programs or commands that run when a user logs on. 
 - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Run
 - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\RunOnce
 - SOFTWARE\Microsoft\Windows\CurrentVersion\RunOnce
 - SOFTWARE\Microsoft\Windows\CurrentVersion\policies\Explorer\Run
 - SOFTWARE\Microsoft\Windows\CurrentVersion\Run
  - For Services: SYSTEM\CurrentControlSet\Services
  
- The SAM hive contains user account information, login information, and group information. This information is mainly located in the following location:
 - SAM\Domains\Account\Users
 
- Windows maintains a list of recently opened files for each user: NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\RecentDocs
 -  if we are looking specifically for the last used PDF files, we can look at the following registry key: NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\RecentDocs\.pdf

- Microsoft Office also maintains a list of recently opened documents: NTUSER.DAT\Software\Microsoft\Office\VERSION
 - The version number for each Microsoft Office release is different. An example registry key will look like this: NTUSER.DAT\Software\Microsoft\Office\15.0\Word
 - Lista de Releases de Office e seus respectivos números de versão: https://docs.microsoft.com/en-us/deployoffice/install-different-office-visio-and-project-versions-on-the-same-computer#office-releases-and-their-version-number
 
- When any user opens a folder, it opens in a specific layout. When any user opens a folder, it opens in a specific layout.
 - These layouts can be different for different folders. This information about the Windows 'shell' is stored and can identify the Most Recently Used files and folders.
 - Since this setting is different for each user, it is located in the user hives. We can find this information on the following locations:
  - USRCLASS.DAT\Local Settings\Software\Microsoft\Windows\Shell\Bags
  - USRCLASS.DAT\Local Settings\Software\Microsoft\Windows\Shell\BagMRU
  - NTUSER.DAT\Software\Microsoft\Windows\Shell\BagMRU
  - NTUSER.DAT\Software\Microsoft\Windows\Shell\Bags
  
  
- When we open or save a file, a dialog box appears asking us where to save or open that file from.
 - It might be noticed that once we open/save a file at a specific location, Windows remembers that location. This implies that we can find out recently used files if we get our hands on this information.
 - We can do so by examining the following registry keys:
  - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\ComDlg32\OpenSavePIDlMRU
  - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\ComDlg32\LastVisitedPidlMRU
  
- Another way to identify a user's recent activity is by looking at the paths typed in the Windows Explorer address bar or searches performed using the following registry keys, respectively.
 - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\TypedPaths
 - NTUSER.DAT\Software\Microsoft\Windows\CurrentVersion\Explorer\WordWheelQuery
 

- Windows keeps track of applications launched by the user using Windows Explorer for statistical purposes in the User Assist registry keys.
- These keys contain information about the programs launched, the time of their launch, and the number of times they were executed.
- The User Assist key is present in the NTUSER hive, mapped to each user's GUID. We can find it at the following location: NTUSER.DAT\Software\Microsoft\Windows\Currentversion\Explorer\UserAssist\{GUID}\Count

- ShimCache is a mechanism used to keep track of application compatibility with the OS and tracks all applications launched on the machine.
 -  Its main purpose in Windows is to ensure backward compatibility of applications.
 - It is also called Application Compatibility Cache (AppCompatCache)
  - It is located in the following location in the SYSTEM hive: SYSTEM\CurrentControlSet\Control\Session Manager\AppCompatCache
  - ShimCache stores file name, file size, and last modified time of the executables.
   - Our goto tool, the Registry Explorer, doesn't parse ShimCache data in a human-readable format, so we go to another tool called AppCompatCache Parser, also a part of Eric Zimmerman's tools.
   
- The AmCache hive is an artifact related to ShimCache.
 - This performs a similar function to ShimCache, and stores additional data related to program executions.
 - This data includes execution path, installation, execution and deletion times, and SHA1 hashes of the executed programs. 
 - This hive is located in the file system at: C:\Windows\appcompat\Programs\Amcache.hve
  - Information about the last executed programs can be found at the following location in the hive:
  - Amcache.hve\Root\File\{Volume GUID}\
  
- Background Activity Monitor or BAM keeps a tab on the activity of background applications.
- Similar Desktop Activity Moderator or DAM is a part of Microsoft Windows that optimizes the power consumption of the device. 
 - Both of these are a part of the Modern Standby system in Microsoft Windows.
 - This location contains information about last run programs, their full paths, and last execution time.
  - SYSTEM\CurrentControlSet\Services\bam\UserSettings\{SID}
  - SYSTEM\CurrentControlSet\Services\dam\UserSettings\{SID}
  
- When performing forensics on a machine, often the need arises to identify if any USB or removable drives were attached to the machine.
 - The following locations keep track of USB keys plugged into a system.
 - These locations store the vendor id, product id, and version of the USB device plugged in and can be used to identify unique devices.
 - These locations also store the time the devices were plugged into the system.
  - SYSTEM\CurrentControlSet\Enum\USBSTOR
  - SYSTEM\CurrentControlSet\Enum\USB
  
- Similarly, the following registry key tracks the first time the device was connected, the last time it was connected and the last time the device was removed from the system.
 - SYSTEM\CurrentControlSet\Enum\USBSTOR\Ven_Prod_Version\USBSerial#\Properties\{83da6326-97a6-4088-9453-a19231573b29}\####
 
- The device name of the connected drive can be found at the following location: SOFTWARE\Microsoft\Windows Portable Devices\Devices

A storage device in a computer system, for example, a hard disk drive or a USB device, is just a collection of bits.
To convert these bits into meaningful information, they need to be organized.
For this purpose, computer scientists and engineers have created different file systems that organize the bits in a hard drive as per a standard, so that information stored in these bits can be interpreted easily.

The File Allocation Table (FAT) is one of these file systems.
It has been the default file system for Microsoft Operating Systems since at least the late 1970s and is still in use, though not the default anymore.
the File Allocation Table creates a table that indexes the location of bits that are allocated to different files.
The FAT file system supports the following Data structures:
Clusters: A cluster is a basic storage unit of the FAT file system. 
          Each file stored on a storage device can be considered a group of clusters containing bits of information.

Directory: A directory contains information about file identification, like file name, starting cluster, and filename length.

File Allocation Table: The File Allocation Table is a linked list of all the clusters. 
It contains the status of the cluster and the pointer to the next cluster in the chain.

The FAT file format divides the available disk space into clusters for more straightforward addressing.
The number of these clusters depends on the number of bits used to address the cluster. Hence the different variations of the FAT file system.
FAT was initially developed with 8-bit cluster addressing, and it was called the FAT Structure.
Later, as the storage needed to be increased, FAT12, FAT16, and FAT32 were introduced. The last one of them was introduced in 1996.

Theoretically, FAT12 used 12-bit cluster addressing for a maximum of 4096 clusters(2^12).
FAT16 used 16-bit cluster addressing for a maximum of 65,536 clusters (2^16)
In the case of FAT32, the actual bits used to address clusters are 28, so the maximum number of clusters is actually 268,435,456 or 2^28. 
However, not all of these clusters are used for file storage.
Some are used for administrative purposes, e.g., to store the end of a chain of clusters, the unusable parts of the disk, or other such purposes.



The chances of coming across a FAT12 filesystem are very rare nowadays. 
FAT16 and FAT32 are still used in some places, like USB drives, SD cards, or Digital cameras. 

The exFAT file system is now the default for SD cards larger than 32GB.
It has also been adopted widely by most manufacturers of digital devices. 

the FAT file system is a very basic file system. It does the job when it comes to organizing our data, 
but it offers little more in terms of security, reliability, and recovery capabilities

Microsoft developed a newer file system called the New Technology File System (NTFS) 
This file system was introduced in 1993 with the Windows NT 3.1.
It became mainstream since Windows XP.

The NTFS file system keeps a log of changes to the metadata in the volume. 
This feature helps the system recover from a crash or data movement due to defragmentation. 
This log is stored in $LOGFILE in the volume's root directory. 
Hence the NTFS file system is called a journaling file system.

The NTFS file system has access controls that define the owner of a file/directory and permissions for each user.

The NTFS file system keeps track of changes made to a file using a feature called Volume Shadow Copies.
Using this feature, a user can restore previous file versions for recovery or system restore.
In recent ransomware attacks, ransomware actors have been noted to delete the shadow copies on a victim's file systems to prevent them from recovering their data.

A file is a stream of data organized in a file system. 
Alternate data streams (ADS) is a feature in NTFS that allows files to have multiple streams of data stored in a single file. 
Internet Explorer and other browsers use Alternate Data Streams to identify files downloaded from the internet (using the ADS Zone Identifier).
Malware has also been observed to hide their code in ADS.

Like the File Allocation Table, there is a Master File Table in NTFS.
However, the Master File Table, or MFT, is much more extensive than the File Allocation Table.
It is a structured database that tracks the objects stored in a volume.
From a forensics point of view, the following are some of the critical files in the MFT:
 - $MFT - The $MFT is the first record in the volume. This file contains a directory of all the files present on the volume.
 - $LOGFILE - stores the transactional logging of the file system. It helps maintain the integrity of the file system in the event of a crash.
 - UsnJrnl -  It contains information about all the files that were changed in the file system and the reason for the change. It is also called the change journal.
 
MFT Explorer is one of Eric Zimmerman's tools used to explore MFT files.


Understanding the file systems makes it easier to know how files are deleted, recovered, and wiped.
a file system stores the location of a file on the disk in a table or a database.
When we delete a file from the file system, the file system deletes the entries that store the file's location on the disk. 
For the file system, the location where the file existed is now available for writing or unallocated.
However, the file contents on disk are still there, as long as they are not overwritten by the file system while copying another file or by the disk firmware while performing maintenance on the disk.


A disk image file is a file that contains a bit-by-bit copy of a disk drive. 


When a program is run in Windows, it stores its information for future use.
This stored information is used to load the program quickly in case of frequent use.
This information is stored in prefetch files which are located in the C:\Windows\Prefetch directory.
Prefetch files have an extension of .pf
Prefetch files contain :
the last run times of the application
the number of times the application was run
and any files and device handles used by the file.

We can use Prefetch Parser (PECmd.exe) from Eric Zimmerman's tools for parsing Prefetch files and extracting data.

Windows 10 stores recently used applications and files in an SQLite database called the Windows 10 Timeline.
The Windows 10 timeline can be found at the following location: C:\Users\<username>\AppData\Local\ConnectedDevicesPlatform\{randomfolder}\ActivitiesCache.db

Windows introduced jump lists to help users go directly to their recently used files from the taskbar.
We can view jumplists by right-clicking an application's icon in the taskbar, and it will show us the recently opened files in that application.
Jumplists include information about the applications executed, first time of execution, and last time of execution of the application against an AppID.


When any new device is attached to a system, information related to the setup of that device is stored in the setupapi.dev.log
C:\Windows\inf\setupapi.dev.log
This log contains the device serial number and the first/last times when the device was connected. 

	
	


One of the reasons for this versatility is that Linux is an open-source Operating System with many different flavors.
It is also very lightweight and can run on very low resources.
It can be considered modular in nature and can be customized as per requirements, meaning that only those components can be installed which are required.

Linux comes in many different flavors, also called distributions.
Sometimes the differences are mostly cosmetic, while sometimes the differences are a little more pronounced.
Some of the common Linux distributions include:
• Ubuntu
• Redhat
• ArchLinux
• Open SUSE
• Linux Mint
• CentOS
• Debian

For a Linux system, everything is stored in a file.
Therefore, to identify forensic artifacts, we will need to know the locations of these files and how to read them.

OS Release Information: /etc/os-release
Information About Users on the System: /etc/passwd
Password Information: /etc/shadow
Group Information: /etc/group
Sudoers List: /etc/sudoers
Logs: /var/log
Hostname: /etc/hostname 
Timezone: /etc/timezone
Network Interface: /etc/network/interfaces
MAC and IP: ip  
Active Network Connections: Netstat 
Running Processes : PS / PS AUX 
DNS Information: /etc/hosts
DNS Servers: /etc/resolv.conf


Persistence mechanisms are ways a program can survive after a system reboot.
This helps malware authors retain their access to a system even if the system is rebooted.  

Cron jobs are commands that run periodically after a set amount of time
 list of Cron jobs in a file located at: /etc/crontab
 
Like Windows, services can be set up in Linux that will start and run in the background after every system boot.
A list of services can be found in the: /etc/init.d

When a bash shell is spawned, it runs the commands stored in the .bashrc
This file can be considered as a startup list of actions to be performed. 

System-wide settings are stored in 
/etc/bash.bashrc and /etc/profile files,


Knowing what programs have been executed on a host is one of the main purposes of performing forensic analysis.
All the commands that are run on a Linux host using sudo are stored in the auth log. "auth.log"

Any commands other than the ones run using sudo are stored in the bash history.  ./bash_history

The Vim text editor stores logs for opened files in Vim in the file named .viminfo in the home directory.  ".viminfo"

One of the most important sources of information on the activity on a Linux host is the log files. 
These log files maintain a history of activity performed on the host and the amount of logging depends on the logging level defined on the system.
Logs are generally found in the /var/log directory.

The Syslog contains messages that are recorded by the host about system activity. /var/log/syslog
With the passage of time, the Linux machine rotates older logs into files such as syslog.1, syslog.2 etc

The /var/log/ directory contains logs for third-party applications such as webserver, database, or file share server logs.





Several features within Autopsy have been developed by the Department of Homeland Security Science and Technology funding.

Autopsy can analyse multiple disk image formats.
Supported Disk Image Formats:
• Raw Single (For example: *.img, *.dd, *.raw, *.bin)
• Raw Split (For example: *.001, *.002, *.aa, *.ab, etc)
• EnCase (For example: *.e01, *.e02, etc)
• Virtual Machines (For example: *.vmdk, *.vhd)

Essentially Ingest Modules are Autopsy plug-ins.
Each Ingest Module is designed to analyse and retrieve specific data from the drive.

S = Score
The Score will show a red exclamation point for a folder/file marked/tagged as notable and a yellow triangle pointing downward for a folder/file marked/tagged as suspicious. 

C = Comment
If a yellow page is visible in the Comment column, it will indicate that there is a comment for the folder/file. 

O = Occurrence 
In a nutshell, this column will indicate how many times this file/folder has been seen in past cases






Kroll Artifact Parser and Extractor (KAPE) parses and extracts Windows forensics artifacts. 
It is a tool that can significantly reduce the time needed to respond to an incident by providing forensic artifacts from a live system 
or a storage device much earlier than the imaging process completes. 

KAPE serves two primary purposes
1) collect files
2) process the collected files as per the provided options.
For achieving these purposes, KAPE uses the concept of targets and modules.

Targets can be defined as the forensic artifacts that need to be collected. 
Modules are programs that process the collected artifacts and extract information from them.

In essence, the KAPE binary ﻿ collects files and processes them as per the provided configuration.

KAPE does not need to be installed.
It is portable and can be used from network locations or USB drives.
 
Targets are the artifacts that need to be collected from a system or image and copied to our provided destination.

﻿KAPE also supports Compound Targets
These are Targets that are compounds of multiple other targets.

Modules, in KAPE's lexicon, run specific tools against the provided set of files.




Volatility is a free memory forensics tool developed and maintained by Volatility Foundation
commonly used by malware and SOC analysts within a blue team or as part of their detection and monitoring solutions.

Volatility is written in Python and is made up of python plugins and modules designed as a plug-and-play way of analyzing memory dumps.

Volatility for Python3: https://github.com/volatilityfoundation/volatility3

Listed below are a few of the techniques and tools that can be used to extract a memory from a bare-metal machine.

FTK Imager
Redline
DumpIt.exe
win32dd.exe / win64dd.exe
Memoryze
FastDump

When using an extraction tool on a bare-metal host, it can usually take a considerable amount of time; 
take this into consideration during your investigation if time is a constraint.

listed below are a few of the hypervisor virtual memory files you may encounter.

VMWare - .vmem
Hyper-V - .bin
Parallels - .mem
VirtualBox - .sav file *this is only a partial memory file


Velociraptor is a unique, advanced open-source endpoint monitoring, digital forensic and cyber response platform.
Velociraptor provides you with the ability to more effectively respond to a wide range of digital forensic and cyber incident response investigations and data breaches

Velociraptor is unique because the Velociraptor executable can act as a server or a client and it can run on Windows, Linux, and MacOS. 
Velociraptor is also compatible with cloud file systems, such as Amazon EFS and Google Filestore. 

Velociraptor can be deployed across thousands, even tens of thousands, client endpoints and runs surprisingly well for an open-source product. 






TheHive Project is a scalable, open-source and freely available Security Incident Response Platform
designed to assist security analysts and practitioners working in SOCs, CSIRTs and CERTs to track, investigate and act upon identified security incidents
Security Analysts can collaborate on investigations simultaneously, ensuring real-time information pertaining to new or existing cases, tasks, observables and IOCs are available to all team members.

https://thehive-project.org/
https://github.com/TheHive-Project/TheHive

TheHive Project operates under the guide of three core functions:
- Collaborate: Multiple analysts from one organisation can work together on the same case simultaneously.
- Elaborate: Investigations correspond to cases.
- Act: A quick triaging process can be supported by allowing analysts to add observables to their cases, leveraging tags and flagging IOCs

Features from the TheHive
 - Case / Task Management: Every investigation is meant to correspond to a case that has been created. Each case can be broken down into one or more tasks
 - Alert Triage: Allows an analyst to go through the imported alerts and decide whether or not they are to be escalated into investigations or incident response.
 - Observable Enrichment with Cortex:  ortex allows analysts to collect more information from threat indicators by performing correlation analysis and developing patterns from the cases. 
 - Active Response: TheHive allows analysts to use Responders and run active actions to communicate, share information about incidents and prevent or contain a threat.
 - Custom Dashboards: Statistics on cases, tasks, observables, metrics and more can be compiled and distributed on dashboards
 - Built-in MISP Integration: Another useful integration is with MISP, a threat intelligence platform for sharing, storing and correlating Indicators of Compromise of targeted attacks and other threats.








The word malware is derived from the term MALicious softWARE.
Therefore, any software that has a malicious purpose can be considered malware.

Malware Analysis is an important skill to have.
As a quick overview, Malware Analysis is performed by the following people in the Security Industry:
 - Security Operations teams analyze malware to write detections for malicious activity in their networks.
 - Incident Response teams analyze malware to determine what damage has been done to an environment to remediate and revert that damage.
 - Threat Hunt teams analyze malware to identify IOCs, which they use to hunt for malware in a network.
 - Malware Researchers in security product vendor teams analyze malware to add detections for them in their security products.
 - Threat Research teams in OS Vendors like Microsoft and Google analyze malware to discover the vulnerabilities exploited and add more security features to the OS/applications.
 
- Always take the following precautions while analyzing malware:
 - Never analyze malware or suspected malware on a machine that does not have the sole purpose of analyzing malware.
 - When not analyzing or moving malware samples around to different locations, always keep them in password-protected zip/rar or other archives so that we can avoid accidental detonation.
 - Only extract the malware from this password-protected archive inside the isolated environment, and only when analyzing it.
 - Create an isolated VM specifically for malware analysis, which has the capability of being reverted to a clean slate once you are done.
 - Ensure that all internet connections are closed or at least monitored.
 - Once you are done with malware analysis, revert the VM to its clean slate for the next malware analysis session to avoid residue from a previous malware execution corrupting the next one.
 
 
 Malware Analysis is like solving a puzzle.
 Different tools and techniques are used to find the pieces of this puzzle, and joining those pieces gives us the complete picture of what the malware is trying to do.
 Most of the time, you will have an executable file (also called a binary or a PE file. PE stands for Portable Executable), a malicious document file or a network capture (PCAP)
 
 To find different puzzle pieces, it's necessary to use various tools - The techniques can be grouped into two categories:
  - Static Analysis
  - Dynamic Analysis 
  
- Static Analysis 
 - When malware is analyzed without being executed, it is called Static Analysis. 
 - the different properties of the PE file are analyzed without running it.
  - Examples: 
   - checking for strings in malware
   - checking the PE header for information related to different sections
   - looking at the code using a disassemble
	
 - Malware often uses techniques to avoid static analysis.
  - Some of these techniques use obfuscation
  - packing
  - or other means of hiding its properties.
   - To circumvent these techniques, we often use dynamic analysis.
   
- Dynamic Analysis 
 - Malware faces a dilemma
 - It has to execute to fulfil its purpose, and no matter how much obfuscation is added to the code, it becomes an easy target for detection once it runs.
  - Dynamic analysis techniques include running the malware in a VM, either in a manual fashion with tools installed to monitor the malware's activity
  - or in the form of sandboxes that perform this task automatically. 
  
- Remnux (Reverse Engineering Malware Linux) is a Linux distribution purpose-built for malware analysis. 
 - It has many tools required for malware analysis already installed on it.  
 

When analyzing a new piece of malware, the first step is usually performing basic static analysis. 
Basic static analysis can be considered sizing up the malware, trying to find its properties before diving deep into analysis.

Though often the file type of malware is visible in the file extension and is obvious, 
sometimes malware authors try to trick users by using misleading file extensions. 
In Linux, to discover the file type, use the "File" command 

Another really important command that provides us with useful information about a file is the "strings" command.
This command lists down the strings present in a file.

Sometimes, the output of the strings command is too big to be shown on the terminal completely.
We can redirect it, write it to a file, and read it using vim or any other tool.


File Hashing provides us with a fixed-size unique number that identifies a file.
A File Hash can therefore be considered a unique identifier for a file, similar to Social Security Numbers or National Identification Numbers used for the citizens of a country. 
Hashing is an important concept in malware analysis.
It can be used as an identifier for specific malware.
this identifier can then be shared with other analysts or searched online for information sharing purposes.
Please note that a single bit of difference in two files will result in different hashes, so changing the hash of a file is as simple as changing one bit in it.

Commonly, commands: "md5sum", "sha1sum" and "sha256sum" hashes are used for file hashing.
We can calculate file hashes by using a simple command in Linux

Scanning a file using AVs or searching for a hash on VirusTotal can also provide useful information about the classification of malware performed by security researchers.
when using an online scanner, it is recommended to search for the malware's hash instead of uploading online to avoid leaking sensitive information online. 


The PE File Header contains the metadata about a Portable Executable file.
This data can help us find a lot of helpful information to help us in our analysis.
A PE file seldom contains all the code that it needs to run on a system on its own. Most of the time, it re-uses code provided by the Operating System.
This is done to use less space and leverage the framework the Operating System has laid to perform tasks instead of re-inventing the wheel.

the following are the most commonly seen sections in a PE file.
.text: This Section generally contains the CPU instructions executed when the PE file is run. This section is marked as executable.
.data: This Section contains the global variables and other global data used by the PE file.
.rsrc: This Section contains resources that are used by the PE file, for example, images, icons, etc.

Te REMNUX have a utilizty  called "pecheck", to analyze the PE Header 

There's a GUI version to check de PE Header: PE-Tree Tool

Don't perform malware analysis on a live machine not purpose-built for malware analysis.

In malware analysis, a sandbox is an isolated environment mimicking the actual target environment of a malware, where an analyst runs a sample to learn more about it.
Malware analysis sandboxes heavily rely on Virtual Machines, their ability to take snapshots and revert to a clean state when required.

For malware analysis using sandboxes, the following considerations make the malware analysis effective:

Virtual Machine mimicking the actual target environment of the malware sample
Ability to take snapshots and revert to clean state
OS monitoring software, for example, Procmon, ProcExplorer or Regshot, etc.
Network monitoring software, for example, Wireshark, tcpdump, etc.
Control over the network through a dummy DNS server and webserver.
A mechanism to move analysis logs and malware samples in and out of the Virtual Machine without compromising the host
(If you have a shared directory with your malware analysis VM that remains accessible when running malware, you might risk malware affecting all files in your shared directory)

One can always set up Open Source Sandboxes. 
These sandboxes provide the framework for performing basic dynamic analysis and are also customizable to a significant extent to help those with a more adventurous mindset.
Cuckoo's Sandbox - is the most widely known sandbox in the malware analysis community. (Obsolete)
CAPE Sandbox  - CAPE Sandbox is so far actively developed and supports Python 3.

Some of the most commonly used online sandboxes are as follows:
Online Cuckoo Sandbox
Online CAPE Sandbox
Any.run
Intezer
Hybrid Analysis

Though online sandboxes provide a useful utility, it is best not to submit a sample online unless you are sure of what you are doing. 
A better approach is to search for the sample's hash on the service you are using to see if someone has already submitted it. 

Malware authors often use packing and obfuscation to make an analyst's life difficult.
A packer obfuscates, compresses, or encrypts the contents of malware. 
These techniques make it difficult to analyze malware statically. 

Sandbox Evasion:
Long sleep calls: Malware authors know that sandboxes run for a limited time. Therefore, they program the malware not to perform any activity for a long time after execution. This is often accomplished through long sleep calls. The purpose of this technique is to time out the sandbox.
User activity detection: Some malware samples will wait for user activity before performing malicious activity. The premise of this technique is that there will be no user in a sandbox. Therefore there will be no mouse movement or typing on the keyboard. Advanced malware also detects patterns in mouse movements that are often used in automated sandboxes. This technique is designed to bypass automated sandbox detection.
Footprinting user activity: Some malware checks for user files or activity, like if there are any files in the MS Office history or internet browsing history. If no or little activity is found, the malware will consider the machine as a sandbox and quit. 
Detecting VMs: Sandboxes run on virtual machines. Virtual machines leave artifacts that can be identified by malware. For example, some drivers installed in VMs being run on VMWare or Virtualbox give away the fact that the machine is a VM. Malware authors often associate VMs with sandboxes and would terminate the malware if a VM is detected.





One of the most popular tools is Volatility, which will allow an analyst to dig deep into the weeds when examining memory artifacts from an endpoint.
Redline will essentially give an analyst a 30,000-foot view (10 kilometers high view) of a Windows, Linux, or macOS endpoint.
Using Redline, you can analyze a potentially compromised endpoint through the memory dump, including various file structures. 

Here is what you can do using Redline:
• Collect registry data (Windows hosts only)
• Collect running processes
• Collect memory images (before Windows 10)
• Collect Browser History
• Look for suspicious strings
• And much more!

There are three ways or options to collect data using Redline: 
• Standard Collector - this method configures the script to gather a minimum amount of data for the analysis. 
• Comprehensive Collector - this method configures the script to gather the most data from your host for further analysis.
• IOC Search Collector (Windows only) - this method collects data that matches with the Indicators of Compromise (IOCs) that you created with the help of IOC Editor.

A handle is a connection from a process to an object or resource in a Windows operating system.
Operating systems use handles for referencing internal objects like files, registry keys, resources, etc.

Some of the other important sections you need to pay attention to are:
• File System 
• Registry
• Windows Services
• Tasks (Threat actors like to create scheduled tasks for persistence)
• Event Logs (this another great place to look for the suspicious Windows PowerShell events as well as the Logon/Logoff, user creation events, and others)
• ARP and Route Entries 
• Browser URL History 
• File Download History

IOC stands for Indicators of Compromise; they are artifacts of the potential compromise and host intrusion on the system or network that you need to look for when conducting threat hunting or performing incident response.
IOCs can be MD5, SHA1, SHA256 hashes, IP address, C2 domain, file size, filename, file path, a registry key, etc.
One of the great tools you can use is IOC Editor, created by FireEye, to create IOC files. You can refer to this link to learn how to use the IOC
https://fireeye.market/assets/apps/S7cWpi9W//9cb9857f/ug-ioc-editor.pdf





Spam and Phishing are common social engineering attacks. 
 In social engineering, phishing attack vectors can be a phone call, a text message, or an email.
 
There are 3 specific protocols involved to facilitate the outgoing and incoming email messages, and they are briefly listed below.
 SMTP (Simple Mail Transfer Protocol) - It is utilized to handle the sending of emails. 
 POP3 (Post Office Protocol) - Is responsible transferring email between a client and a mail server. 
 IMAP (Internet Message Access Protocol) - Is responsible transferring email between a client and a mail server. 
 
POP3
Emails are downloaded and stored on a single device.
Sent messages are stored on the single device from which the email was sent.
Emails can only be accessed from the single device the emails were downloaded to.
If you want to keep messages on the server, make sure the setting "Keep email on server" is enabled, 
or all messages are deleted from the server once downloaded to the single device's app or software.

IMAP
Emails are stored on the server and can be downloaded to multiple devices.
Sent messages are stored on the server.
Messages can be synced and accessed across multiple devices.

(Ilustração)
(Explicação)

we need to understand that there are two parts to an email:
 the email header (information about the email, such as the email servers that relayed the email)
 the email body (text and/or HTML formatted text)
   The syntax for email messages is known as the Internet Message Format (IMF).
   
   
Review this Knowledge Base (KB) article from Media Temple on viewing the raw/full email headers in various email clients: https://mediatemple.net/community/products/grid/204644060/how-do-i-view-email-headers-for-a-message


https://web.archive.org/web/20221219232959/https://mediatemple.net/community/products/all/204643950/understanding-an-email-header


Different types of malicious emails can be classified as one of the following:
 Spam - unsolicited junk emails sent out in bulk to a large number of recipients. The more malicious variant of Spam is known as MalSpam.
 Phishing -  emails sent to a target(s) purporting to be from a trusted entity to lure individuals into providing sensitive information. 
 Spear phishing - takes phishing a step further by targeting a specific individual(s) or organization seeking sensitive information.  
 Whaling - is similar to spear phishing, but it's targeted specifically to C-Level high-position individuals (CEO, CFO, etc.), and the objective is the same. 
 Smishing - takes phishing to mobile devices by targeting mobile users with specially crafted text messages. 
 Vishing - is similar to smishing, but instead of using text messages for the social engineering attack, the attacks are based on voice calls. 
 
Below are typical characteristics phishing emails have in common:
 The sender email name/address will masquerade as a trusted entity (email spoofing)
 The email subject line and/or body (text) is written with a sense of urgency or uses certain keywords such as Invoice, Suspended, etc. 
 The email body (HTML) is designed to match a trusting entity (such as Amazon)
 The email body (HTML) is poorly formatted or written (contrary from the previous point)
 The email body uses generic content, such as Dear Sir/Madam. 
 Hyperlinks (oftentimes uses URL shortening services to hide its true origin)
 A malicious attachment posing as a legitimate document
 
 
There are many reasons for spammers to embed tracking pixels (very small images) into their spam emails. 
To read more about this concept, refer to this post on The Verge - https://www.theverge.com/22288190/email-pixel-trackers-how-to-stop-images-automatic-download




Below is a checklist of the pertinent information an analyst (you) is to collect from the email header:
 Sender email address
 Sender IP address
 Reverse lookup of the sender IP address
 Email subject line
 Recipient email address (this information might be in the CC/BCC field)
 Reply-to email address (if any)
 Date/time
 
Below is a checklist of the artifacts an analyst (you) needs to collect from the email body:
 Any URL links (if an URL shortener service was used, then we'll need to obtain the real URL link)
 The name of the attachment
 The hash value of the attachment (hash type MD5 or SHA256, preferably the latter)
 
- Ferramenta que analisa cabeçalhos de e-mails: https://toolbox.googleapps.com/apps/messageheader/analyzeheader

Message Transfer Agent (MTA) is software that transfers emails between sender and recipient - https://csrc.nist.gov/glossary/term/mail_transfer_agent

Uma ferramenta importante é analisar os links de e-mails: https://www.convertcsv.com/url-extractor.htm








There are various actions a defender can take to help protect the users from falling victim to a malicious email. 

Some examples of these actions are listed below:
 - Email Security (SPF, DKIM, DMARC)
 - SPAM Filters (flags or blocks incoming emails based on reputation)
 - Email Labels (alert users that an incoming email is from an outside source)
 - Email Address/Domain/URL Blocking (based on reputation or explicit denylist)
 - Attachment Blocking (based on the extension of the attachment)
 - Attachment Sandboxing (detonating email attachments in a sandbox environment to detect malicious activity)
 - Security Awareness Training (internal phishing campaigns)
 
Per MITRE ATT&CK Framework, Phishing for Information is described as an attempt to trick targets into divulging information, and contains three sub-techniques.

Sender Policy Framework (SPF) is used to authenticate the sender of an email.
An SPF record is a DNS TXT record containing a list of the IP addresses that are allowed to send email on behalf of your domain.

(workflow SPF)

DKIM stands for DomainKeys Identified Mail and is used for the authentication of an email that’s being sent.


DMARC, (Domain-based  Message Authentication Reporting, & Conformance) an open source standard, 
uses a concept called alignment to tie the result of two other open source standards
SPF (a published list of servers that are authorized to send email on behalf of a domain) 
DKIM (a tamper-evident domain seal associated with a piece of email)


S/MIME (Secure/Multipurpose internet Mail Extensions) is a widely accepted protocol for sending digitally signed and encrypted messages.
As you can tell from the definition above, the 2 main ingredients for S/MIME are:
 - Digital Signatures
 - Encryption






In order to hire a security engineer, an organization perceives a security engineer as someone who:
 Owns the overall security of an organization. The main person responsible for securing an organization's digital assets.
 Ensures that the organization's cyber security risk is minimized at all times.
 Devises strategies and creates systems that minimize the risk posed by cyber security threats to an organization.
 Periodically conducts tests to ensure the robustness of the cyber security posture of an organization, identifies weak points, and prepares mitigations.
 Develops and implements secure network solutions.
 Architects and engineers trustworthy, reliable, and secure systems.
 Collaborates and coordinates with other teams to establish security protocols across the organization.
 
An engineer takes large problems, breaks them down into smaller chunks, and then solves them
Therefore a security engineer is someone that follows this process for security problems.
Meaning that even though you might have a job description, each day might be quite different since you are faced with various problems.

One of the primary steps in ensuring an organization's security is to maintain an organization's asset inventory
In terms of cyber security, this will mean managing and maintaining an inventory of an organization's digital assets.
They must also ensure that this asset inventory is regularly maintained and updated and includes all the required information about assets such as 
  asset name, 
  type, 
  IP addresses, 
  physical location, 
  place in the network, 
  applications running on an asset, 
  access permissions (only within the organization or public-facing), 
  and the asset owner details.
  
An organization needs robust security policies to maintain a sound security posture. 
A security engineer helps the organization create security policies based on established SI principles

A security engineer ensures that the organization is secure by design. 
The engineer understands that the security posture receives the most Return on Investment (ROI) if it follows a secure-by-design philosophy. 
 This means that the security engineer takes steps to implement a Secure Network Architecture
 ensures the organization's Windows, Linux, and Active Directory are hardened, and software development follows the Secure Software Development Lifecycle.
 
While securely designing the organization's network and infrastructure might be an excellent first step, a security engineer understands that their job is far from done after that.
They understand that security is hard work that requires continuous effort.
While a security engineer must ensure everything is done correctly, a compromise requires just one loophole to be successful.
 To mitigate risks a security engineer plans to conduct regular security assessments, audits, and red-teaming and purple-teaming exercises to continuously improve the security posture.
  While security engineers might not be performing assessments and audits themselves, they are primarily involved in 
   helping schedule these activities, 
   creating Request for Quotations (RFQs) for external parties to perform these activities, 
   and helping prioritize and implement the findings from them.
   
It must be noted that eliminating all risks might not be possible when running business operations.


Organizations keep evolving with time, which also results in changes in their security posture. 
To ensure a robust security posture, the security engineer keeps track of changes in the organization's digital assets that can affect the security posture 
and takes measures to improve the security posture with the organization's evolution. 
Let's assume an organization wants to upgrade the e-commerce module of its website for its corporate customers. The new module will require a risk assessment, penetration testing, and vulnerability assessment before integrating with the website. 
The security engineer will ensure that all these requirements are fulfilled and that the integration will not introduce security vulnerabilities. 
Furthermore, it will also be ensured that the new module follows all the security policies and guidelines laid out by the organization.

The threat landscape is continuously evolving. 
New software versions are released, and vulnerabilities are found in the older versions. 


A significant part of a security engineer's duties includes ensuring compliance with regulatory and organizational requirements. 
Depending on the industry, clientele, and location of the organization, it might be subject to various compliance standards such as PCI-DSS, HIPAA, SOC2, ISO27001, NIST-800-53, and more
A security engineer works closely with both internal and external auditors to detect any non-compliance issues and effectively address them.
Additionally, they are responsible for upholding the organization's security certifications as needed.

A security engineer might sometimes be required to configure or fine-tune security tools such as SIEMs, Firewalls, WAFs, EDRs, and more.

Tabletop exercises are often conducted to gauge the operational readiness of an organization from a security point of view.
Certain scenarios are identified to be exercised, and security team members must explain their respective roles in the scenarios under discussion. 
For example, a scenario might include the compromise of an endpoint device through a phishing email. All the team members will then explain their respective steps per the organization's playbooks.

A robust security posture requires organizations to plan for untoward incidents, disasters, or crises. 
In any such scenario, the top priority of the executive management is to maintain business continuity.







Before we start discussing the different security principles, it is vital to know the adversary against whom we are protecting our assets.
It is impossible to achieve perfect security; no solution is 100% secure.
Therefore, we try to improve our security posture to make it more difficult for our adversaries to gain access.

Before we can describe something as secure, we need to consider better what makes up security. 
When you want to judge the security of a system, you need to think in terms of the security triad: confidentiality, integrity, and availability (CIA).
 Confidentiality ensures that only the intended persons or recipients can access the data.
 Integrity aims to ensure that the data cannot be altered; moreover, we can detect any alteration if it occurs.
 Availability aims to ensure that the system or service is available when needed.
 
  Going one more step beyond the CIA security triad, we can think of:
   Authenticity: Authentic means not fraudulent or counterfeit. Authenticity is about ensuring that the document/file/data is from the claimed source.
   Nonrepudiation: Repudiate means refusing to recognize the validity of something. 
      Nonrepudiation ensures that the original source cannot deny that they are the source of a particular document/file/data. 
	  This characteristic is indispensable for various domains, such as shopping, patient diagnosis, and banking.
	  
In 1998, Donn Parker proposed the Parkerian Hexad, a set of six security elements. They are:
 Availability
 Utility
 Integrity
 Authenticity
 Confidentiality
 Possession
 
 
The security of a system is attacked through one of several means. 
It can be via the disclosure of secret data, alteration of data, or destruction of data.

The opposite of the CIA Triad would be the DAD Triad: Disclosure, Alteration, and Destruction.

 Disclosure is the opposite of confidentiality. In other words, disclosure of confidential data would be an attack on confidentiality.
 Alteration is the opposite of Integrity. For example, the integrity of a cheque is indispensable.
 Destruction/Denial is the opposite of Availability.
 
Protecting against disclosure, alteration, and destruction/denial is of utter significance. 
This protection is equivalent to working to maintain confidentiality, integrity and availability.

how can we create a system that ensures one or more security functions? The answer would be in using security models.
 we will introduce three foundational security models:
  Bell-LaPadula Model
  The Biba Integrity Model
  The Clark-Wilson Model
  
  
- The Bell-LaPadula Model aims to achieve confidentiality by specifying three rules:
  Simple Security Property: This property is referred to as “no read up”; it states that a subject at a lower security level cannot read an object at a higher security level. This rule prevents access to sensitive information above the authorized level.
  Star Security Property: This property is referred to as “no write down”; it states that a subject at a higher security level cannot write to an object at a lower security level. This rule prevents the disclosure of sensitive information to a subject of lower security level.
  Discretionary-Security Property: This property uses an access matrix to allow read and write operations. An example access matrix is shown in the table below and used in conjunction with the first two properties.
  
  
- The Biba Model aims to achieve integrity by specifying two main rules:
  Simple Integrity Property: This property is referred to as “no read down”; a higher integrity subject should not read from a lower integrity object.
  Star Integrity Property: This property is referred to as “no write up”; a lower integrity subject should not write to a higher integrity object.
  
- Clark-Wilson Model
  The Clark-Wilson Model also aims to achieve integrity by using the following concepts:
   Constrained Data Item (CDI): This refers to the data type whose integrity we want to preserve.
   Unconstrained Data Item (UDI): This refers to all data types beyond CDI, such as user and system input.
   Transformation Procedures (TPs): These procedures are programmed operations, such as read and write, and should maintain the integrity of CDIs.
   Integrity Verification Procedures (IVPs): These procedures check and ensure the validity of CDIs.
   
   
The reader can explore many additional security models. Examples include:
	Brewer and Nash model
	Goguen-Meseguer model
	Sutherland model
	Graham-Denning model
	Harrison-Ruzzo-Ullman model
	
	
ISO/IEC 19249 lists five architectural principles:
 Domain Separation
 Layering
 Encapsulation
 Redundancy
 Virtualization
 
ISO/IEC 19249 teaches five design principles:
 Least Privilege
 Attack Surface Minimisation
 Centralized Parameter validation
 Cnetralized General Security Services
 Preparing for Error and Exception Handling 
 
 
Two security principles that are of interest to us regarding trust:
  Trust but Verify
  Zero Trust

Trust but Verify: This principle teaches that we should always verify even when we trust an entity and its behaviour
Zero Trust: This principle treats trust as a vulnerability, and consequently, it caters to insider-related threats.

Vulnerability: Vulnerable means susceptible to attack or damage. In information security, a vulnerability is a weakness.
Threat: A threat is a potential danger associated with this weakness or vulnerability.
Risk: The risk is concerned with the likelihood of a threat actor exploiting a vulnerability and the consequent impact on the business.
















- Four Pilars of Information Security (IAAA)
 - Identification - is the process of verifying who the user is.
 - Authentication - is the process of ensuring that the user is who they claim to be.
 - Authorisation - determines what the user is allowed to access.
 - Accountability - tracks user activity to ensure they are responsible for their actions. 
 
- IAAA helps prevent unauthorised access, data breaches, and other security incidents.
- By implementing these best practices, organisations can protect their sensitive information and resources from internal and external threats.

- Without proper authentication, severe damage can be incurred; 
- Authentication and identification are core components of any information system and network.
 - Authentication can be in 3 things: 
  - Something you have - Something you have refers to an object, usually physical, that you have.
  - Something you are - refers to biometric readers. Examples include fingerprint readers, facial recognition, retina scanners, and voice recognition.
  - Something you Know - Something you know refers to something that you know or have memorised. 
  
- Multi-factor authentication (MFA) refers to using two or more of the above mechanisms (something you know/have/are).
- The purpose is to have additional security in case one authentication mechanism gets compromised.

- Once authenticated, a user should be granted the proper level of access.
- Authorisation specifies what the authenticated user should be allowed to access and do.

- Accountability ensures that users are accountable for the actions they perform on a system.
- after authenticating their identity and getting authorised to access a system, they can be held responsible for their actions. 
- Accountability is possible if we have auditing capabilities, which usually require proper logging functionality.
- In other words, everyone is accountable for their actions.

- Logging is the process of recording events that occur within a system. 
- This process includes user actions, system events, and errors.
- Because accountability is a crucial component of any secure infrastructure, proper care should be taken to ensure that logging is performed properly and securely. 
- logs should be tamper-proof.

- Log forwarding is the process of sending log data from one system to another. 
- This process often aggregates log data from multiple sources into a central location for more accessible analysis and management. 
- Log forwarding can also be used to send log data to a cloud-based service for storage and analysis.

- Security Information and Event Management (SIEM) is a technology that aggregates log data from multiple sources and analyses it for signs of security threats.
- SIEM solutions can help organisations identify anomalies, detect potential security incidents, and provide alerts to security teams.

- Identity Management (IdM) includes all the necessary policies and technologies for identification, authentication, and authorisation.
- IdM aims to ensure that authorised people have access to the assets and resources needed for their work while unauthorised people are denied access. 

- IdM is an essential component of cybersecurity that refers to the process of managing and controlling digital identities.
- It involves the management of user identities, their authentication, authorisation, and access control.
- IdM systems are used to manage user identities across an organisation’s network.

- IAM is a more comprehensive concept than IdM. 
- Identity and Access Management (IAM)
- It encompasses all the processes and technologies to manage and secure digital identities and access rights.
- IAM systems ensure that only authorised users have access to specific resources and data and that their access is monitored and controlled.
- IAM systems use various technologies to manage access, including role-based access control, multi-factor authentication, and single sign-on.
 - IAM systems help organisations comply with regulatory requirements such as HIPAA, GDPR, and PCI DSS. 
 - They provide functionalities to manage the lifecycle of user identities, including onboarding, offboarding, and access revocation.

- IdM and IAM are essential components of cybersecurity.
- They ensure that only authorised individuals have access to specific resources and information.

- Replay Attack: Uso da senha criptografada para logar em um serviço / sistema 

- An encrypted password that is always the same value is easy to circumvent.
- We need some mechanism to ensure that the response won’t be reused repeatedly. 
- One approach would be to use the current time and date as part of the response.
- In other words, the user would send an encryption of the current time (and date) along with the password.

- A system controls access to various resources based on the chosen model. Some of the common access control models are:
 - Discretionary Access Control (DAC)
 - Role-Based Access Control (RBAC)
 - Mandatory Access Control (MAC)
 
- Many have already used Discretionary Access Control (DAC) when sharing files or folders with friends and colleagues.
 - When using DAC, the resource owner will explicitly add users with the proper permissions.
 - The whole process is straightforward and fully controlled by the data owner.
 
- Role-Based Access Control (RBAC) uses a very intuitive approach.
 - Each user has one or more roles or functional positions; furthermore, they are authorised to access different resources based on their roles.
 - An accountant needs to access the company accounting books but does not need to access research and development labs or documents.
 
- An operating system using Mandatory Access Control (MAC) would prioritise security and significantly limit users’ abilities. 

- SSO - Instead of a user having to remember multiple usernames and passwords, they only need to remember a single set of login credentials.
- They can authenticate themselves to one system, granting them access to the other systems necessary for their work.
- require the user to log in once and grant them access to all the needed services; that’s what SSO does.
 - SSO allows organisations to authenticate users once before granting them access to the resources required for their work. We can achieve many advantages from this. We will mention a few.
  - One strong password: Expecting a user to remember a single strong password is more acceptable than asking them to remember ten different strong passwords.
  - Easier MFA: Adding MFA to every different service is a humongous task to accomplish and maintain. With SSO, MFA needs to be enabled and configured once.
  - Simpler Support: Support requests like password reset become more straightforward as they are now confined to a single account.
  - Efficiency: A user does not need to log in every time they need to access a new service.










- By adopting a proactive and strategic stance towards cyber security, organisations can mitigate the risks posed by malicious actors and safeguard their sensitive systems against potentially catastrophic breaches.

- Governance: Managing and directing an organisation or system to achieve its objectives and ensure compliance with laws, regulations, and standards.
- Regulation: A rule or law enforced by a governing body to ensure compliance and protect against harm.
- Compliance: The state of adhering to laws, regulations, and standards that apply to an organisation or system.

- Information security governance represents an organisation's established structure, policies, methods, and guidelines designed to guarantee the privacy, reliability, and accessibility of its information assets.


- Information security governance falls under the purview of top-tier management and includes the following processes:
 - Strategy: Developing and implementing a comprehensive information security strategy that aligns with the organisation's overall business objectives.
 - Policies and procedures: Preparing policies and procedures that govern the use and protection of information assets.
 - Risk management: Conduct risk assessments to identify potential threats to the organisation's information assets and implement risk mitigation measures.
 - Performance measurement: Establishing metrics and key performance indicators (KPIs) to measure the effectiveness of the information security governance program.
 - Compliance: Ensuring compliance with relevant regulations and industry best practices.
 
- The following are the benefits of implementing governance and regulation:
 - More Robust Security Posture
 - Increased Stakeholder Confidence
 - Regulatory Compliance
 - Better alignment with business objectives
 - Informed decision-making
 - Competitive advantage
 




- The information security framework provides a comprehensive set of documents that outline the organisation's approach to information security and governs how security is implemented, managed, and enforced within the organisation.
 - Policies: A formal statement that outlines an organisation's goals, principles, and guidelines for achieving specific objectives.
 - Standards: A document establishing specific requirements or specifications for a particular process, product, or service.
 - Guidelines: A document that provides recommendations and best practices (non-mandatory) for achieving specific goals or objectives.
 - Procedures: Set of specific steps for undertaking a particular task or process.
 - Baselines: A set of minimum security standards or requirements that an organisation or system must meet.
 
- Here are some generalised steps used to develop policies, standards, guidelines, etc.
 - Identity the scope and purpose 
 - Research and review
 - Draft the document
 - Review and approval
 - Implementation and communication
 - Review and update
 
- Preparing a Password Policy 
 - Define password requirements: Minimum length, complexity, and expiration.
 - Define password usage guidelines: Specify how passwords should be used, such as requiring unique passwords for each account, prohibiting the sharing of passwords, and prohibiting default passwords.
 - Define password storage and transmission guidelines: Using encryption for password storage and requiring secure connections for password transmission.
 - Define password change and reset guidelines: How often passwords should be changed etc. 
 - Communicate the policy: Communicate the password policy to all relevant employees and stakeholders, and ensure that they understand the requirements and guidelines. Develop training and awareness programs to ensure that employees follow the policy.
 - Monitor compliance: Monitor compliance with the password policy and adjust the policy as needed based on feedback and changes in the threat landscape or regulatory environment.
 
- Making an Incident Response Procedure
 - Define incident types: Unauthorised access, malware infections, or data breaches.
 - Define incident response roles and responsibilities: Identify the stakeholders,  such as incident response team members, IT personnel, legal and compliance teams, and senior management. 
 - Detailed Steps: Develop step-by-step procedures for responding to each type of incident,  including initial response steps, such as containing the incident and preserving evidence; analysis and investigation steps, such as identifying the root cause and assessing the impact; response and recovery steps, such as mitigating the incident, reporting and restoring normal operations.
 - Report the incident to management and document the incident response process for future reference.
 - Communicate the incident response procedures.
 - Review and update the incident response procedures.
 

- Governance and Risk Compliance (GRC) framework
 - It focuses on steering the organisation's overall governance, enterprise risk management, and compliance in an integrated manner. 
 - It is a holistic approach to information security that aligns with the organisation's goals and objectives and helps to ensure that the organisation operates within the boundaries of relevant regulations and industry standards. 
  - GRC framework has the following three components:
   - Governance Component: Involves guiding an organisation by setting its direction through information security strategy,  which includes policies, standards, baselines, frameworks, etc.
   - Risk Management Component: Involves identifying, assessing, and prioritising risks to the organisation and implementing controls and mitigation strategies to manage those risks effectively.
   - Compliance Component: Ensuring that the organisation meets its legal, regulatory, and industry obligations and that its activities align with its policies and procedures. 
   
- Developing and implementing a GRC framework involves various steps:
 - Define the scope and objectives: This step involves determining the scope of the GRC program and defining its goals.
 - Conduct a risk assessment: In this step, the organisation identifies and assesses its cyber risks.
 - Develop policies and procedures: Policies and procedures are developed to guide cyber security practices within the organisation.
 - Establish governance processes: Governance processes ensure the GRC program is effectively managed and controlled.
 - Implement controls: Technical and non-technical controls are implemented to mitigate risks identified in risk assessment.
 - Monitor and measure performance: Processes are established to monitor and measure the effectiveness of the GRC program.
 - Continuously improve: The GRC program is constantly reviewed and improved based on performance metrics, changing risk profiles, and stakeholder feedback. 
 
- Personal data is "Any data associated with an individual that can be utilised to identify them either directly or indirectly"
 - Key points of the law include the following:
  - Prior approval must be obtained before collecting any personal data.
  - Personal data should be kept to a minimum and only collected when necessary.
  - Adequate measures are to be adopted to protect stored personal data.

- NIST 800-53 is a publication titled "Security and Privacy Controls for Information Systems and Organisations",
- developed by the National Institute of Standards and Technology (NIST)
 - that provides a catalogue of security controls to protect the CIA triad of information systems.

- First and foremost, businesses must conduct a thorough discovery process to recognise and catalogue their data assets, information systems, and associated threats.
- This includes understanding data flows, system dependencies, and potential vulnerabilities.

- ﻿The strategic planning, execution, and continuous administration of security measures are all part of Information Security (IS) management
- which protects information assets from unauthorised access, use, disclosure, interruption, alteration, and destruction.
 - compliance refers to observing information security-related legal, regulatory, contractual, and industry-specific standards.
 
- ISO 27001 is an internationally recognised standard for requirements to plan, develop, run, and update an organisation's Information Security Management System (ISMS)
 - It has the following core components:
  - Scope: This specifies the ISMS's boundaries, including the covered assets and processes.
  - Information security policy: A high-level document defining an organisation's information security approach.
  - Risk assessment: Involves identifying and evaluating the risks to the confidentiality, integrity, and availability of the organisation's information.
  - Risk treatment: Involves selecting and implementing controls to reduce the identified risks to an acceptable level.
  - Statement of Applicability (SoA): This document specifies which controls from the standard are applicable and which are not.
  - Internal audit: This involves conducting periodic audits of the ISMS to ensure that it is operating effectively.
  - Management review: Review the performance of ISMS at regular intervals.
  


- Regular monitoring, measurement, and continual development are crucial to guarantee the efficacy and continued alignment of the ISMS with the organization's objectives.

 
- SOC 2 was developed by the American Institute of Certified Public Accountants (AICPA) as a compliance/auditing framework. 
- It focuses on assessing the efficacy of a company's data security based on the CIA triad.
- SOC 2 can reassure customers, stakeholders, and business partners that the company has put sufficient controls in place to safeguard its systems, data, and sensitive information.
- The SOC 2 framework is essential for service providers interacting with client data or offering solutions that process, store, or transmit sensitive data.

- The primary purpose of the SOC 2 audit is to ensure that third-party service providers store and process sensitive information securely. 
- The following steps can be taken by an organisation’s management team before and during an audit:
 - Determine the scope: This may include specific systems, processes, or locations that are relevant to the security and privacy of customer data.
 - Choose a suitable auditor: Select a qualified auditor with experience conducting SOC 2 audits for financial companies. Consider factors such as the auditor's reputation, experience, and availability.
 - Plan the audit: Work with the auditor to plan the audit, including the audit timeline, the scope of the audit, and the audit criteria.
 - Prepare for the audit: Prepare for the audit by reviewing your security and privacy controls, policies, and procedures. Identify any gaps or deficiencies and develop a plan to address them.
 - Conduct the audit: The auditor will review your controls and conduct testing to assess their effectiveness. The audit may include interviews with key personnel, documentation reviews, and controls tests.
 - Receive the audit report: Once the audit is complete, the auditor will provide a report detailing the audit results. The report may include a description of your controls, any deficiencies or gaps identified, and recommendations for improvement.








- Threat modelling is a systematic approach to identifying, prioritising, and addressing potential security threats across the organisation.
- By simulating possible attack scenarios and assessing the existing vulnerabilities of the organisation's interconnected systems and applications, threat modelling enables organisations to develop proactive security measures and make informed decisions about resource allocation. 

- Threat modelling aims to reduce an organisation's overall risk exposure by identifying vulnerabilities and potential attack vectors, allowing for adequate security controls and strategies.



- High Level Process:
 - Defining the scope
 - Asset Identification
 - Identify Threats
 - Analyse Vulnerabilities and Prioritise Risks
 - Develop and Implement Countermeasures
 - Monitor and Evaluate 

- In addition to the high-level methodology discussed above, creating an attack tree is another good way to identify and map threats.
 - An ﻿attack tree is a graphical representation used in threat modelling to systematically describe and analyse potential threats against a system, application or infrastructure.
 - It provides a structured, hierarchical approach to breaking down attack scenarios into smaller components.
 
- MITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) is a comprehensive, globally accessible knowledge base of cyber adversary behaviour and tactics. 
 - It contains:
  1. Technique Name and Details
  2. Procedure Examples
  3. Mitigations
  4. Detections
  5. References
  
- The MITRE ATT&CK Navigator is an open-source, web-based tool that helps visualise and navigate the complex landscape of the MITRE ATT&CK Framework.





- The DREAD framework is a risk assessment model developed by Microsoft to evaluate and prioritise security threats and vulnerabilities. It is an acronym that stands for:



- The categories are commonly phrased with the following questions to ingest the definitions provided above quickly:
 - Damage - How bad would an attack be?
 - Reproducibility - How easy is it to reproduce the attack?
 - Exploitability - How much work is it to launch the attack?
 - Affected Users - How many people will be impacted?
 - Discoverability - How easy is it to discover the vulnerability?
 
- As mentioned above, the DREAD framework is an opinion-based model that heavily relies on an analyst's interpretation and assessment.

- ﻿The DREAD Framework is typically used for Qualitative Risk Analysis




- The STRIDE framework is a threat modelling methodology also developed by Microsoft, which helps identify and categorise potential security threats in software development and system design.
 - The acronym STRIDE is based on six categories of threats, namely:



- Categories:

Spoofing 
 Sending an email as another user.
 Creating a phishing website mimicking a legitimate one to harvest user credentials.
 
Tampering
 Updating the password of another user.
 Installing system-wide backdoors using an elevated access.
 
Repudiation
 Denying unauthorised money-transfer transactions, wherein the system lacks auditing.
 Denying sending an offensive message to another person, wherein the person lacks proof of receiving one.
 
Information Disclosure 
 Unauthenticated access to a misconfigured database that contains sensitive customer information.
 Accessing public cloud storage that handles sensitive documents.
 
Denial of Service
 Flooding a web server with many requests, overwhelming its resources, and making it unavailable to legitimate users.
 Deploying a ransomware that encrypts all system data that prevents other systems from accessing the resources the compromised server needs.
 
Elevation of Privilege
 Creating a regular user but being able to access the administrator console.
 Gaining local administrator privileges on a machine by abusing unpatched systems.


- Teams and Responsabilities:








- ﻿PASTA, or Process for Attack Simulation and Threat Analysis, is a structured, risk-centric threat modelling framework designed to help organisations identify and evaluate security threats and vulnerabilities within their systems, applications, or infrastructure. 


- Below is an overview of the seven-step methodology of the PASTA Framework.

1. Define the Objectives
2. Define the Technical Scope
3. Decompose the Application
4. Analyse the Threats
5. Vulnerabilities and Weaknesses Analysis
6. Analyse the Attacks
7. Risk and Impact Analysis 

- PASTA Methodology Guidelines









- Basic Terminology 
 - Threat: an intentional or accidental event that can compromise the security of an information system. Examples include hacking, phishing attacks, human error, and natural disasters. (human-made, technical, or natural.)
 - Vulnerability: a software, hardware, or network weakness that cybercriminals can exploit to gain unauthorised access or compromise a system.
 - Asset: a valuable resource or component (tangible or intangible) that an organisation relies upon to achieve its objectives.
 - Risk: the probability of a threat source exploiting an existing vulnerability and resulting in adverse business effects.
 - Risk Management (RM): the process of identifying, assessing, and mitigating risk to maintain acceptable levels.
 
 
- A Risk Management Policy is a set of procedures and processes designed to minimise the chances of an adverse event or outcome for an organisation.

- Information Systems Risk Management is a system of policies, procedures, and practices that seek to protect a company’s computer system from various internal and external threats.

- There are several frameworks for risk assessment. Example methodologies are:
 - NIST SP800-30
 - Facilitated Risk Analysis Process (FRAP)
 - Operationally Critical threat, asset, and vulnerability evaluation (OCTAVE)
 - Failure Modes and Effect Analysis (FMEA)
 
- Based on NIST SP 800-30, the risk management process entails four steps:
 - Frame Risk
 - Assess Risk
 - Respond to Risk
 - Monitor Risk 
 
- To create a reasonable risk frame, organisations must identify the following:
 - Risk Assumptions
 - Risk Constrains
 - Risk Tolerance
 - Priorities and Trade-offs
 
- The goal of the risk assessment is to determine the following:
 - Threats: What are the threats that you need to consider?
 - Vulnerabilities: What are the vulnerabilities that you have to deal with?
 - Impact: What would be the impact if a threat exploited a vulnerability?
 - Likelihood: What is the likelihood of this vulnerability being exploited?
 
- We have two approaches when it comes to risk analysis:
 - Qualitative Risk Analysis, where we assign ratings to risks. The ratings can be a qualitative adjective, such as high, medium, and low. Alternatively, it can be something symbolic, such as red, yellow, and green.
 - Quantitative Risk Analysis, where we assign monetary values and use that as a basis for decision-making.
 
- The response you choose against the risk takes into account the severity of the threat, the probability of occurrence, and the costs of the possible countermeasures.
 - Avoid Risk
 - Transfer Risk
 - Mitigate Risk
 - Accept Risk 
 
- monitoring risks activities requires a focus on the following areas:
 - Effectiveness
 - Change
 - Compliance





One of the simplest ciphers is the Caesar cipher, used more than 2000 years ago.
Another type of cipher is called transposition cipher, which encrypts the message by changing the order of the letters.

If the encrypted message can be broken in one week, the encryption used would be considered insecure. 
However, if the encrypted message can be broken in 1 million years, the encryption would be considered practically secure.


Cryptographic Algorithm or Cipher: This algorithm defines the encryption and decryption processes.
Key: The cryptographic algorithm needs a key to convert the plaintext into ciphertext and vice versa.
plaintext is the original message that we want to encrypt
ciphertext is the message in its encrypted form


On the other end, the recipient provides the decrypt process with the same key used by the sender to recover the original plaintext from the received ciphertext. 
Without knowledge of the key, the recipient won’t be able to recover the plaintext.


National Institute of Standard and Technology (NIST) published the Data Encryption Standard (DES) in 1977.
DES is a symmetric encryption algorithm that uses a key size of 56 bits.

NIST published the Advanced Encryption Standard (AES) in 2001.
Like DES, it is a symmetric encryption algorithm; however, it uses a key size of 128, 192, or 256 bits, and it is still considered secure and in use today

https://csrc.nist.gov/publications/detail/fips/197/final

(tabela de algoritmos seguros)

All the algorithms mentioned so far are block cipher symmetric encryption algorithms. 
A block cipher algorithm converts the input (plaintext) into blocks and encrypts each block.
A block is usually 128 bits.



There are many programs available for symmetric encryption. We will focus on two, which are widely used for asymmetric encryption as well:
GNU Privacy Guard
OpenSSL Project


Symmetric encryption requires the users to find a secure channel to exchange keys. 
In other words, we need a channel where no third party can eavesdrop and read the traffic;

When using an asymmetric encryption algorithm, we would generate a key pair: a public key and a private key.
The public key is shared with the world, or more specifically, with the people who want to communicate with us securely.

The private key must be saved securely, and we must never let anyone access it.


- Symmetric encryption requires the users to find a secure channel to exchange keys.
- By secure channel, we are mainly concerned with confidentiality and integrity.

- Asymmetric encryption makes it possible to exchange encrypted messages without a secure channel
- we just need a reliable channel.
- By reliable channel, we mean that we are mainly concerned with the channel’s integrity and not confidentiality.

- When using an asymmetric encryption algorithm, we would generate a key pair: a public key and a private key.
- The public key is shared with the world, or more specifically, with the people who want to communicate with us securely.
- The private key must be saved securely, and we must never let anyone access it.
- If a message is encrypted with one key, it can be decrypted with the other. 
 - If Alice encrypts a message using Bob’s public key, it can be decrypted only using Bob’s private key.
 - Reversely, if Bob encrypts a message using his private key, it can only be decrypted using Bob’s public key.
 
- In practice, symmetric encryption algorithms allow faster operations than asymmetric encryption

- Beyond confidentiality, asymmetric encryption can solve integrity, authenticity and nonrepudiation.
- asymmetric encryption can be relatively slow to encrypt large files and vast amounts of data. 

- Diffie-Hellman is an asymmetric encryption algorithm.
- It allows the exchange of a secret over a public channel. 


- A cryptographic hash function is an algorithm that takes data of arbitrary size as its input and returns a fixed size value, called message digest or checksum, as its output. 

- Some older hash functions, such as MD5 (Message Digest 5) and SHA-1, are cryptographically broken.
- By broken, we mean that it is possible to generate a different file with the same checksum as a given file. 
- This means that we can create a hash collision.

- Hash-based message authentication code (HMAC) is a message authentication code (MAC) that uses a cryptographic key in addition to a hash function.

- For a certificate to get signed by a certificate authority, we need to:
 - Generate Certificate Signing Request (CSR): You create a certificate and send your public key to be signed by a third party.
 - Send your CSR to a Certificate Authority (CA): The purpose is for the CA to sign your certificate. The alternative and usually insecure solution would be to self-sign your certificate.
 
- For this to work, the recipient should recognize and trust the CA that signed the certificate. 
- And as we would expect, our browser trusts DigiCert Inc as a signing authority; otherwise, it would have issued a security warning instead of proceeding to the requested website.
- You can use openssl to generate a certificate signing request using the command openssl req -new -nodes -newkey rsa:4096 -keyout key.pem -out cert.csr
 - Once the CSR file is ready, you can send it to a CA of your choice to get it signed and ready to use on your server.
 - openssl req -x509 -newkey -nodes rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 365 (Create a selfsigned certifiate)
 
 
 https://cheatsheetseries.owasp.org/cheatsheets/Password_Storage_Cheat_Sheet.html
 
- Cryptography’s role starts with checking the certificate. 
- For a certificate to be considered valid, it means it is signed. 
- Signing means that a hash of the certificate is encrypted with the private key of a trusted third party; the encrypted hash is appended to the certificate.





- a vulnerability is defined as "A weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source".

- Vulnerability management is an ongoing, proactive, and frequently automated activity that protects computer systems, networks, and enterprise solutions from cyberattacks and data breaches. 
- Vulnerability Scanning 
 - Since vulnerability management is the process surrounding vulnerability scanning, it is essential to know how vulnerability scans are conducted and the tools at hand.
 - Some popular commercial vulnerability scanning tools include Nessus, Nexpose, and Acunetix. On the other hand, some good open-source solutions like Greenbone (community edition), OWASP ZAP and many more. 

- vulnerability management is generally viewed as an open, standards-based approach employing the National Institute of Standards and Technology's (NIST) Security Content Automation Protocol (SCAP) standard.
 - The primary components of SCAP are as follows: 
  - Common Vulnerabilities and Exposures (CVE): MITRE maintains the CVE list of publicly documented vulnerabilities and exposures. 
   - CVE Details is also a renowned website for searching CVEs and their impact. ( https://www.cve.org/ e https://www.cvedetails.com/ )
   
   
- Common Configuration Enumeration (CCE): A CCE gives system configuration issues unique identifiers to quickly and accurately link configuration data from different information sources and tools. 
 - For instance, CCE identifiers can be used to match up configuration assessment tool results with recommended best practices.
 - This is comparable to the CVE list, which gives publicly reported system vulnerability IDs.
 
- Common Platform Enumeration (CPE): CPE is a method for classifying and identifying devices, operating systems (OS), and application types inside an infrastructure.
 - CPE is widely used in security and vulnerability management tools to identify various assets and to take accurate automated decisions through correlation with CVE and CCE.
 
- Common Vulnerability Scoring System (CVSS): CVSS is a scoring system that rates the severity of vulnerabilities and identifies their characteristics.
 - It assigns severity scores to all defined vulnerabilities, which is used to prioritise mitigation efforts and the required resources based on the severity.
 
- There are numerous public sites with information on vulnerabilities; however, the National Vulnerability Database (NVD) administered by NIST is a comprehensive database of CVE-assigned known vulnerabilities.
 - https://nvd.nist.gov/
 
 
- There are six essential phases in the vulnerability management lifecycle that can be mapped out from the NIST Cybersecurity Framework;
 - https://www.nist.gov/cyberframework
  1. Discover
  2. Prioritize
  3. Assess
  4. Report
  5. Remediate
  6. Verify
  
  
  
  
- Before reporting a vulnerability for remediation, it is highly advised to confirm that it is not a false positive since vulnerability scanners are prone to such errors.


- This task will briefly discuss a renowned framework used worldwide for vulnerability management.
 - The National Institute of Standards and Technology (NIST) created the Cybersecurity Framework (CSF) as a guidance for organisations to better manage and reduce their cybersecurity risks.
 - https://www.nist.gov/cyberframework
 








- Networking is one of the most critical components of a corporate environment but can often be overlooked from a security standpoint.
 - A properly designed network permits not only internet usage and device communication but also redundancy, optimization, and security.
 
- In a well-designed network, if a switch goes down, then packets can be redistributed through another route with no loss in uptime.
 - If a web server is compromised, it cannot traverse the network and access important information. 
 - A system administrator should be confident that their servers are secure if a random device joins a network, knowing that the device is segmented from the rest of the network and cannot access those systems.
 
- VLANs (Virtual LAN) are used to segment portions of a network at layer two and differentiate devices. 
 - VLANs are configured on a switch by adding a "tag" to a frame. The 802.1q or dot1q tag will designate the VLAN that the traffic originated from.

https://www.openvswitch.org/ - Virtual Switch para estudos 

- But how does a VLAN connect to the internet or access resources in other VLANS? 
 - Since they are segmented, they cannot communicate outside their tagged devices. 
 - Just as routers are used to communicate between traditional networks, routers can be used to route between VLANs.
  - Before modern solutions were introduced, network engineers would physically connect a switch and router separately for each VLAN present. Nowadays, that problem is solved through the ROAS (Router on a Stick) design.
  - VLANs are configured to communicate with a router through a designated interface of a switch, known as a switch port.
   - The connection between the switch and router is known as a trunk. 
   - VLANs are routed through the switch port, requiring only one trunk/connection between the switch and router, hence, "on a stick."
  
- With the introduction of VLANs, there is a shift in network architecture design to include security as a key consideration. 
 - Security, optimization, and redundancy should all be considered when designing a network, ideally without compromising one component.
 
- This brings us to the question, how do we properly implement VLANs as a security boundary? 
 - Security zones! Security zones define what or who is in a VLAN and how traffic can travel in and out.
 - Depending on whom you speak to, every network architect may have a different approach/opinion to the language or requirements surrounding security zones. 
 


- While security zones mostly factor in what will happen internally, it is equally important to consider how new traffic or devices will enter the network, be assigned, and interact with internal systems. 

- Security zones and access controls will physically direct how and where traffic goes.
 -  But how is it decided what resources users or devices have access to? Traffic rules are often governed by company security policy or compliance as equally as security controls that determine access permissions. 
 
- ACL(s) (Access Control List(s)).
 - An ACL is used as a loose standard to create a ruleset for different implementations and access control protocols. 
 - An ACL contains ACE(s) (Access Control Entry) or rules that define a list’s profile based on pre-defined criteria (source address, destination address, etc.)
 

- Traffic correlation is standardized as the state of a packet, e.g. (protocol, process, direction, etc.)
 - We must employ a firewall to parse the state of a packet and enforce policies based on that state.
 
- Zone-pairs are a direction-based and stateful policy that will enforce the traffic in single directions per each VLAN, hence, zone-pair. 
 - For example, DMZ → LAN or LAN → DMZ.
 
- SSL/TLS inspection uses an SSL proxy to intercept protocols, including HTTP, POP3, SMTP, or other SSL/TLS encrypted traffic
 - Once intercepted, the proxy will decrypt the traffic and send it to be processed by a UTM (Unified Threat Management) platform. 
 - UTM solutions will employ deep SSL inspection, feeding the decrypted traffic from the proxy into other UTM services, including but not limited to web filters or IPS (Intrusion Prevention System), to process the information.
 
- Cisco defines DHCP snooping as "a security feature that acts like a firewall between untrusted hosts and trusted DHCP servers."
 - DHCP snooping was introduced to combat rogue DHCP servers; 
 - it will validate and rate-limit DHCP traffic as necessary.
 - If a host is untrusted, its traffic will be filtered and rate-limited.
 
- Cisco defines ARP inspection as "a security feature that validates Address Resolution Protocol (ARP) packets in a network."
 - ARP inspection will validate and rate-limit ARP packets as necessary; if an ARP packet's MAC and IP address do not match, the protocol will intercept, log, and discard the packet.
 - ARP inspection uses the DHCP binding database filled from DHCP snooping as its list of binding IP addresses.



- It is evident that we need to ensure physical security for our computer systems; however, in the unlikely event that physical security is breached, we need to provide additional layers of protection. 
 - Many BIOS and UEFI firmware allows you to add a boot password. 
 
- We can consider adding a GRUB password depending on the Linux system we want to protect. Many tools help achieve that. 
 - One tool is grub2-mkpasswd-pbkdf2, which prompts you to input your password twice and generates a hash for you.
 - This configuration would prevent unauthorised users from resetting your root password.
  - It is important to note that adding a password for GRUB is not available for systems deployed using cloud service providers (such as our Linux VM);
  
  
- Encryption makes data unreadable without the decryption key.
 - A disk drive full of encrypted data should be as good as a damaged one.
 - many modern Linux distributions ship with LUKS (Linux Unified Key Setup)
 
- We have the following fields:

LUKS phdr: It stands for LUKS Partition Header. LUKS phdr stores information about the UUID (Universally Unique Identifier), the used cipher, the cipher mode, the key length, and the checksum of the master key.

KM: KM stands for Key Material, where we have KM1, KM2, …, KM8. Each key material section is associated with a key slot, which can be indicated as active in the LUKS phdr. 
    When the key slot is active, the associated key material section contains a copy of the master key encrypted with a user's password. 
	In other words, we might have the master key encrypted with the first user's password and saved in KM1, encrypted with the second user's password and saved in KM2, and so on.
	
Bulk Data: This refers to the data encrypted by the master key. The master key is saved and encrypted by the user's password in a key material section.


- The first Linux firewall was a packet filtering firewall, i.e., a stateless firewall.
 - A stateless firewall can inspect certain fields in the IP and TCP/UDP headers to decide upon a packet but does not maintain information about ongoing TCP connections
 
- Current Linux firewalls are stateful firewalls; 
 - they keep track of ongoing connections and restrict packets based on specific fields in the IP and TCP/UDP headers and based on whether the packet is part of an ongoing connection.
 
- It is worth noting that it is impossible to allow and deny packets based on the process but instead on the port number. 
 - This limitation differs from MS Windows’ built-in firewall, which can restrict and allow traffic per application.
 
- On a Linux system, a solution such as SELinux or AppArmor can be used for more granular control over processes and their network access.
 - For example, we can allow only the /usr/bin/apache2 binary to use ports 80 and 443 while preventing any other binary from doing so on the underlying system
 
-  The netfilter project provides packet-filtering software for the Linux kernel 2.4.x and later versions.

- UFW stands for uncomplicated firewall. 



- Providing remote access to a system is a very convenient way to access your system and files when you are not physically present at the target system’s keyboard.
- However, this also means that you are voluntarily providing a service that attackers will target. Common attacks include:
 - Password sniffing
 - Password guessing and brute-forcing
 - Exploiting the listening service
 
- When you set up your Linux system with SSH for remote administration, you also make your Linux box available for all interested parties.

- There are a few guidelines that you can use:
 - Disable remote login as root; force login as non-root users.
 - Disable password authentication; force public key authentication instead.
  - The configuration of the OpenSSH server can be controlled via the sshd_config file, usually located at /etc/ssh/sshd_config
  
  
  
- Using a non-root account for everyday work is recommended to avoid sabotaging your system.
 - To avoid logging in as root, the better approach would be to have an account -created for administrative purposes- added to the sudoers, i.e. group who can use the sudo command.
 
- A straightforward way is to modify the /etc/passwd and change the root shell to /sbin/nologin
 - So the "root" user cant login 
 
- The libpwquality library provides many options for password constraints. The configuration file can be found at:
 - /etc/security/pwquality.conf on RedHat and Fedora
 - /etc/pam.d/common-password on Debian and Ubuntu.
 
- As part of system maintenance, it is vital to disable user accounts that no longer need access to the system in question. 
 - For instance, these users might have moved to another department or quit the company.
   - We should do the same for local services.
   - In other words, we should set the shell to sbin/nologin for all the local service accounts such as www-data, mongo, and nginx, to name a few. 
   
   
- Every piece of software you install on your system also increases the number of potential vulnerabilities.
 - One of the easiest ways to improve your security posture is by removing or disabling unneeded services and packages.
 - In simple terms, we need to minimise the number of installed system packages as every package carries some risk, and we cannot know when a related vulnerability will be discovered. 
 - The best policy is to avoid installing unneeded packages.
 
- After you remove any packages that are not required and disable preinstalled services that might not be removed, it is critical to set your firewall rules accordingly. 

- Avoid Legacy Protocols
 - At one point in the past, Telnet was the primary protocol to remote access a system; 
 - the TFTP protocol was commonly used to transfer files. Such protocols should no longer be allowed as secure alternatives have been released.
 
- Remove Identification Strings
 - Whenever you connect to a remote server, it usually replies with its version number.
 - This information would reveal various information to the attacker, such as the name of the server/program, the version number, and the host operating system.  
 
- It is vital that you keep your system updated with the latest security patches and bug fixes.
 - You can update a Debian-based distribution, such as Ubuntu, with the following two commands:
  - apt update to download package information from the configured sources
  - apt upgrade to install available upgrades for all packages from the configured sources
  
- You can update a RedHat or Fedora system using the following:
 - dnf update on newer releases (Red Hat Enterprise Linux 8 and later)
 - yum update on older releases (Red Hat Enterprise Linux 7 and earlier)
 
 
 
- Ubuntu releases a Long Term Support (LTS) version every two years.  



Most log files on Linux systems are stored in the /var/log directory. Here are a few of the logs that can be referenced when looking into threats:
 /var/log/messages - a general log for Linux systems
 /var/log/auth.log - a log file that lists all authentication attempts (Debian-based systems)
 /var/log/secure - a log file that lists all authentication attempts (Red Hat and Fedora-based systems)
 /var/log/utmp - an access log that contains information regarding users that are currently logged into the system
 /var/log/wtmp - an access log that contains information for all users that have logged in and out of the system
 /var/log/kern.log - a log file containing messages from the kernel
 /var/log/boot.log - a log file that contains start-up messages and boot information











At its most basic level, virtualization is the concept of encapsulating the capabilities and features of a physical machine in a virtual environment, known as a virtual machine.

virtualization comes from a need of the following:
 Decrease expenses
 Scale
 Efficiency 
 
 
Formally, virtualization abstracts or creates an abstraction layer over computer hardware.
An abstraction layer allows a single device to be divided into multiple virtual computers, also known as virtual machines (VMs).

A hypervisor provides the ability to create the abstraction layer between hardware and software.
will also generally include some form of management application or software to provide an interface between the end user and the abstraction layer to create or load virtual machines.

Type 1 hypervisors, also known as bare metal hypervisors, create an abstraction layer directly between hardware and virtual machines without a common operating system between them
Instead, the hypervisor is the operating system and is often headless, with only a web-based management portal remotely accessed. 
These hypervisors are designed for scale and to deploy a large number of virtual machines at once.

Type 2 hypervisors, also known as hosted hypervisors, create an abstraction layer from a software application built on top of a pre-existing operating system.
type 2 hypervisors are often managed directly from the application through a GUI.


Hypervisors work as expected for a large number of use cases but begin to encounter issues when scaling lightweight applications.
Microservices give us a good example of an application architecture that encounters issues when deployed from a hypervisor.
A microservice is an application structure that is broken up into smaller services that are scalable and use lightweight protocols and features.

Containers are the current solution to the issues encountered with hypervisors at scale.

Containers have a lot in common with virtual machines, but instead of being completely abstracted from the host operating system, containers share some properties with the host operating system.
Containers have their own filesystem, a portion of computing resources (CPU, RAM), a process space, and more. 
Apart from the obvious benefits of being lightweight, containers are also portable and robust because they are not completely abstracted. 

Container engines are our second type of virtualization. 
As virtual machines use a hypervisor to create an abstraction layer for virtualization, containers use a container engine to create an abstraction layer using logical resources.

Docker is a container platform and engine that is used to run Docker "images" as containers.
Each Docker image is built of a base image, such as Alpine or Ubuntu, that is specifically built for use in containers and is lightweight.
To build a Docker image, a Dockerfile must be created, which defines the base image for a container and any commands to be run.


Docker Hub is a remote repository for Docker images, similar to GitHub - a remote repository for Git.

Through the use of hypervisors and containers, most problems associated with traditional computing are resolved, such as cost and efficiency.
 what if we need a faster and more scalable solution? That is, as load or other criteria changes, the resources or the number of instances allocated to the application or service increase or decrease on the fly as needed.
 Kubernetes, also shortened to "K8s," is one such solution known as an orchestration platform. 
 An orchestration platform aims to integrate into other products, such as Docker, and extend their capabilities or "synchronize" them with other products or applications.
 Kubernetes relies on these traditional virtualization models like hypervisors and containers and extends their uses, features, and capabilities.
  Horizontal Scaling
  Extensibility
  Self-healing
  Automated rollouts and rollbacks 















The Windows registry is a unified container database that stores configurational settings, essential keys and shared preferences for Windows and third-party applications.
Usually, on the installation of most applications, it uses a registry editor for storing various states of the application. 

Event Viewer is an app that shows log details about all events occurring on your computer, including driver updates, hardware failures, changes in the operating system, invalid authentication attempts and application crash logs.
Hackers and malicious actors access Event Viewer to increase their attack surface and enhance the target system's profiling.

Event categories are as below:
 Application: Records events of already installed programs.
 System: Records events of system components.
 Security: Logs events related to security and authentication etc.
 
Telemetry is a data collection system used by Microsoft to enhance the user experience by preemptively identifying security and functional issues in software. 
An application seamlessly shares data (crash logs, application-specific) with Microsoft to improve the user experience for future releases.  
Telemetry functionality is achieved by Universal Telemetry Client (UTC) services available in Windows and runs through diagtrack.dll



Per best practice, the Admin account should only be used to carry out tasks like software installation and accessing the registry editor, service panel, etc. 
Routine functions like access to regular applications, including Microsoft Office, browser, etc., can be allowed to standard accounts.

User Account Control (UAC) is a feature that enforces enhanced access control and ensures that all services and applications execute in non-administrator accounts.
It helps mitigate malware's impact and minimises privilege escalation by bypassing UAC.

Principle of Least Privilege, which states that (Per CISA) “a subject should be given only those privileges needed for it to complete its task. If a subject does not need an access right, the subject should not have that right”.

Windows Defender Firewall is a built-in application that protects computers from malicious attacks and blocks unauthorised traffic through inbound and outbound rules or filters.

Network devices like routers, ethernet cards, WiFI adapters etc., enable data sharing between computers.
If the device is improperly configured or not being used by the owner, it is recommended to disable the interface so that threat actors cannot access them and use them for data retrieval from the victim's computer.

SMB is a file-sharing protocol exploited by hackers in the wild. 
The protocol is primarily used for file sharing in a network; therefore, you must disable the protocol if your computer is not part of a network

The domain name system (DNS) is a naming system that translates Fully Qualified Domain Names (FQDN) into IP addresses.
If the attacker places himself in the middle, he may intercept and manipulate DNS requests and point them to attacker-controlled systems since DNS replies are neither authenticated nor encrypted.
Malicious actors try to edit the file's content to reroute traffic to their command and control server.


The address resolution protocol resolves MAC addresses from given IP addresses saved in the workstations ARP cache
The ARP offers no authentication and accepts responses from any user in the network.

downloading applications from the Microsoft Store ensures that the downloaded software is not malicious. 

Windows Defender primarily offers four main functionalities:
 Real-time protection - Enables periodic scanning of the computer.
 Browser integration - Enables safe browsing by scanning all downloaded files, etc.
 Application Guard - Allows complete web session sandboxing to block malicious websites or sessions to make changes in the computer.
 Controlled Folder Access - Protect memory areas and folders from unwanted applications.
 
AppLocker is a recently introduced feature that allows users to block specific executables, scripts, and installers from execution through a set of rules. 

Encryption of the computer is one of the most vital things to which we usually pay little attention.
The worst nightmare is that someone gets unfettered access to your devices' data.

To run applications safely, we can use a temporary, isolated, lightweight desktop environment called Windows Sandbox. 
We can install software inside this safe environment, and this software will not be a part of our host machine, it will remain sandboxed.
Once the Windows Sandbox is closed, everything, including files, software, and states will be deleted.
We would require Virtualisation enabled on our OS to run this feature. 

﻿Secure boot – an advanced security standard - checks that your system is running on trusted hardware and firmware before booting
which ensures that your system boots up safely while preventing unauthorised software access from taking control of your PC, like malware.
You are already in a secure boot environment if you run a modern PC with Unified Extensible Firmware Interface UEFI (the best replacement for BIOS) or Windows 10

Creating file backups is the best option to avoid disasters like malware attacks or hardware failure.

The most critical part of hardening computers is enabling the Windows auto-updates.  





The domain acts as a core unit regarding the logical structure of the Active Directory. 
It initially stores all the critical information about the objects that belong to the domain only.

A Domain Controller is an Active Directory server that acts as the brain for a Windows server domain; it supervises the entire network. 
Within the domain, it acts as a gatekeeper for users' authentication and IT resources authorisation.

Trees and Forests are the two most critical concepts of the Active Directory.
Trees are responsible for sharing resources between the domains.

When the sharing of the standard global catalogue, directory schema, logical structure, and directory configuration between the collections of trees is made successfully, it is called a Forest.

AD trust is the established communication bridge between the domains in Active Directory. When we say one domain trusts another in the AD network, it means its resources can be shared with another domain

AD trusts categorised based on characteristics are known as Transitive and non-Transitive trusts. 
Transitive trust reflects a two-way relationship between domains. 

The user account password for Windows isn't stored in clear text; instead, it stores passwords with two types of hash representation.
When the password for any user account is changed or set with fewer than 15 characters, both LM hash (LAN Manager hash) and NT hash (Windows NT hash) are generated by Windows and can be stored in AD.
The LM hash is relatively weaker than the NT and is prone to a fast brute-force attack. 
The best recommendation is to prevent Windows from storing the password's LM hash.

SMB stands for Server Message Block.
Generally, Microsoft-based networks utilise this protocol for file and print communication.
Moreover, it allows secure transmission over the network.
Configuring SMB signing through group policy is crucial to detect Man in the Middle (MiTM) attacks that may result in modification of SMB traffic in transit.
 SMB signing ensures the integrity of data for both client and server. All supported Windows versions have an SMB packet signing option.
 
Light Weight Directory Access Protocol (LDAP) enables locating and authenticating resources on the network.
Hackers may introduce replay or MiTM attacks to launch custom LDAP requests. 
Therefore, LDAP signing is a Simple Authentication and Security Layer (SASL) property that only accepts signed LDAP requests and ignores other requests (plain-text or non-SSL)

Active Directory password security is critical to address because of security breaches and password reuse. 

Attackers use various corporate password-compromise techniques, including brute force, dictionary, password spraying, credential attacks etc.
All organisations must have a strict password policy to defend against all such attacks. 
Password policies mean different rules for creating passwords, including length, complexity, and changing frequency. 

Implementing the least privilege model requires limiting the user or application access to minimise security risks and attack surfaces.

Implementing the least privilege model requires setting up the different account types for diverse purposes. It includes the following account types:
  User accounts: You must promote using regular user accounts for most people in the network, who are necessary to perform their regular duties.
  Privilege accounts: These are the accounts with elevated privileges and are further classified as first and second privilege accounts. 
  Shared accounts: These accounts are shared amongst a group of people, as the visitors with bare minimum privileges, to give limited access for a specific time. 
  
The Active Directory Tiered Access Model (TAM) comprises plenty of technical controls that reduce the privilege escalation risks.
It consists of a logical structure that separates Active Directory's assets by creating boundaries for security purposes. 

Tier 0: Top level and includes all the admin accounts, Domain Controller, and groups.
Tier 1: Domain member applications and servers. 
Tier 2: End-user devices like HR and sales staff (non-IT personnel).

Accounts audit is a crucial task mainly carried out by setting up the correct account, assigning privileges, and applying restrictions.
Three audit types related to accounts must be done periodically: usage, privilege, and change audits. 
 Usage audits allow monitoring each account's specific tasks and validating their access rights. 
 A privilege audit allows you to check if every account in the system has the least privilege.
 Change audits allow you to look for any improper changes to account permissions, passwords, or settings. Any unacceptable change to these may lead to a data breach.
 
Microsoft Security Compliance Toolkit (MSCT) is an official toolkit provided by Microsoft to implement and manage local and domain-level policies.


Zero Logon (Get admin access to an AD without credentials).
Breaching AD (Getting the first set of credentials in an AD environment).
Exploiting AD (Learn common AD exploitation techniques).
Post-Exploitation basics (What an attacker does after gaining an initial foothold of AD).

Kerberoasting is a common and successful post-exploitation technique for attackers to get privileged access to AD.
The attacker exploits Kerberos Ticket Granting Service (TGS) to request an encrypted password, and then the attacker cracks it offline through various brute force techniques.
These attacks are difficult to detect as the request is made through an approved user, and no unusual traffic pattern is generated during this process.
You can prevent the attack by ensuring an additional layer of authentication through MFA or by frequent and periodic Kerberos Key Distribution Centre (KDC) service account password reset.

During AD configuration, some share folders are publicly accessible or left unauthenticated, providing an initial foothold for attackers for lateral movement.












Endpoint devices refer to any device that can generate or consume data on a network, such as Laptops, Desktops, Smartphones, Tablets, Printers, Servers, and IoT Devices.
They are typically located at the edge of a network and interact directly with users.






Hardening techniques are meant to reduce the attack surface of a system or network by removing unnecessary functionality, limiting access, and implementing various security controls.

Some standard methods are mentioned below:
 Updating & Patching
 Disabling unnecessary services & ports
 Principle of Least Privilege (POLP)
 Logs Monitoring
 Backup regularly
 Enforcing Strong Passwords
 Multi-Factor Authentication (MFA)


Secure protocols play a critical role in network device hardening by protecting against unauthorised access and data breaches.
They ensure that sensitive data transmitted between devices is encrypted and cannot be intercepted by malicious actors.

Logging in network devices is essential for detecting and investigating security incidents, identifying performance issues, and complying with regulatory requirements.
It provides a record of events and activities on the device, which can be used for troubleshooting, forensic analysis, and auditing purposes.

The following techniques are generally used for logging:
 Syslog: A protocol to standardise the transfer of log messages, with the purpose of storing and analysing log messages to a central server.
 SNMP: Traps a notification sent by a network device to a management system when a predefined event occurs.
 NetFlow: A protocol used to collect and analyse network traffic data for monitoring and security analysis.
 Packet Captures: Capturing network traffic and storing it for analysis using a tool like Wireshark.
 
Virtual private networks (VPNs) are now needed in the age of remote work and online communication to protect sensitive data and preserve privacy. 
Hardening VPNs entails adopting additional security measures, such as multi-factor authentication and encryption techniques, to make it more challenging for hackers to access the network. 
the following are some of the significant hardening practices for a VPN server:
 Use strong encryption algorithm
 Keep VPN gateway software up-to-date
 Implement strong authentication
 Change default settings
 Enable Perfect Forward Secrecy (PFS)
 
 
Routers and switches must be hardened for the network infrastructure to be secure and reliable.
Every network needs routers and switches, often the first line of defence against potential security risks and attacks.
By hardening these devices, we can lower the possibility of unauthorised access, avoid data breaches, and ensure network service availability.
 Recommended Hardening Techniques
  Setting up the device
  Change default credentials
  Enable secure network protocols
  Disabling unnecessary scripts
  Securing Wi-Fi
  Manage traffic rules
  Monitor traffic
  Configuring port forwarding
  Monitoring scheduled tasks
  Update firmware
   Configuring port security
   Preventing ARP spoofing
   Preventing rogue DHCP servers
   Enabling IPv6
   
Network monitoring tools are enablers for maintaining the security and performance of networks.
These tools use sophisticated algorithms and protocols to capture and analyse real-time network traffic.
In addition, they enable network administrators to detect and troubleshoot problems such as bandwidth bottlenecks, network outages, and security threats.












A network protocol specifies how two devices, or more precisely processes, communicate with each other.

A network protocol is a pre-defined set of rules and processes to determine how data is transmitted between devices, such as end-user devices, networking devices, and servers.
The fundamental objective of all protocols is to allow machines to connect and communicate seamlessly, regardless of any difference in their internal design, structure, logic, or operation.
In analogy, a networking protocol is like a “common language” that helps make communication possible among people with different native languages and from various parts of the globe.

Each protocol represents specific layers of the OSI or TCP/IP model and operates as per the functionality of that layer.
TCP and UDP-based protocols operate on specific network ports.

SSL Certificate Checker: https://www.sslshopper.com/ssl-checker.html

Verification E-mail: https://mxtoolbox.com/SuperTool.aspx













﻿Cloud computing is one of the IT industry's most common and evolving terms. In simple terms, it means delivering computing services over the internet. 
The customer does not need to buy and maintain physical data centres and servers in cloud computing.
Instead, all services can be used with pay-as-you-go pricing 

It is becoming popular due to the following characteristics:
 Scalability
 Simplicity
 Cost Effective
 Enhance Automation
 
The following three cloud computing models are based on what the cloud provider offers and the needs of customers/organisations.
 Infrastructure as a Service (IaaS) - The customer has complete control of operating systems, services and applications.
 Platform as a Service (PaaS) - It contains all services offered in IaaS with the addition of an operating system (the user manages that in IaaS). 
 Software as a Service (SaaS) - It includes every service that is being provided in IaaS and PaaS.
  


Cloud Deployment Models
 Public Cloud - resources provided by cloud providers are shared among multiple customers.
 Private Cloud- customers will not share the underlying resources (hardware and software) as in the public cloud, and resources are dedicated to a single customer. 
 Hybrid Cloud - It is a combination of a public and private cloud.
 
Important Terminologies
 Virtualisation
 Compute
 Storage 
 Networking
 





Data is an asset and can be anything and any piece of information that any customer or organisation has.
 There are three main classes of data depending on their sensitivity:
  Confidential Data - Most Critical Data any organisation can have
  Internal Data - Internal data, if exposed, causes moderate risk or harm to the company
  Public Data - Any information intended for the public 
  
Cloud Data Lifecycle 





Security Aspects in Cloud Data Lifecycle
Create Update
 Implementing SSL/TLS
 Encryption
 Secure Connections
 
Store
 Encryption
 Backup
 
Use 
 Secure Connections
 Secure Platform
 Restrict Permissions
 Secure Virtualisation 
 
Share 
 Jurisdiction
 Data Loss Prevention (DLP)

Archive 
 Encryption 
 Physical Security 
 Location
 Backup Procedure 

Destroy 



Security Issues in the Cloud & Its Solution
 Data Confidentiality
 Virtualisation Issues
 Insecure Interfaces and API
 Malicious Insiders
 Account or Service Hijacking
 Access Control Mechanism (ACM) 


Riscos de Segurança a respeito de modelos de deploy de nuvem





Access Management is implemented through the following measures:
 Create Identities
 Authentication Factors
 Roles
 
Identity & Access Management 
 Give rights & permissions of resources in your amazon account to other people without sharing passwords, etc.
 Grant role-based access to users based on their access rights.
 Enable multi-factor authentication.
 Enable and manage permissions and access policies across amazon accounts & resources.
 

In a typical cloud environment, there are the following types of policies:
 Identity-based Policies: Attached to identities and grant permissions.
 Resource-based Policies: These are implemented on resources (data & services) and define who is authorised to access that resource.
 Session-based Policies: These temporary policies allow access to specific resources for a particular time.
 
Generally, network security of cloud infrastructure is maintained by following a layered approach:
 Layer 1 – Network Security through Security Groups
 Layer 2 – Network Security through Network Access Control Lists (NACLs)
 Layer 3 - Vendor Specific Security Solutions
 
Storage security in a cloud environment aims to ensure that data must remain safe while at rest and in transit during the various phases of the data lifecycle. 
The following approaches provide cloud storage protection:
 Create Geographical Boundaries: Define geographical regions and set policies permitting data access.
 Set Role-based Authorisation: Create identities and assign roles to access a particular data set per the rights and privileges.
 Data Encryption: Almost all cloud service providers allow data encryption at rest. With this approach, server-side encryption is applied to data.
 
Security through Monitoring & Logging
 Real-time Logging
 Monitoring & Logging of API Calls
 Credential Reports
 
 
 
Security through access management: Ensure that the right people should perform the right job within the right set of permissions.
Security through policies: Set conditions and guidelines under which users & resources can perform specific actions.
Security through networking: Ensure that cloud instances remain safe from network-oriented attacks.
Security through storage management: Ensure the security of sensitive data stored in cloud storage through various means, including encryption and geographical settings, etc.








In simple terms, auditing is like a check-up for a company or organisation.
 It involves carefully examining the company’s processes, internal controls, and financial statements to ensure everything runs smoothly according to the policies and laws.


In more formal terms, auditing is a systematic, independent, and objective process of gathering and evaluating evidence to determine if an organisation, its policies, processes, controls, or financial statements comply with applicable laws, regulations, and industry standards.

In information systems, monitoring is about continually checking a computer’s or network’s performance and behaviour.
It involves watching over various components such as applications, storage, and networking to make sure they’re working well together.

Auditing of information systems involves the systematic, independent, and objective examination of an organisation’s IT infrastructure, processes, and controls.

Some primary objectives of an information systems audit include the following:
 Assess the effectiveness of internal controls
 Identify and assess risks
 Assess the efficiency and effectiveness of information systems
 Ensure compliance with laws and regulations
 
information systems audit serves the following objectives:
 Risk assessment
 Regulatory compliance
 IT governance
 Security management
 Operational and performance evaluation
 Data management and quality
 Business continuity and disaster recovery
 Fraud detection and prevention
 
Internal audits: These are performed by an organisation’s personnel or staff members assigned to the internal audit function.
External audits: External audits are conducted by independent auditors not employed by the organisation being audited. 
Third-party audits: This type of audit is conducted when an organisation needs to assess its IT systems or controls within third parties, such as vendors, service providers, or subcontractors. 

An audit framework is a structured approach comprising principles, concepts, and practices used to conduct an audit. 
It provides guidelines on planning, executing, and reporting on an audit effectively and ensures that audits are objective and consistent.
Audit frameworks help auditors assess an organisation’s policies, processes, controls, and compliance with regulations while providing efficiency, value, and transparency to the audit process.


COSO: The Committee of Sponsoring Organizations of the Treadway Commission (COSO) is a private-sector initiative that develops frameworks for enterprise risk management, internal control, and fraud deterrence.
COBIT: The Control Objectives for Information and Related Technology (COBIT) is a framework for the governance and management of information and technology (IT).
ISAE 3402: ISAE3402 is an international standard that provides guidance on the assurance of controls over financial reporting.
ISO 27001: ISO 27001 is an international standard for information security management. It provides a set of best practices for information security management.
ITIL: ITIL stands for Information Technology Infrastructure Library. It is a framework of best practices for IT Service Management (ITSM). 
PCI DSS: The Payment Card Industry Data Security Standard (PCI DSS) is a set of security requirements for organisations that accept payment cards. 
SOX: The Sarbanes-Oxley Act of 2002 (SOX) is a federal law that establishes auditing and financial reporting requirements for public companies.





Audit Process
 Primary Stages 
  Planning
  Information Gathering
  Risk Assessment and Control Evaluation
  Testing
  Analysis and Findings
  Reporting
  Follow-Up
  
  
Audit Areas
 The following is a list of some areas that we might consider inspecting when performing an information systems audit:
  Information Systems Hardware
  OS
  File Systems
  Database Management Systems
  Network Infrastructure
  Network Operating Controls
  IT Operational
  Lights Out Operations
  Problem Management Operations
  Monitoring Operations
  Procurement
  Busness Continuity Planning
  Disaster Recovery Planning
  
 Audit Scenario 
  Planning
   Define the scope of the audit
   Identify the relevant COBIT controls
   Develop an audit Plan 
   
  Execution
   Gather Evidence
   Assess the Evidence
   
  Assessment 
   Identify Gaps in Compliance
   Make Recommendations for Improvement 
   
  Reporting
   Prepare the audit report
   Communicate the audit report
   
  Follow Up
   Monitor the implementation fo recommendations
   
 
   
Logging is the process of recording events as they take place on a computer system.
 Purposes of the Logging
  Troubleshooting
  Monitoring
  Auditing
  Compliance
  
Most Linux distributions store the log files and directories in /var/log.
For handling logs, many Linux distributions use system logging daemons like rsyslog, syslog-ng, and journald to manage, process, and store log events.

To efficiently work with Linux logs, we need to consider the following:
 Log to a central location
 Use a tool to filter and parse the logs
 Setup alerts


Microsoft Windows makes it possible to audit various aspects of the system. 
 Here is a list of example events you can audit:
  Account logon events
  Account management
  Privilege use
  Directory service access
  Policy change
  System events
  



The key capabilities of SIEM technology include:
 Data Aggregation
 Correlation and analysis
 Alerting and reporting
 Forensic Analysis
 Threat intelligence Feeds
 Automation and Orchestration







Open Worldwide Application Security Project (OWASP)
a non-profit and collaborative online community that aims to improve application security via a set of security principles, articles, documentation etc.


API stands for Application Programming Interface
It is a middleware that facilitates the communication of two software components utilising a set of protocols and definitions
  In the API context, the term 'application' refers to any software having specific functionality
  'interface' refers to the service contract between two apps that make communication possible via requests and responses.
   The API documentation contains all the information on how developers have structured those responses and requests.
    API is a building block for developing complex and enterprise-level applications.
	
Generally, API endpoints are utilised for a common practice of retrieving and manipulating data through object identifiers.
 BOLA refers to Insecure Direct Object Reference (IDOR)
 which creates a scenario where the user uses the input functionality and gets access to the resources they are not authorised to access.
 In an API, such controls are usually implemented through programming in Models (Model-View-Controller Architecture) at the code level.
 
The absence of controls to prevent unauthorised object access can lead to data leakage and, in some cases, complete account takeover. 
User's or subscribers' data in the database plays a critical role in an organisation's brand reputation; if such data is leaked over the internet, that may result in substantial financial loss.

Mitigation Measures
 An authorisation mechanism that relies on user policies and hierarchies should be adequately implemented. 
 Strict access controls methods to check if the logged-in user is authorised to perform specific actions. 
 Promote using completely random values (strong encryption and decryption mechanism) for nearly impossible-to-predict tokens.


User authentication is the core aspect of developing any application containing sensitive data.
 Broken User Authentication (BUA) reflects a scenario where an API endpoint allows an attacker to access a database or acquire a higher privilege than the existing one. 
  The primary reason behind BUA is either invalid implementation of authentication like using incorrect email/password queries etc., or the absence of security mechanisms like authorisation headers, tokens etc.
  
In broken user authentication, attackers can compromise the authenticated session or the authentication mechanism and easily access sensitive data.
 Malicious actors can pretend to be someone authorised and can conduct an undesired activity, including a complete account takeover. 


Mitigation Measures 
 Ensure complex passwords with higher entropy for end users.
 Do not expose sensitive credentials in GET or POST requests.
 Enable strong JSON Web Tokens (JWT), authorisation headers etc.
 Ensure the implementation of multifactor authentication (where possible), account lockout, or a captcha system to mitigate brute force against particular users. 
 Ensure that passwords are not saved in plain text in the database to avoid further account takeover by the attacker. 
 
 
Excessive data exposure occurs when applications tend to disclose more than desired information to the user through an API response
 A malicious actor can successfully sniff the traffic and easily access confidential data, including personal details, such as account numbers, phone numbers, access tokens and much more. 
 
Mitigation Measures 
 Never leave sensitive data filtration tasks to the front-end developer. 
 Ensure time-to-time review of the response from the API to guarantee it returns only legitimate data and checks if it poses any security issue. 
 Avoid using generic methods such as to_string() and to_json(). 
 Use API endpoint testing through various test cases and verify through automated and manual tests if the API leaks additional data.
 


Lack of resources and rate limiting means that APIs do not enforce any restriction on the frequency of clients' requested resources or the files' size
 which badly affects the API server performance and leads to the DoS (Denial of Service) or non-availability of service. 
 
Nowadays, attackers are using such attacks to ensure the non-availability of service for an organisation
 thus tarnishing the brand reputation through increased downtime.
 
Mitigation Measures 
 Ensure using a captcha to avoid requests from automated scripts and bots.
 Ensure implementation of a limit, i.e., how often a client can call an API within a specified time and notify instantly when the limit is exceeded. 
 Ensure to define the maximum data size on all parameters and payloads, i.e., max string length and max number of array elements. 
 
 
 
Broken Function Level Authorisation reflects a scenario where a low privileged user (e.g., sales) bypasses system checks and gets access to confidential data by impersonating a high privileged user (Admin).
 Broken Function Level Authorisation reflects IDOR permission, where a user, most probably an intruder, can perform administrative-level tasks. 
 
Mitigation Measures 
 Ensure proper design and testing of all authorisation systems and deny all access by default. 
 Ensure that the operations are only allowed to the users belonging to the authorised group. 
 Make sure to review API endpoints against flaws regarding functional level authorisation and keep in mind the apps and group hierarchy's business logic.  


















Mass assignment reflects a scenario where client-side data is automatically bound with server-side objects or class variables.
hackers exploit the feature by first understanding the application's business logic and sending specially crafted data to the server, acquiring administrative access or inserting tampered data.
This functionality is widely exploited in the latest frameworks like Laravel, Code Ignitor etc.
The attack may result in data tampering and privilege escalation from a regular user to an administrator. 


Mitigation Measures 
 Before using any framework, one must study how the backend insertions and updates are carried out. In the Laravel framework, fillable and guarded arrays mitigate the above-mentioned scenarios. 
 Avoid using functions that bind an input from a client to code variables automatically.
 Allowlist those properties only that need to get updated from the client side. 
 
 
 
Security misconfiguration depicts an implementation of incorrect and poorly configured security controls that put the security of the whole API at stake.
Several factors can result in security misconfiguration, including improper/incomplete default configuration, publically accessible cloud storage, Cross-Origin Resource Sharing (CORS), and error messages displayed with sensitive data. 
  Intruders can take advantage of these misconfigurations to perform detailed reconnaissance and get unauthorised access to the system. 
  
Security misconfiguration can give intruders complete knowledge of API components. 
Firstly, it allows intruders to bypass security mechanisms. 

Mitigation Measures 
 Limit access to the administrative interfaces for authorised users and disable them for other users. 
 Disable default usernames and passwords for public-facing devices (routers, Web Application Firewall etc.).
 Disable directory listing and set proper permissions for every file and folder. 
 Remove unnecessary pieces of code snippets, error logs etc. and turn off debugging while the code is in production.
 
 
Injection attacks are probably among the oldest API/web-based attacks and are still being carried out by hackers on real-world applications. 
Injection flaws occur when user input is not filtered and is directly processed by an API: enabling the attacker to perform unintended API actions without authorisation.
An injection may come from Structure Query Language (SQL), operating system (OS) commands, Extensible Markup Language (XML) etc.
Nowadays, frameworks offer functionality to protect against this attack through automatic sanitisation of data; however, applications built in custom frameworks like core PHP are still susceptible to such attacks. 
Injection flaws may lead to information disclosure, data loss, DoS, and complete account takeover. 
The successful injection attacks may also cause the intruders to access the sensitive data or even create new functionality and perform remote code execution. 

Mitigation Measures
 Ensure to use a well-known library for client-side input validation.
 If a framework is not used, all client-provided data must be validated first and then filtered and sanitised. 
 Add necessary security rules to the Web Application Firewall (WAF). Most of the time, injection flaws can be mitigated at the network level.
 Make use of built-in filters in frameworks like Laravel, Code Ignitor etc., to validate and filter data. 
 
 
 
Inappropriate Asset Management refers to a scenario where we have two versions of an API available in our system
Everything is wholly switched to APIv2, but the previous version, APIv1, has not been deleted yet.
Plenty of other obsolete features of APIv1 make it possible to find vulnerable scenarios, which may lead to data leakage and server takeover via a shared database amongst API versions.

Mitigation Measures 
 Access to previously developed sensitive and deprecated API calls must be blocked at the network level.
 APIs developed for R&D, QA, production etc., must be segregated and hosted on separate servers.
 Ensure documentation of all API aspects, including authentication, redirects, errors, CORS policy, and rate limiting. 
 Adopt open standards to generate documentation automatically.
 
 
 
Insufficient logging & monitoring reflects a scenario when an attacker conducts malicious activity on your server; 
however, when you try to track the hacker, there is not enough evidence available due to the absence of logging and monitoring mechanisms
Several organisations only focus on infrastructure logging like network events or server logging but lack API logging and monitoring. 
Information like the visitor's IP address, endpoints accessed, input data etc., along with a timestamp, enables the identification of threat attack patterns.
If logging mechanisms are not in place, it would be challenging to identify the attacker and their details.


Mitigation Measures 
 Ensure use of the Security Information and Event Management (SIEM) system for log management. 
 Keep track of all denied accesses, failed authentication attempts, and input validation errors, using a format imported by SIEM and enough detail to identify the intruder.
 Handle logs as sensitive data and ensure their integrity at rest and transit. Moreover, implement custom alerts to detect suspicious activities as well. 












Secure Software Development Lifecycle (S-SDLC)

Secure SDLC models aim to introduce security at every stage of the SDLC.
Apart from faster development and reduction of costs, integrating security across the SDLC helps discover and reduce vulnerabilities early, reducing business risk massively. 
Examples of introducing security at all stages are architecture analysis during design, code review and scanners during the development stage and conducting security assessments (e.g. penetration tests) before deployment.

﻿Summary
 Security is a constant concern, improving software quality and security constantly.
 Boosting security education and awareness: all stakeholders know each phase's security recommendations and requirements.
 Flaws are detected early before deployment, reducing the risk of getting hacked or disrupted.
 Costs are reduced, and speed increases, thanks to the early detection and resolution of vulnerabilities. Business risk, brand reputation damage, and fines that could lead to economic disaster for a company are prevented.
 
 
Secure SDLC involves instilling security processes at all lifecycle phases.
From security testing tools to writing security requirements alongside functional requirements.
Like with every new process, understanding your gaps and state is critical for successfully introducing a new tool, solution, or change.
 Perform a gap analysis to determine what activities and policies exist in your organisation and how effective they are.
 Create Software Security Initiatives (SSI) by establishing realistic and achievable goals with defined metrics for success. 
 When performing a gap analysis, every policy should have defined procedures to make them effective.
 Invest in security training for engineers as well as appropriate tools.
 


Risk Assessment - during the early stages of SDLC, it is essential to identify security considerations that promote a security by design approach when functional requirements are gathered in the planning and requirements stages
Threat Modelling - is the process of identifying potential threats when there is a lack of appropriate safeguards.
Code Scanning / Review -  Code reviews can be either manual or automated. 
Security Assessments - Like Penetration Testing & Vulnerability Assessments are a form of automated testing that can identify critical paths of an application that may lead to exploitation of a vulnerability. 


Risk refers to the likelihood of a threat being exploited, negatively impacting a resource or the target it affects.
For example, vulnerabilities being exploited after a new version of the software is published, design flaws, and poorly reviewed code can increase the risk of these scenarios.
Risk management is an important pillar to integrate into the SDLC to mitigate risk in a product or service

The first step in the risk assessment process is to assume the software will be attacked and consider the factors that motivate the threat actor. 
The next step is risk evaluation. Include factors like the worst-case scenario if the attacker has successfully attacked the software. 
The last factor is the accessibility of the target. Determine whether the target accepts requests across a network or only local access, whether authentication is needed for establishing a connection, or if anyone can send requests. 

There are several types of Risk assessments best suited for different scenarios. Below are the different types of Risk Assessments:
In a Qualitative Risk Assessment, the goal is to assess and classify risk into thresholds like "Low", "Medium", and "High". 
It systematically examines what can cause harm and what decisions should be made to define or improve adequate control measures.

The Quantitative Risk Assessment is used to measure risk with numerical values. Instead of "Low", "Medium", and "High", you would have numbers that represent those bands. 


Threat modelling is best integrated into the design phase of an SDLC before any code is written. 
Threat modelling is a structured process of identifying potential security threats and prioritising techniques to mitigate attacks so that data or assets can be protected

There are various methods to perform threat modelling. 
STRIDE, DREAD, and PASTA are among the common threat modelling methodologies.

STRIDE stands for Spoofing, Tampering, Repudiation, Information Disclosure, Denial Of Service, and Elevation/Escalation of Privilege.
STRIDE is built upon the CIA triad principle (Confidentiality, Integrity & Availability).
Security professionals that perform STRIDE are looking to answer "What could go wrong with this system"

The abbreviation DREAD stands for five questions about each potential: Damage Potential, Reproducibility, Exploitability, Affected Users and Discoverability.
It's a model that ranks threats by assigning identified threats according to their severity and priority.
STRIDE, the DREAD model also can be used in assessing, analysing and finding the risk probability by threat rating. 

PASTA is short for Process for Attack Simulation and Threat Analysis
it is a risk-centric threat modelling framework. PASTA's focus is to align technical requirements with business objectives. 
PASTA involves the threat modelling process from analysing threats to finding ways to mitigate them, but on a more strategic level and from an attacker's perspective.
PASTA is divided into seven stages:
 Define Objectives
 Define Technical Scope
 Decomposition & Analysis
 Threat Analysis
 Vulnerabilities & Weaknesses Analysis
 Attack / Exploit Enumeration & Modelling
 Risk Impact Analysis
 
 
 
 
 
Implementing a secure code review in the phases of an SDLC, especially during the implementation phase, will increase the resilience and security of the product without bearing any additional cost for future patches. 
Secure code review is defined as a measure where the code itself is verified and validated to ensure vulnerabilities that are found can be mitigated and removed to avoid vulnerabilities and flaws.

Code review can be done manually or automated.
A manual code review is where an expert analyses and checks the source code by going line by line to identify vulnerabilities.

Static analysis examines the source code without executing the program - detects bugs at the implementation level
Dynamic analysis looks at the source code when the program is running - dynamic analysis detects errors during program runtime. 

SAST means Static Application Security Testing, a white box testing method that directly analyses the source code.
Many people tend to develop an application that could automate or execute processes quickly and improve performance and user experience
To summarise, SAST is used to scan source code for security vulnerabilities. 

SCA is used to scan dependencies for security vulnerabilities, helping development teams track and analyse any open-source component brought into a project

DAST means Dynamic Application Security Testing, a black-box testing method that finds vulnerabilities at runtime
DAST is a tool to scan any web application to find security vulnerabilities. 
This tool is used to detect vulnerabilities inside a web application that has been deployed to production.

IAST means Interactive Application Security Testing that analyses code for security vulnerabilities while the app is running.
It is usually deployed side by side with the main application on the application server.
IAST is an application security tool designed for web and mobile applications to detect and report issues even while running.

"RASP" stands for Runtime Application Self Protection. RASP is a runtime application integrated into an application to analyse inward and outward traffic and end-user behavioural patterns to prevent security attacks.
s RASP is used after product release, making it a more security-focused tool when compared to the others that are known for testing.



A security assessment plays a primary role in achieving security in SDLC and should be implemented in all phases where possible. 
There are two types of assessments: Penetration Testing and Vulnerability Assessment. 
 Vulnerability Assessments focus on Finding Vulnerabilities, but do not validate them or simulate the findings to prove they are exploitable in reality.
 Typically, automated tools run against an organisation's network and systems. Examples of tools: are OpenVAS, Nessus (Tenable), and ISS Scanner. 
 
It Includes Vulnerability Testing but goes more in-depth. It is extended by testing/validating of vulnerabilities, quantifying risks and attempting to penetrate systems. 







There are several methodologies for SDLC; some examples of widely used methodologies are:
 Microsoft's Security Development Lifecycle (SDL)
 OWASP Secure Software Development Life Cycle Project (S-SDLC)
 Software Security Touchpoints
 
The Software Assurance Maturity Model (SAMM) is an open framework to help organisations formulate and implement a software security strategy tailored to the organisation's specific risks
Another critical security model is the Building Security In Maturity Model (BSIMM). BSIMM is a study of real-world software security initiatives and reflects the current state of software security. 













Static Application Security Testing (SAST)
one of the many tools used to detect vulnerabilities during the development lifecycle of an application.

One of the methods used for developing secure applications is frequently testing the code for security bugs through a process known as code review.
Code reviews consist of looking at the source code of an application to search for possible vulnerabilities using a white box approach. 
By having access to the code, the reviewer can guarantee a greater coverage of the application's functionalities and lower the time required to find bugs.

Code reviews are often done through a combination of manual analysis and automated tools for the best results. 
On the one hand, manual code reviews have the advantage of a human evaluating the code, which allows for a thorough analysis and more precise results.
However, since an application often has thousands and thousands of lines of code, the task can quickly become overwhelming for the reviewer, leading to some vulnerabilities being missed because of fatigue.

On the other hand, automated tools excel at finding common vulnerabilities almost instantly, saving loads of time to the reviewer.
Automated tools will also perform consistently, no matter the size of the code base, so they won't miss vulnerabilities as a human could do, as long as they have predefined rules to match them.

The cost of a manual review will often be higher, as a reviewer must spend lots of time tracing vulnerabilities through the code.
Automated tools will perform their analysis almost instantly.

For these reasons, you will typically want to run automated tests early in the development lifecycle to take care of all the low-hanging fruits with lower costs and have manual reviews spaced periodically or when important project goals are met to take care of complex vulnerabilities that the automated tools may not be able to detect.

Local File Inclusion (LFI) attacks are made possible by the misuse of one of the following functions in PHP


Static Application Security Testing (SAST) refers to using automated tools for code analysis.
The idea is not to replace manual code reviews but to provide a simple method to automate simple code checks to quickly find vulnerabilities during the development process without requiring a specialised individual.

SAST complements other techniques, such as Dynamic Application Security Testing (DAST) or Software Composition Analysis (SCA)
to provide a holistic approach to application security during the development lifecycle. 





While every SAST tool is different, most of them will perform two main tasks:
 Transform the code into an abstract model: SAST tools usually ingest the source code and produce an abstract representation for further analysis. 
  Most SAST tools will represent the code using Abstract Syntax Trees (AST), but some tools may have other equivalent proprietary structures.
  This allows for easier code analysis in a way that is independent of the programming language in use. 
  
 
 Analyse the abstract model for security issues
  Different analysis techniques will be used to search for potential vulnerabilities in the code model.
  
Semantic analysis: This type of analysis can be compared to grepping for potentially insecure functions while doing manual code reviews.
Dataflow analysis: Dataflow analysis will trace how information flows from inputs the user can manipulate to potentially vulnerable functions
Control flow analysis: Analyses the order of operations in the code in search for race conditions, use of uninitialised variables or resource leaks. 
Structural analysis: Analyses specific code structures of each programming language. 
Configuration analysis: Searches for application configuration flaws rather than the code itself. 
 
It is important to note that not every SAST tool will implement all the analysis techniques above

False positives: The tool reports on a vulnerability that isn't present in the code.
False negatives: The tool doesn't report a vulnerability that is present in the code.

Depending on each case, SAST can be implemented in one of the following ways:
 CI/CD integration: Each time a pull request or a merge is made, SAST tools will check the code for vulnerabilities. 
 IDE integration: SAST tools can be integrated into the developers' favourite IDEs instead of waiting for a pull request or merge to occur.






Dynamic Application Security Testing (DAST) 
One of the many ways to test an application for vulnerabilities is to take a running instance and attack it just like an outsider attacker would. 

Dynamic Application Security Testing (DAST) is the process of testing a running instance of a web application for weaknesses and vulnerabilities. 
It focuses on a black-box testing approach where vulnerabilities are found just like a regular attacker would find them.
Simply put, DAST identifies vulnerabilities by trying to exploit them, either manually or through automated tools.

It is important to note that DAST doesn't replace other methods to find vulnerabilities in applications, but rather complements them. 
A secure development lifecycle will often mix several techniques in order to provide a good enough vulnerability coverage.

There are two ways in which DAST can be performed:
 Manual DAST: A security engineer will manually perform tests against an application to check for vulnerabilities.
 Automatic DAST: An automated tool will scan the web application for vulnerabilities.



 

In general terms, a DAST tool will perform at least the two following tasks against the target website:

Spidering/Crawling: The tool will navigate through the web app, trying to map the application and identify a list of pages and parameters that can be attacked.
Vulnerability Scanning: The tool will try to launch attack payloads against the identified pages and parameters. The user can typically customise the type of attacks to include only the ones relevant to the target application.









Weaponizing a vulnerability refers to the process of taking a known vulnerability in a software or system and creating an exploit for it 
which can then be used to gain unauthorized access or perform other malicious actions. 

The goal of weaponizing vulnerabilities is to take advantage of a single vulnerability or set of vulnerabilities that can be chained to get elevated access to a system.
Despite the safeguards already in place, every system or equipment within a firm may have vulnerabilities.

An exploit can take various forms, such as an executable file designed for a specific endpoint, which can be delivered through a text message, an email attachment, or even a file that is hidden within digital files.
When the exploit is executed, an attacker can perform various actions on the targeted system, such as controlling it remotely or locally, disrupting its functionality, stealing data, or any other activities that the system's resources allow.

An exploit can be either local or remote.
An attacker who already has access to a specific computing resource can execute code locally to escalate their privileges.
An attacker who exploits a vulnerability remotely over a network or communication channel to gain control of a system uses a specific type of exploit, known as a remote exploit.

An exploit is a technical tool that may be used against any user in a standalone or connected cyber environment.
In a standalone environment, an exploit may be delivered via removable media. 


The five stages of the Lifecycle of a Vulnerability Framework are 
 Discovery, 
 Coordination, 
 Mitigation, 
 Management, 
 Lessons Learned
 
Exploiting 0-day vulnerabilities can take a few days to several months or even years.



Adversaries with access to the updated software can reverse-engineer the patch to find the vulnerability.
When a CVE is formally released, the public can immediately access information about the vulnerability.

Chaining exploits in tandem, also known as "exploit chaining" or "multi-stage exploitation", 
is a technique hackers use to string together multiple exploits for vulnerabilities to gain complete control of a target system.
The desired goal is to develop a complete Remote Code Execution (RCE) chain, allowing the attacker to execute arbitrary code on the target system with the highest privileges.
 Exploit Chain:
  1. Reconnaissance
  2. Initial Exploit
  3. Privilege Escalation
  4. Persistence
  5. Lateral Movement 
  6. RCE 
  
organizations must keep their systems updated and patched and implement proper network segmentation and best security practices to minimize the risk of such attacks.

The first and most crucial step in exploiting a target system is finding and using a vulnerability that provides an initial entry point.
The easiest way to check for SQL injection is by using special characters. 



Scripts: One way to automate everyday tasks and security checks is to use scripts.
Scheduling Tools: Scheduling tools are another way to automate common tasks and security checks.
SOAR Platforms: A third way to automate everyday tasks and security checks is to use Security Orchestration, Automation and Response Platforms.

When automating tasks and security checks, it is important to consider the following best practices:
 Test your scripts and automation tools in a controlled environment before deploying them in production.
 Ensure that your scripts and automation tools are well-documented and easy to understand.
 Use logging and monitoring to keep track of the tasks that have been automated.
 Review your automation and security checks regularly to ensure they are still practical and relevant.
 Keep your scripts and automation tools updated and patched to address any security vulnerabilities.











Developer Operations (DevOps)
This is the name given to how project management was approached back in the day (the 70s). 
The cycle constituted and relied on a hierarchy, where every member had a specific responsibility. 


With the challenges teams were facing with waterfall, businesses started developing ways that allowed more flexibility and adaptability.
Somewhere in early 2000,  The Agile Methodology was coined. 
 Agile Manifesto, emphasising four values for agile development:
  Individuals and interactions over processes and tools
  Working software over comprehensive documentation
  Customer collaboration over contract negotiation
  Responding to change vs following a plan
  
Thanks to the advent of DevOps, today's development infrastructure is fully automated and operates on a self-service basis

Shifting Left.
This means that  DevOps teams focus on instilling security from the earliest stages in the development lifecycle and introducing a more collaborative culture between development and security.

DevSecOps is an approach that relies heavily on automation and platform design that integrates security as a shared responsibility.
It is a culture-driven development style that normalises security as a day-to-day operation.

DevSecOps helps bring down vulnerabilities, maximises test coverage, and intensifies the automation of security frameworks
This reduces risk massively, assisting organisations in preventing brand reputation damage, and economic losses due to security flaws incidents, making life easier for auditing and monitoring.










everything first starts at the SOC (Security Operations Centre)
Here, a team of analysts monitor the security of the organisation.
In essence, this team is monitoring events in the organisation's estate.
If an event is an anomaly or unexpected, an alert is generated.
if the alert is real, the team will perform a triage process to determine the severity. 
If the severity of the alert is sufficient, an incident will be raised.

EDR or AV Alert - Usually these tools would create an alert for anomalous activity that has occurred on a specific host.
Network Tap Alert - Network taps provide alerts for anomalous network activity. 
SIEM Alert - The Security Information and Event Management (SIEM) system could alert on a custom rule that was created by the analysts

sometimes the alert information is not sufficient and we have to gather more information than what is currently provided. This process is usually referred to as Digital Forensics.
Here, we perform a much more hands-on investigation that can include the following:
 Recovering the hard disk from the infected host to investigate how the malware got on there in the first place.
 Recovering the data from volatile memory (such as from the computer's RAM) from the infected host to investigate how the malware works.
 Recovering system and network logs from several devices to uncover how the malware spread.
 
The overall goal of Incident Response is to try and understand the scope of the incident. -> What happened?

Incident Response covers the technical aspect of dealing with an incident. This is the portion that is responsible for answering the primary question:


Incident Management covers the process aspect of dealing with an incident. -> How do we respond to what happened?
Incident Management has to take care of several things, such as:
 Triaging the incident to accurately update the severity of the incident as new information becomes available and getting more stakeholders involved to help deal with the incident, such as Subject Matter Experts (SMEs).
 Guiding the incident actions through the use of playbooks.
 Deciding which containment, eradication, and recovery actions will be taken to deal with the incident.
 Deciding the communication that will be sent internally and externally while the team deals with the incident.
 Documenting the information about the incident, such as the actions taken and the effect that they had on dealing with the incident.
 Closing the incident and taking the information to learn from the incident and improve future processes and procedures. 00


incident management process
 preparation
 Detection and Analysis
 Containment, Eradication, and Recovery
 Post-Incident Activity 
 
Preparation is key to effectively deal with an incident.
During an incident, it is often stressful and every minute counts to ensure that the incident can be dealt with as fast as possible to reduce the amount of damage. 
In these stressful environments, it is often easy to forget things, which then could have severe consequences.
In order to prepare, there are several things that the team can perform, such as:
 Identify and document key stakeholders and call trees that will be used during an incident
 Create and update playbooks that aid the team in following a set process for incidents with a known nature
 Exercise the team's ability to deal with an incident through tabletop exercises and cyber war games
 Continuously perform threat hunting to help create new alert rules based on modern attacker techniques
 
Often organisations will split the detection and analysis phases into two. 
This is to introduce a middle step called triaging.
As mentioned before, not all alerts will classify as an incident and even if an incident occur, there are different levels of incidents. 
The triage step is responsible for determining the severity of the incident. 
During this phase, the blue team works to better understand the scope of the incident and provide this information to the incident manager. This can include actions such as the following:
 Reviewing alerts in the AV, EDR, and SIEM dashboards
 Performing a forensic investigation of artefacts both on systems and the network
 Analysing malware that is discovered to better understand how it works and create new signatures that can be used to identify it
 

Once the scope of the incident is better understood, the team will start with containment, eradication, and recovery.
This is the primary phase of incident management, where we try to deal with the incident.
Often organisations will split this phase into three different ones, each to deal with the following:
 Containment - Actions taken to "stop the bleed". These are actions meant to stop the incident from growing larger.
 Eradication - Actions taken to eradicate the threat actor from the estate.
 Recovery - Actions taken to recover the environment allow the organisation to go back to Business as Usual (BAU).
 
Once an incident has been closed, that isn't the end of the incident management process.
As a last step, we want to evaluate what happened during the incident in order to learn lessons and improve how we deal with incidents in the future.




Insufficient Hardening is something that happens even before the incident.
Organisations often prioritise speed and profits over security. 
Once a solution is deployed, there may still be some configurations that did not adhere to security best practices but were performed to get the solution up and running faster.
The hardening process reverses these configurations to bring them back in line with security best practices.
In the event that this step is skipped, the likelihood of incidents is increased.
To make it easier, organisations have started to perform Hardening during the development rather than simply at the end. This is known as the Shift Left principle

In order for the blue team to be alerted to incidents, they first have to receive the relevant information that can result in events and alerts.
Often it is seen that organisations are not performing adequate logging of information. 
This can be seen as "flying blind" since the blue team would not be able to even know that an incident is occurring.


A big mistake that often happens during incident response and management is not understanding the incident scope. 
While it is often impossible to fully understand the incident scope, best efforts should be made. 
In cases where the incident scope is underestimated, the actions taken against the threat actor would not be sufficient to eradicate them from the system.



The last common pitfall during incidents is insufficient backups. 
In the event that an incident results in disruptive actions such as ransomware being deployed, the only saving grace is backups that can be used to recover the estate. 













Logging is used to provide a "source of truth" for activity that occurs on a network.
Logging is most commonly used, but not limited to incident response and security monitoring. 
During the incident response process, a user may be held accountable for an action or behavior, and logging plays a crucial role in proving a user's actions.

Accountability is the final pillar of the Identification, Authentication, Authorization, and Accountability (IAAA) model. 
The model is used to protect and maintain confidentiality, integrity, and availability of information.

Accountability holds users and peers on a network responsible for their actions. Logging is a large part of this pillar and maintains a record of activities.

To ensure the efficacy of accountability, logs and other data sources must be protected, and their authenticity must be proved. 
If it cannot be proven that a log was kept in its original state, it loses its integrity for accountability and the incident response process.

Logging aids any member involved in the incident response process.

Depending on the log source, it may provide different benefits or visibility into a network or device. Some examples may include:
 Files created.
 Emails sent.
 Other TTPs (Tactics, Techniques, and Procedures) as outlined by the MITRE ATT&CK framework.
 
Because logs play an important role in incident response, they must be authentic and, when analyzed, identical to when they were produced.

Adding accountability to the incident response process, when log sources are guaranteed to be authentic, a user can be held accountable for their actions, as proven by logs.
This use of accountability is more formally known as non-repudiation and contributes to many threat models, such as the STRIDE model.
Non-repudiation means that an individual cannot contest an action
the opposite of repudiation, where an individual disputes an action.

A Security Information and Event Management system (SIEM) is a tool used to collect, index, and search data from various endpoints and network locations.

Below is a summary of the benefits and features that an SIEM can offer at the most basic level:
 Real-time log ingestion.
 Alerting against abnormal activities.
 24/7 monitoring and visibility.
 Data insights and visualization.
 Ability to investigate past incidents.
 
Examples of SIEMs may include Wazuh, Splunk, ELK, and QRadar. 

SIEMs are typically architected with three components used for searching, indexing, and load-balancing; these components are commonly known as the
 search head, 
 indexer, 
 and forwarder
 
 how data arrives from a device to the indexer; this process is commonly known as data ingestion.
  Types of data ingestion
     Agent/forwarder
     Port-forwarding
     Syslog
     Upload
	 
	 
Cold storage is a process or standard for storing data, which can be summarized as storing a large quantity of data optimally.

Cold storage is rarely accessed and thus does not require high-performance storage devices. 
Examples of cold storage may include low-cost hard drives or even tape drives


hot storage is data accessed often and requires higher performance, which may consist of solid-state drives and, in some cases, high-performance hard drives.

There may be other levels of access and performance throughout the life cycle of data that can be referred to as warm storage. 

The standard for how long data stays in each phase will depend on regulatory requirements and company guidelines.
An example of a storage process may be that data is stored hot for six months, warm for three months, and cold for three years. Depending on the data, it may be indefinitely stored in cold storage.

A good log source may not include only one log. Due to the nature of a network, it may require multiple log types to create one quality log source

A log source could also be collecting too much information; that is, if several types of logs are collecting the same data or creating the same alerts, it can increase noise, storage complexity, and other consequences.

Correlation
using multiple log types and sources is beneficial for validating logs and creating a complete story of an incident. 


















Even if you are not directly in the blue team, this does not mean you will never have to deal with cyber incidents. 
if the system you are responsible for has an incident, you may be called upon by the blue team to assist.

The biggest mistake that is performed during incidents is shutting the host down. This is wrong for the following two main reasons:
  A significant amount of important evidence is found in volatile spaces, meaning it is lost as soon as the device loses power
  It immediately alerts the threat actor that we might be on to them, meaning they might start a more disruptive attack
  
For the latter, it means that as a first step, we should not even disable network access on the host, as this can have the same effect.
Instead, we want to make sure that evidence is preserved.
We also want to ensure that we preserve evidence in order of volatility.

volatility order with reasoning: https://datatracker.ietf.org/doc/html/rfc3227#section-2.1
 1. Registers and Cache
 2. Routing Table, ARP Cache, Process Table, Kernel Statistics and Memory
    Routes and ARP entries have a specific time-to-live, meaning if we are unable to capture this data in time, we might not have the full picture of what network communication took place at the time the incident occurred.
	we will need to capture evidence from the Random Access Memory (RAM).
 3. Temporary File Systems
	 While these files are often preserved longer on the host, we do not want to take any chances in losing these files that may be important for the investigation.
 4. Disk 
     The next step is to make sure that we take a snapshot of the host's drive. 
     While this evidence portion may not be as volatile as the others, it can play an important part in legal proceedings and should, therefore, be prioritised. 
 5. Remote Logging and Monitoring 
     all hosts should forward their logs to a secure remote location
 6. Physical Configuration and Network Topology
     The physical configuration of the host and network topology at the time of the incident is usually not volatile at all.
	 However, this evidence can usually assist us in our investigation.
 7. Archival Media
     Last on the list is backups. 
	 While this information will usually not be volatile, it can be used as evidence to help us determine how far back the incident went when comparing artefacts on the current disk to that found on backups.
	 
Don't trust the programs on the system. This means that you should not use the actual software on the host to perform your evidence collection. 
Don't run programs that modify the access times of files. When a file was accessed is evidence itself. 

In order for the evidence to be admissible in court, we have to be able to prove that it has not been tampered with.


The blue team is usually seasoned to deal with incidents and is ready to act. 
Like a fire brigade, they should be performing many exercises and know the drill when an incident occurs.
A playbook provides steps and actions that were predefined to help the team deal with incidents.
The goal of a playbook is to ensure that the process followed during an incident is repeatable and that no actions are forgotten. 
if credentials were compromised through phishing, the phishing playbook would indicate that the team would, at that point, also start using the account compromise playbook. 

Call trees indicate who has to be informed and who is responsible for informing them. 



Once we have raised the alarm bells, the next step is containment. 
While waiting for the fire brigade to arrive, we want to ensure the damage is kept as small as possible.

The incident management process speaks to performing containment, eradication, and recovery.

there are other means of isolation that can be performed:
 Network Segmentation - The host is isolated from the network perspective by being placed into a different network segment. 
 Physical Isolation - The host is collected and fully isolated from the network and users. 
 Virtual Isolation - The host is restricted from communicating through the use of software.
 
 
Some say that slow internet is worse than no internet, but this is a valid technique for blue teams
Instead of performing full isolation, the team can decide to rate limit the network speed, which can often be done through the EDR.
Doing this, the chance that the threat actor would suspect that we are onto them is less since everything is still working
Considering that threat actors have to use command and control channels, they would also be unable to pinpoint the exact problem causing the slow connection. 
 Slowing down the connection will allow the team to perform a more in-depth analysis of the actions being performed
 
 
Business Continuity Plan (BCP)
A BCP is a plan meant to help recover from an incident. 

A BCP is very similar to a Disaster Recovery Plan (DRP).
However, the DRP will mainly focus on the technical recovery of our division. 
A BCP is more encompassing and covers elements such as communication to internal and external stakeholders. 

As a security engineer, you may be responsible for creating a BCP. The following steps can be followed to create an effective BCP:
 Perform a Business Impact Analysis - You have to plan for the worst-case scenario.
 Define the Potential Recovery Actions - Based on the scenarios of the first step, you can determine what potential recovery actions would be possible.
 Plan the BCP Team Structure - When the BCP is invoked, there are certain responsibilities that have to be fulfilled, such as documenting actions and alerting stakeholders.
 Test the BCP Plan - In order to ensure that your BCP works as expected, you have to train your team on using it and then test it using a tabletop exercise.
 
As mentioned, the Business Impact Analysis assessment will use quantitative measures.
 Recovery Point Objective - The amount of data we are willing to accept can be lost.
 Recovery Time Objective - The amount of time required to recover the hardware of our system
 Work Recovery Time -The amount of time required to recover the software and data of our system
 Maximum Tolerable Downtime -The maximum amount of downtime that we are willing to accept.
 Mean Time Between Failures - How long our system will operate between incidents on average
 Mean Time To Repair - How long it will take to recover our system on average














Wireshark, a tool used for creating and analyzing PCAPs (network packet capture files), is commonly used as one of the best packet analysis tools. 

Some things to think about before going headfirst into attempting to collect and monitor live packet captures.
 Begin by starting with a sample capture to ensure that everything is correctly set up and you are successfully capturing traffic.
 Ensure that you have enough compute power to handle the number of packets based on the size of the network, this will obviously vary network by network.
 Ensure enough disk space to store all of the packet captures.
 
Network taps are a physical implant in which you physically tap between a cable, these techniques are commonly used by Threat Hunting/DFIR teams and red teams in an engagement to sniff and capture packets.

MAC Floods are a tactic commonly used by red teams as a way of actively sniffing packets. 
MAC Flooding is intended to stress the switch and fill the CAM table.
Once the CAM table is filled the switch will no longer accept new MAC addresses and so in order to keep the network alive, the switch will send out packets to all ports of the switch.

ARP Poisoning is another technique used by red teams to actively sniff packets.
By ARP Poisoning you can redirect the traffic from the host(s) to the machine you're monitoring from.
This technique will not stress network equipment like MAC Flooding however should still be used with caution and only if other techniques like network taps are unavailable.

Packet Filtering is a very important part of packet analysis especially when you have a very large number of packet sometimes even 100,000 plus.


HTTP is used to send GET and POST requests to a web server in order to receive things like webpages. 
Knowing how to analyze HTTP can be helpful to quickly spot things like SQLi, Web Shells, and other web-related attack vectors.
HTTP is one of the most straight forward protocols for packet analysis, the protocol is straight to the point and does not include any handshakes or prerequisites before communication.




HTTPS or Hypertext Transfer Protocol Secure can be one of the most annoying protocols to understand from a packet analysis perspective and can be confusing to understand the steps needed to take in order to analyze HTTPS packets.

Before sending encrypted information the client and server need to agree upon various steps in order to make a secure tunnel.
 Client and server agree on a protocol version
 Client and server select a cryptographic algorithm
 The client and server can authenticate to each other; this step is optional
 Creates a secure tunnel with a public key
















 Crisis Management Team (CMT)
 

 


When the CMT is invoked, the first hour is one of the most crucial. 
Similar to any investigation, as more time progresses, rebuilding what has happened and recovering from it becomes harder.
We refer to this as the Golden Hour. 

The first step in the Golden Hour is to assemble the CMT. 
Once the CMT has been established, the very first step is to understand what has happened and what actions should be taken immediately.
For a cyber crisis, this is usually done in the form of a CSIRT briefing where the CSIRT provides:
 A summary of the information discovered up to this point
 A summary of the actions that have already been taken by the team and the effect they had on the incident
 Recommendations as to what nuclear actions should be taken immediately by the CM
 

Once the CMT has been established and the Golden Hour actions have been performed, the CMT starts with a cyclic process to deal with the crisis,


the CMT usually consists of members that are not as technical. 
 
SME - Subject Matter Expert 
 
 












Wireshark, a tool used for creating and analyzing PCAPs (network packet capture files), is commonly used as one of the best packet analysis tools. 

Some things to think about before going headfirst into attempting to collect and monitor live packet captures.
 Begin by starting with a sample capture to ensure that everything is correctly set up and you are successfully capturing traffic.
 Ensure that you have enough compute power to handle the number of packets based on the size of the network, this will obviously vary network by network.
 Ensure enough disk space to store all of the packet captures.
 
Network taps are a physical implant in which you physically tap between a cable, these techniques are commonly used by Threat Hunting/DFIR teams and red teams in an engagement to sniff and capture packets.

MAC Floods are a tactic commonly used by red teams as a way of actively sniffing packets. 
MAC Flooding is intended to stress the switch and fill the CAM table.
Once the CAM table is filled the switch will no longer accept new MAC addresses and so in order to keep the network alive, the switch will send out packets to all ports of the switch.

ARP Poisoning is another technique used by red teams to actively sniff packets.
By ARP Poisoning you can redirect the traffic from the host(s) to the machine you're monitoring from.
This technique will not stress network equipment like MAC Flooding however should still be used with caution and only if other techniques like network taps are unavailable.

Packet Filtering is a very important part of packet analysis especially when you have a very large number of packet sometimes even 100,000 plus.

Wireshark's filter syntax can be simple to understand making it easy to get a hold of quickly. To get the most out of these filters you need to have a basic understanding of boolean and logic operators.
Wireshark only has a few that you will need to be familiar with:
 and - operator: and / &&
 or - operator: or / ||
 equals - operator: eq / ==
 not equal - operator: ne / !=
 greater than - operator: gt /  >
 less than - operator: lt / <
  Other operators: https://www.wireshark.org/docs/wsug_html_chunked/ChWorkBuildDisplayFilterSection.html
  
 
Wireshark Filtering: https://wiki.wireshark.org/DisplayFilters

(camadas OSI)

Packets consist of 5 to 7 layers based on the OSI model. 

(cores de pacotes)


ARP or Address Resolution Protocol is a Layer 2 protocol that is used to connect IP Addresses with MAC Addresses. 
They will contain REQUEST messages and RESPONSE messages.
To identify packets the message header will contain one of two operation codes:
 Request (1)
 Reply (2)
 
an example of suspicious traffic would be many requests from an unrecognized source.
You need to enable a setting within Wireshark however to resolve physical addresses. To enable this feature, navigate to View > Name Resolution > Ensure that Resolve Physical Addresses is checked.

ARP is one of the simpler protocols to analyze, all you need to remember is to identify whether it is a request or reply packet and who it is being sent by. 



ICMP or Internet Control Message Protocol is used to analyze various nodes on a network.
This is most commonly used with utilities like ping and traceroute. 
A type that equals 8 means that it is a request packet, if it is equal to 0 it is a reply packet. 
When these codes are altered or do not seem correct that is typically a sign of suspicious activity.

There are two other details within the packet that are useful to analyze: timestamp and data.
The timestamp can be useful for identifying the time the ping was requested it can also be useful to identify suspicious activity in some cases.




TCP or Transmission Control Protocol handles the delivery of packets including sequencing and errors.
TCP can give useful insight into a network when analyzing however it can also be hard to analyze due to the number of packets it sends. 
This is where you may need to use other tools like RSA NetWitness and NetworkMiner to filter out and further analyze the captures.

A common thing that you will see when analyzing TCP packets is known as the TCP handshake, which you should already be familiar with. 
It includes a series of packets: syn, synack, ack; That allows devices to establish a connection.

Within Wireshark, we can also see the original sequence number by navigating to edit > preferences > protocols > TCP > relative sequence numbers (uncheck boxes).




Nessus vulnerability scanner is exactly what you think is its! A vulnerability scanner!
It uses techniques similar to Nmap to find and report vulnerabilities, which are then, presented in a nice GUI for us to look at.
Nessus is different from other scanners as it doesn't make assumptions when scanning,

https://www.tenable.com/products/nessus










OpenVAS, an application used to scan endpoints and web applications to identify and detect vulnerabilities.
It is commonly used by corporations as part of their mitigation solutions to quickly identify any gaps in their production or even development servers or applications.


From the OpenVAS GitHub repository "This is the Open Vulnerability Assessment Scanner (OpenVAS) of the Greenbone Vulnerability Management (GVM) Solution.
It is used for the Greenbone Security Manager appliances and is a full-featured scan engine that executes a continuously updated and extended feed of Network Vulnerability Tests (NVTs)."


OpenVAS is a service within a larger framework of services known as Greenbone Vulnerability Management (GVM)













"Proof of Concept" emphasis, providing a breakdown of the vulnerable method within this issue.
 
MS-NRPC (Microsoft NetLogon Remote Protocol) 
MS-NRPC is a critical authentication component of Active Directory that handles authentication for User and Machine accounts.
In short -- the attack mainly focuses on a poor implementation of Cryptography.


Proof of Concepts are incredibly important to every exploit, without them, the exploit's are almost entirely theoretical.













Kerberos is the default authentication service for Microsoft Windows domains
It is intended to be more "secure" than NTLM by using third party ticket authorization as well as stronger encryption.
Even though NTLM has a lot more attack vectors to choose from Kerberos still has a handful of underlying vulnerabilities just like NTLM that we can use to our advantage.

Ticket Granting Ticket (TGT) - A ticket-granting ticket is an authentication ticket used to request service tickets from the TGS for specific resources from the domain.
Key Distribution Center (KDC) - The Key Distribution Center is a service for issuing TGTs and service tickets that consist of the Authentication Service and the Ticket Granting Service.
Authentication Service (AS) - The Authentication Service issues TGTs to be used by the TGS in the domain to request access to other machines and service tickets.
Ticket Granting Service (TGS) - The Ticket Granting Service takes the TGT and returns a ticket to a machine on the domain.
Service Principal Name (SPN) - A Service Principal Name is an identifier given to a service instance to associate a service instance with a domain service account. Windows requires that services have a domain service account which is why a service needs an SPN set.
KDC Long Term Secret Key (KDC LT Key) - The KDC key is based on the KRBTGT service account. It is used to encrypt the TGT and sign the PAC.
Client Long Term Secret Key (Client LT Key) - The client key is based on the computer or service account. It is used to check the encrypted timestamp and encrypt the session key.
Service Long Term Secret Key (Service LT Key) - The service key is based on the service account. It is used to encrypt the service portion of the service ticket and sign the PAC.
Session Key - Issued by the KDC when a TGT is issued. The user will provide the session key to the KDC along with the TGT when requesting a service ticket.
Privilege Attribute Certificate (PAC) - The PAC holds all of the user's relevant information, it is sent along with the TGT to the KDC to be signed by the Target LT Key and the KDC LT Key in order to validate the user.





The AS-REQ step in Kerberos authentication starts when a user requests a TGT from the KDC.
In order to validate the user and create a TGT for the user, the KDC must follow these exact steps.
 The first step is for the user to encrypt a timestamp NT hash and send it to the AS.
 The KDC attempts to decrypt the timestamp using the NT hash from the user, if successful the KDC will issue a TGT as well as a session key for the user.
 
the TGT is provided by the user to the KDC, in return, the KDC validates the TGT and returns a service ticket.

A service ticket contains two portions: the service provided portion and the user-provided portion. 
 Service Portion: User Details, Session Key, Encrypts the ticket with the service account NTLM hash.
 User Portion: Validity Timestamp, Session Key, Encrypts with the TGT session key.
 

AS-REQ - 1.) The client requests an Authentication Ticket or Ticket Granting Ticket (TGT).
AS-REP - 2.) The Key Distribution Center verifies the client and sends back an encrypted TGT.
TGS-REQ - 3.) The client sends the encrypted TGT to the Ticket Granting Server (TGS) with the Service Principal Name (SPN) of the service the client wants to access.
TGS-REP - 4.) The Key Distribution Center (KDC) verifies the TGT of the user and that the user has access to the service, then sends a valid session key for the service to the client.
AP-REQ - 5.) The client requests the service and sends the valid session key to prove the user has access.
AP-REP - 6.) The service grants access


The main ticket you will receive is a ticket-granting ticket (TGT). 
These can come in various forms, such as a .kirbi for Rubeus and .ccache for Impacket. A ticket is typically base64 encoded and can be used for multiple attacks. 

The ticket-granting ticket is only used to get service tickets from the KDC
When requesting a TGT from the KDC, the user will authenticate with their credentials to the KDC and request a ticket. 
The server will validate the credentials, create a TGT and encrypt it using the krbtgt key. 
The encrypted TGT and a session key will be sent to the user.

Kerbrute Enumeration - No domain access required 
Pass the Ticket - Access as a user to the domain required
Kerberoasting - Access as any user required
AS-REP Roasting - Access as any user required
Golden Ticket - Full domain compromise (domain admin) required 
Silver Ticket - Service hash required 
Skeleton Key - Full domain compromise (domain admin) required



﻿Kerbrute is a popular enumeration tool used to brute-force and enumerate valid active-directory users by abusing the Kerberos pre-authentication.

By brute-forcing Kerberos pre-authentication, you do not trigger the account failed to log on event which can throw up red flags to blue teams.
When brute-forcing through Kerberos you can brute-force by only sending a single UDP frame to the KDC allowing you to enumerate the users on the domain from a wordlist.




Rubeus is a powerful tool for attacking Kerberos. Rubeus is an adaptation of the kekeo tool and developed by HarmJ0y the very well known active directory guru.
Rubeus has a wide variety of attacks and features that allow it to be a very versatile tool for attacking Kerberos. 
Just some of the many tools and attacks include overpass the hash, ticket requests and renewals, ticket management, ticket extraction, harvesting, pass the ticket, AS-REP Roasting, and Kerberoasting.

Rubeus - https://github.com/GhostPack/Rubeus 



Kerberoasting allows a user to request a service ticket for any service with a registered SPN then use that ticket to crack the service password.




Very similar to Kerberoasting, AS-REP Roasting dumps the krbasrep5 hashes of user accounts that have Kerberos pre-authentication disabled.
Unlike Kerberoasting these users do not have to be service accounts the only requirement to be able to AS-REP roast a user is the user must have pre-authentication disabled.
There are other tools out as well for AS-REP Roasting such as kekeo and Impacket's GetNPUsers.py. 
Rubeus is easier to use because it automatically finds AS-REP Roastable users whereas with GetNPUsers you have to enumerate the users beforehand and know which users may be AS-REP Roastable.

Have a strong password policy. With a strong password, the hashes will take longer to crack making this attack less effective
Don't turn off Kerberos Pre-Authentication unless it's necessary there's almost no other way to completely mitigate this attack other than keeping Pre-Authentication on.

Mimikatz is a very popular and powerful post-exploitation tool most commonly used for dumping user credentials inside of an active directory network however we'll be using mimikatz in order to dump a TGT from LSASS memory


Pass the ticket works by dumping the TGT from the LSASS memory of the machine.

The Local Security Authority Subsystem Service (LSASS) is a memory process that stores credentials on an active directory server and can store Kerberos ticket along with other credential types to act as the gatekeeper and accept or reject the credentials provided.

Pass the Ticket Mitigation
Don't let your domain admins log onto anything except the domain controller - This is something so simple however a lot of domain admins still log onto low-level computers leaving tickets around that we can use to attack and move laterally with

The key difference between the two tickets is that a silver ticket is limited to the service that is targeted whereas a golden ticket has access to any Kerberos service.










Malware" consists of two words combined; malicious and software. 
Typically, Malware is designed to cause damage to Computers or Networks, this may be on a very large scale or only on a local network (LAN).




The Creeper Program, also known as the "Creeper worm" or "virus", was the first-ever virus to be created.
Written by Bob Thomas in 1971, the program used ARPANET (Advanced Research Projects Agency Network) to transfer itself between computers (keep reading to learn about ARPANET). 
Creeper was created in the programming language PDP-10 Assembly, which ran on the operating system TENEX.
 Bob Thomas came up with the idea from the unreliability of computers;
  "computers fail from time to time and work is lost. So I got interested in the possibility of moving an executing program from one computer to another without interrupting the ongoing operation of the program, at least to the extent that to an external observer nothing had happened."
  
Creeper was surprisingly named after a green ghoul on Scooby-doo!


ARPANET originally started out with two specific protocols; Remote login and transferring files. 



Reaper was created not too long after Creeper was released.
The creator was Ray Tomlinson which you may recognise as the same person to re-design creeper.


According to malware.wiki Reaper is called a nematode, which is a type of malware which removes other malware 
but Reaper was actually the first anti-virus software produced, the term "nematode" is not commonly used nor could I find any documentation for the term.

The Wabbit (Rabbit) virus was written in 1974. The name, which derived from Elmer Fudd's way of saying "Rabbit" in the looney tunes cartoons, 
was one of the first self-replicating malware.
The name also connotes to the fast pace in which the software would replicate itself, like that of a rabbit reproducing.
Wabbit would work so fast that the system would figuratively choke on its resources and end up crashing. 
Rabbit was one of the best versions of malware, not only for its ingenious idea but for its use in education.

Rabbit works by creating an infinite loop that continually creates system processes and copies of the original file, creating a high number of CPU cycles (the time for the execution of one process) which "constipates" the system and consumes operating system resources, 
causing it to get slower until its eventual crash. 




In 1975 the first Trojan was written.
ANIMAL, created by John Walker, would act as a game and ask the user a number of questions to guess the type of animal they were thinking of.


Richard Skrenta, a 15-year-old high school student known for his pranks and practical jokes, created one of the first microcomputer viruses that spread outside of a computer system or laboratory (also known as "in the wild").
The malware worked by attaching itself to the Apple II operating systems and spread via floppy disk.
The technique that Elk Cloner used is a technique now called "boot sector virus".
The program was placed into a game's code until an unsuspecting victim started the game for the 50th time. 


Released in 1988, Robert Tappan Morris created a worm that was supposed to highlight security flaws of the academic networks that it travelled to.
Morris was the first person to be arrested from a felony conviction in the US under the 1986 Computer Fraud and Abuse act.







Cascade was notably the first type of malware to use a form of encryption.







Not only is malware analysis a form of incidence response, but it is also useful in understanding how the behaviours of variants of malware result in their respective categorisation.
 When analysing malware, it is important to consider the following:
  Point of Entry (PoE) I.e. Was it through spam that our e-mail filtering missed and the user opened the attachment?
  What are the indicators that malware has even been executed on a machine? Are there any files, processes, or perhaps any attempt of "un-ordinary" communication?
  How does the malware perform? Does it attempt to infect other devices? Does it encrypt files or install anything like a backdoor / Remote Access Tool (RAT)?
  Most importantly - can we ultimately prevent and/or detect further infection?!
  
Despite the many variants of malware, attacks can generally be classified into two types: Targeted and Mass Campaign.
 A "Targeted" attack is just that - targeted. In most cases, malware attacks that occur this way are created for a specific purpose against a specific target. 
 On the other hand, the "Mass Campaign" classification can be akin to many real life examples, and is the most common type of attacks.
  The entire purpose of this type of Malware is to infect as many devices as possible and perform whatever it may - regardless of target.
  
Much like any interaction with an Operating System, there is always remnants of such activity even if there is little trace

The ultimate process of a malware attack can be broken down into a few broad steps:
 1. Delivery
 2. Execution
 3. Maintaining persistence (not always the case!)
 4. Persistence
 5. Propagation (not always!)
  These steps will generate lots of data. Namely: network traffic such as communicating with hosts, file system interaction like read/writes and modification.
  
  
In Summary, there are two categories of fingerprints that malware may leave behind on a Host after an attack:
 Host-Based Signatures
  These are generally speaking the results of execution and any persistence performed by the Malware.
  For example, has a file been encrypted? Has any additional software been installed?
  
 Network-Based Signatures
  At an overview, this classification of signatures are the observation of any networking communication taking place during delivery, execution and propagation
  
  
Static Analysis Tools:
 Dependency Walker (depends)
 PeID
 PE Explorer
 PEview
 ResourceHacker
 IDA Freeware
 WinDbg
 
files have identifying attributes within its hex - known as file headers.



Packing is one form of obfuscation that malware Authors employ to prevent the analysis of programmes. 
 
 
 
Disassemblers reverse the compiled code of a program from machine code to human-readable instructions (assembly)
"Debuggers" essentially facilitate execution of the program - where the analyser can view the changes made throughout each "step" of the program.













A Penetration test or pentest is an ethically-driven attempt to test and analyse the security defences to protect these assets and pieces of information. 
A penetration test involves using the same tools, techniques, and methodologies that someone with malicious intent would use and is similar to an audit.



Recall that a penetration test is an authorised audit of a computer system's security and defences as agreed by the owners of the systems.



The ROE is a document that is created at the initial stages of a penetration testing engagement.
 This document consists of three main sections 
  Permission
  Test Scope
  Rules
  
  
  
The steps a penetration tester takes during an engagement is known as the methodology. 
A practical methodology is a smart one, where the steps taken are relevant to the situation at hand. 
 Stages 
   Information Gathering
   Enumeration / Scanning
   Exploitation
   Privilege Escalation
   Post-Explitation 
   
Industry Standard Methodologies
 OSSTMM
 OWASP
 NIST Cybersecurity Framework 
 NCSC CAF
 
 
 
Black-Box Testing
 This testing process is a high-level process where the tester is not given any information about the inner workings of the application or service.
 Black-Box testing significantly increases the amount of time spent during the information gathering and enumeration phase to understand the attack surface of the target.
 
Grey-Box Testing
  It is a combination of both black-box and white-box testing processes.
  The tester will have some limited knowledge of the internal components of the application or piece of software.
  With Grey-Box testing, the limited knowledge given saves time, and is often chosen for extremely well-hardened attack surfaces
  
White-Box Testing
 The tester will be testing the internal components of the application or piece of software and, for example, ensuring that specific functions work correctly and within a reasonable amount of time.
 The tester will have full knowledge of the application and its expected behaviour and is much more time consuming than black-box testing.











Defence in Depth is the use of multiple varied layers of security to an organisation's systems and data in the hopes that multiple layers will provide redundancy in an organisation's security perimeter.


The CIA triad is an information security model that is used in consideration throughout creating a security policy.
This model has an extensive background, ranging from being used in 1998.

Consisting of three sections: Confidentiality, Integrity and Availability (CIA), this model has quickly become an industry standard today. 

It is vital to administrate and correctly define the various levels of access to an information technology system individuals require. 
The levels of access given to individuals are determined on two primary factors:
 The individual's role/function within the organisation
 The sensitivity of the information being stored on the system
 

The Bell-La Padula Model is used to achieve confidentiality.
The Bell LaPadula Model is popular within organisations such as governmental and military. 
 This is because members of the organisations are presumed to have already gone through a process called vetting. 
 Vetting is a screening process where applicant's backgrounds are examined to establish the risk they pose to the organisation.
  Therefore, applicants who are successfully vetted are assumed to be trustworthy - which is where this model fits in.
  
  
The Biba model is arguably the equivalent of the Bell-La Padula model but for the integrity of the CIA triad.
This model applies the rule to objects (data) and subjects (users) that can be summarised as "no write up, no read down".
This rule means that subjects can create or write content to objects at or below their level but can only read the contents of objects above the subject's level.


Threat modelling is the process of reviewing, improving, and testing the security protocols in place in an organisation's information technology infrastructure and services.
 A critical stage of the threat modelling process is identifying likely threats that an application or system may face, the vulnerabilities a system or application may be vulnerable to.
 
The threat modelling process is very similar to a risk assessment made in workplaces for employees and customers.
 The principles all return to:
  Preparation
  Identification
  Mitigations
  Review
  
It is, however, a complex process that needs constant review and discussion with a dedicated team.
An effective threat model includes:
  Threat intelligence
  Asset identification
  Mitigation capabilities
  Risk assessment
  
To help with this, there are frameworks such as STRIDE 
(Spoofing identity, Tampering with data, Repudiation threats, Information disclosure, Denial of Service and Elevation of privileges)  
 and PASTA (Process for Attack Simulation and Threat Analysis) 
 
A breach of security is known as an incident. 
Actions taken to resolve and remediate the threat are known as Incident Response (IR) and are a whole career path in cybersecurity.

An incident is responded to by a Computer Security Incident Response Team (CSIRT) 
which is prearranged group of employees with technical knowledge about the systems and/or current incident. 
To successfully solve an incident, these steps are often referred to as the six phases of Incident Response that takes place, listed in the table below:
 Preparation
 Identification 
 Containment 
 Eradication
 Recovery
 Lessons Learned 





If you are playing the role of an attacker, you need to gather information about your target systems.
 
If you are playing the role of a defender, you need to know what your adversary will discover about your systems and networks.

Reconnaissance (recon) can be defined as a preliminary survey to gather information about a target.
 Passive Reconnaissance - publicly available knowledge
  Looking up DNS records of a domain from a public DNS server.
  Checking job ads related to the target website.
  Reading news articles about the target company.
 
 Active Reconnaissance
  It requires direct engagement with the target. Think of it like you check the locks on the doors and windows, among other potential entry points.
   Connecting to one of the company servers such as HTTP, FTP, and SMTP.
   Calling the company in an attempt to get information (social engineering).
   Entering company premises pretending to be a repairman.
   


WHOIS is a request and response protocol that follows the RFC 3912 specification.
 The WHOIS server replies with various information related to the domain requested. 
 various information about the domain name we were looking up.
 
Find the IP address of a domain name using nslookup - Name Server Look Up
  A	         IPv4 Addresses
  AAAA	     IPv6 Addresses
  CNAME	     Canonical Name
  MX	     Mail Servers
  SOA	     Start of Authority
  TXT	     TXT Records
  
  
For more advanced DNS queries and additional functionality, you can use dig, the acronym for “Domain Information Groper,”




Lack of proper regular updates usually leads to vulnerable services.

one can use an online service that offers detailed answers to DNS queries, such as DNSDumpster.

Shodan.io tries to connect to every device reachable online to build a search engine of connected “things” in contrast with a search engine for web pages.









Active reconnaissance requires you to make some kind of contact with your target.
 This contact can be a phone call or a visit to the target company under some pretence to gather more information, usually as part of social engineering.
 Alternatively, it can be a direct connection to the target system, whether visiting their website or checking if their firewall has an SSH port open.
 
 It is possible to let your active reconnaissance appear as regular client activity.
 
Since 80 and 443 are default ports for HTTP and HTTPS, the web browser does not show them in the address bar. 
However, it is possible to use custom ports to access a service.

Developer Tools lets you inspect many things that your browser has received and exchanged with the remote server.
For instance, you can view and even modify the JavaScript (JS) files, inspect the cookies set on your system and discover the folder structure of the site content.

There are also plenty of add-ons for Firefox and Chrome that can help in penetration testing. Here are a few examples:
 FoxyProxy lets you quickly change the proxy server you are using to access the target website.
 User-Agent Switcher and Manager gives you the ability to pretend to be accessing the webpage from a different operating system or different web browser.
 Wappalyzer provides insights about the technologies used on the visited websites. Such extension is handy, primarily when you collect all this information while browsing the website like any other user. 
 
 
 
The primary purpose of ping is to check whether you can reach the remote system and that the remote system can reach you back.
 the ping command sends a packet to a remote system, and the remote system replies. 
 This way, you can conclude that the remote system is online and that the network is working between the two systems.
 
If you prefer a pickier definition, the ping is a command that sends an ICMP Echo packet to a remote system.
If the remote system is online, and the ping packet was correctly routed and not blocked by any firewall, the remote system should send back an ICMP Echo Reply.


Technically speaking, ping falls under the protocol ICMP (Internet Control Message Protocol)
ICMP supports many types of queries, but, in particular, we are interested in ping 


Generally speaking, when we don’t get a ping reply back, there are a few explanations that would explain why we didn’t get a ping reply, for example:
 The destination computer is not responsive; possibly still booting up or turned off, or the OS has crashed.
 It is unplugged from the network, or there is a faulty network device across the path.
 A firewall is configured to block such packets. The firewall might be a piece of software running on the system itself or a separate network appliance. Note that MSWindows firewall blocks ping by default.
 Your system is unplugged from the network.
 
 
 
As the name suggests, the traceroute command traces the route taken by the packets from your system to another host. 
The purpose of a traceroute is to find the IP addresses of the routers or hops that a packet traverses as it goes from your system to a target host.


The TELNET (Teletype Network) protocol was developed in 1969 to communicate with a remote system via a command-line interface (CLI).
From a security perspective, telnet sends all the data, including usernames and passwords, in cleartext.
Using telnet 10.10.205.91 PORT, you can connect to any service running on TCP



Netcat or simply nc has different applications that can be of great value to a pentester. 
Netcat supports both TCP and UDP protocols.

 













trying to port-scan offline systems will only waste time and create unnecessary noise on the network.
 
Nmap was created by Gordon Lyon (Fyodor), a network security expert and open source programmer. It was released in 1997
Nmap, short for Network Mapper, is free, open-source software released under GPL license. 
Nmap is an industry-standard tool for mapping networks, identifying live hosts, and discovering running services

A network segment is a group of computers connected using a shared medium
For instance, the medium can be the Ethernet switch or WiFi access point.
In an IP network, a subnetwork is usually the equivalent of one or more network segments connected together and configured to use the same router. 
The network segment refers to a physical connection, while a subnetwork refers to a logical connection.
A subnetwork, or simply a subnet, has its own IP address range and is connected to a more extensive network via a router.


Subnets with /16, which means that the subnet mask can be written as 255.255.0.0. This subnet can have around 65 thousand hosts. (255 x 255)
Subnets with /24, which indicates that the subnet mask can be expressed as 255.255.255.0. This subnet can have around 250 hosts.


If you are connected to the same subnet, you would expect your scanner to rely on ARP (Address Resolution Protocol) queries to discover live hosts. 
An ARP query aims to get the hardware address (MAC address) so that communication over the link-layer becomes possible
ARP packets are bound to their subnet.


ARP has one purpose: sending a frame to the broadcast address on the network segment and asking the computer with a specific IP address to respond by providing its MAC (hardware) address.


When a privileged user tries to scan targets on a local network (Ethernet), Nmap uses ARP requests. A privileged user is root or a user who belongs to sudoers and can run sudo.

When a privileged user tries to scan targets outside the local network, Nmap uses ICMP echo requests, TCP ACK (Acknowledge) to port 80, TCP SYN (Synchronize) to port 443, and ICMP timestamp request.

When an unprivileged user tries to scan targets outside the local network, Nmap resorts to a TCP 3-way handshake by sending SYN packets to ports 80 and 443.









a TCP port or UDP port is used to identify a network service running on that host.

At the risk of oversimplification, we can classify ports in two states:
 Open port indicates that there is some service listening on that port.
 Closed port indicates that there is no service listening on that port.
 
Nmap considers the following six states:

Open: indicates that a service is listening on the specified port.
Closed: indicates that no service is listening on the specified port, although the port is accessible. By accessible, we mean that it is reachable and is not blocked by a firewall or other security appliances/programs.

Filtered: means that Nmap cannot determine if the port is open or closed because the port is not accessible. This state is usually due to a firewall preventing Nmap from reaching that port. Nmap’s packets may be blocked from reaching the port; alternatively, the responses are blocked from reaching Nmap’s host.

Unfiltered: means that Nmap cannot determine if the port is open or closed, although the port is accessible. This state is encountered when using an ACK scan -sA.
Open|Filtered: This means that Nmap cannot determine whether the port is open or filtered.
Closed|Filtered: This means that Nmap cannot decide whether a port is closed or filtered.


TCP header flags are:
URG: Urgent flag indicates that the urgent pointer filed is significant. The urgent pointer indicates that the incoming data is urgent, and that a TCP segment with the URG flag set is processed immediately without consideration of having to wait on previously sent TCP segments.
ACK: Acknowledgement flag indicates that the acknowledgement number is significant. It is used to acknowledge the receipt of a TCP segment.
PSH: Push flag asking TCP to pass the data to the application promptly.
RST: Reset flag is used to reset the connection. Another device, such as a firewall, might send it to tear a TCP connection. This flag is also used when data is sent to a host and there is no service on the receiving end to answer.
SYN: Synchronize flag is used to initiate a TCP 3-way handshake and synchronize sequence numbers with the other host. The sequence number should be set randomly during TCP connection establishment.
FIN: The sender has no more data to send.










The Telnet protocol is an application layer protocol used to connect to a virtual terminal of another computer.
Using Telnet, a user can log into another computer and access its terminal (console) to run programs, start batch processes, and perform system administration tasks remotely.
Telnet is no longer considered a secure option, especially that anyone capturing your network traffic will be able to discover your usernames and passwords



Hypertext Transfer Protocol (HTTP) is the protocol used to transfer web pages.
Three popular choices for HTTP servers are:
 Apache
 Internet Information Services (IIS)
 nginx
 
 
 
It is worth noting that there are two modes for FTP:
 Active: In the active mode, the data is sent over a separate channel originating from the FTP server’s port 20.
 Passive: In the passive mode, the data is sent over a separate channel originating from an FTP client’s port above port number 1023. 
 
 
Email delivery over the Internet requires the following components:
 Mail Submission Agent (MSA)
 Mail Transfer Agent (MTA)
 Mail Delivery Agent (MDA)
 Mail User Agent (MUA) 
 
You (MUA) want to send postal mail.
The post office employee (MSA) checks the postal mail for any issues before your local post office (MTA) accepts it.
The local post office checks the mail destination and sends it to the post office (MTA) in the correct country.
The post office (MTA) delivers the mail to the recipient mailbox (MDA).
The recipient (MUA) regularly checks the mailbox for new mail. They notice the new mail, and they take it. 






















The null scan does not set any fla
A TCP packet with no flags set will not trigger any response when it reaches an open port, as shown in the figure below. 
a lack of reply in a null scan indicates that either the port is open or a firewall is blocking the packet.


The FIN scan sends a TCP packet with the FIN flag set.
no response will be sent if the TCP port is open. 
Nmap cannot be sure if the port is open or if a firewall is blocking the traffic related to this TCP port.
 However, the target system should respond with an RST if the port is closed. 
 Consequently, we will be able to know which ports are closed and use this knowledge to infer the ports that are open or filtered.
 It's worth noting some firewalls will 'silently' drop the traffic without sending an RST.
 
 
The Xmas scan gets its name after Christmas tree lights. 
An Xmas scan sets the FIN, PSH, and URG flags simultaneously. 
Like the Null scan and FIN scan, if an RST packet is received, it means that the port is closed. Otherwise, it will be reported as open|filtered.


Most target systems respond with an RST packet regardless of whether the TCP port is open. In such a case, we won’t be able to discover the open ports. 


As the name implies, an ACK scan will send a TCP packet with the ACK flag set. 


When you are on the same subnet as the target machine, you would be able to spoof your MAC address as well.
You can specify the source MAC address using --spoof-mac SPOOFED_MAC. This address spoofing is only possible if the attacker and the target machine are on the same Ethernet (802.3) network or same WiFi (802.11).



A firewall is a piece of software or hardware that permits packets to pass through or blocks them.
It functions based on firewall rules, summarized as blocking all traffic with exceptions or allowing all traffic with exceptions.
A traditional firewall inspects, at least, the IP header and the transport layer header. A more sophisticated firewall would also try to examine the data carried by the transport layer.

An intrusion detection system (IDS) inspects network packets for select behavioural patterns or specific content signatures.
It raises an alert whenever a malicious rule is met.



Spoofing the source IP address can be a great approach to scanning stealthily.
The idle scan, or zombie scan, requires an idle system connected to the network that you can communicate with. 
The idle (zombie) scan requires the following three steps to discover whether a port is open:
 Trigger the idle host to respond so that you can record the current IP ID on the idle host.
 Send a SYN packet to a TCP port on the target. The packet should be spoofed to appear as if it was coming from the idle host (zombie) IP address.
 Trigger the idle machine again to respond so that you can compare the new IP ID with the one received earlier.
 
 
You might consider adding --reason if you want Nmap to provide more details regarding its reasoning and conclusions.












An enormous part of penetration testing is knowing the skills and resources for whatever situation you face. 
A vulnerability in cybersecurity is defined as a weakness or flaw in the design, implementation or behaviours of a system or application.


we should know that there are arguably five main categories of vulnerabilities:

Operating System
These types of vulnerabilities are found within Operating Systems (OSs) and often result in privilege escalation.

(Mis)Configuration-based
These types of vulnerability stem from an incorrectly configured application or service. For example, a website exposing customer details.

Weak or Default Credentials
Applications and services that have an element of authentication will come with default credentials when installed. For example, an administrator dashboard may have the username and password of "admin". These are easy to guess by an attacker. 

Application Logic
These vulnerabilities are a result of poorly designed applications. For example, poorly implemented authentication mechanisms that may result in an attacker being able to impersonate a user.

Human-Factor
Human-Factor vulnerabilities are vulnerabilities that leverage human behaviour. For example, phishing emails are designed to trick humans into believing they are legitimate.



Vulnerability management is the process of evaluating, categorising and ultimately remediating threats (vulnerabilities) faced by an organisation.
It is arguably impossible to patch and remedy every single vulnerability in a network or computer system and sometimes a waste of resources.
After all, only approximately 2% of vulnerabilities only ever end up being exploited
Instead, it is all about addressing the most dangerous vulnerabilities and reducing the likelihood of an attack vector being used to exploit a system.

Vulnerability scoring serves a vital role in vulnerability management and is used to determine the potential risk and impact a vulnerability may have on a network or computer system.
For example, the popular Common Vulnerability Scoring System (CVSS) awards points to a vulnerability based upon its features, availability, and reproducibility.

As it stands, the current version is CVSSv3.1 (with version 4.0 currently in draft) a score is essentially determined by some of the following factors (but many more):

  1. How easy is it to exploit the vulnerability?

  2. Do exploits exist for this?

  3. How does this vulnerability interfere with the CIA triad?
  
Calculadora de riscos: https://nvd.nist.gov/vuln-metrics/cvss/v3-calculator

k. A vulnerability is given a classification (out of five) depending on the score that is has been assigned.

None	        0
Low	        0.1 - 3.9
Medium   	4.0 - 6.9
High	    7.0 - 8.9
Critical	9.0 - 10.0


Advantages of CVSS
CVSS has been around for a long time.	
CVSS is popular in organisations.	
CVSS is a free framework to adopt and recommended by organisations such as NIST.	

Disadvantages of CVSS
CVSS was never designed to help prioritise vulnerabilities, instead, just assign a value of severity.
CVSS heavily assesses vulnerabilities on an exploit being available. However, only 20% of all vulnerabilities have an exploit available (Tenable., 2020) .
Vulnerabilities rarely change scoring after assessment despite the fact that new developments such as exploits may be found.




VPR - Vulnerability Priority Rating 
The VPR framework is a much more modern framework in vulnerability management - developed by Tenable, an industry solutions provider for vulnerability management.

This framework is considered to be risk-driven; meaning that vulnerabilities are given a score with a heavy focus on the risk a vulnerability poses to the organisation itself, rather than factors such as impact (like with CVSS).

Unlike CVSS, VPR scoring takes into account the relevancy of a vulnerability. For example, no risk is considered regarding a vulnerability if that vulnerability does not apply to the organisation (i.e. they do not use the software that is vulnerable)

 VPR is also considerably dynamic in its scoring, where the risk that a vulnerability may pose can change almost daily as it ages.
 
 
 
VPR uses a similar scoring range as CVSS, which I have also put into the table below.  

However, two notable differences are that VPR does not have a "None/Informational" category, and because VPR uses a different scoring method
the same vulnerability will have a different score using VPR than when using CVSS.

Low	        0.0 - 3.9
Medium	    4.0 - 6.9
High	    7.0 - 8.9
Critical	9.0 - 10.0

Advantages of VPR
VPR is a modern framework that is real-world.	
VPR considers over 150 factors when calculating risk.	
VPR is risk-driven and used by organisations to help prioritise patching vulnerabilities.	
Scorings are not final and are very dynamic, meaning the priority a vulnerability should be given can change as the vulnerability ages.	

Disadvantages of VPR
VPR is not open-source like some other vulnerability management frameworks.
VPR can only be adopted apart of a commercial platform.
VPR does not consider the CIA triad to the extent that CVSS does; meaning that risk to the confidentiality, integrity and availability of data does not play a large factor in scoring vulnerabilities when using VPR.



CMS - Content Management System 
For example, a CMS whilst they all have the same purpose, often have very different designs and behaviours (and, in turn, potentially different vulnerabilities).

 Vulnerabilities Database
 https://nvd.nist.gov/vuln
 http://exploit-db.com/
 
Vulnerability	        A vulnerability is defined as a weakness or flaw in the design, implementation or behaviours of a system or application.
Exploit	                An exploit is something such as an action or behaviour that utilises a vulnerability on a system or application.
Proof of Concept (PoC)	A PoC is a technique or tool that often demonstrates the exploitation of a vulnerability. 

The National Vulnerability Database is a website that lists all publically categorised vulnerabilities.
In cybersecurity, vulnerabilities are classified under “Common Vulnerabilities and Exposures” (Or CVE for short).
 https://nvd.nist.gov/vuln

These CVEs have the formatting of CVE-YEAR-IDNUMBER


Exploit-DB is a resource that we, as hackers, will find much more helpful during an assessment.
http://exploit-db.com/
Exploit-DB retains exploits for software and applications stored under the name, author and version of the software or application.












the vulnerability scanner Nessus

Advantages and Disadvantages for using Vulnerability Scanner

Advantage
Automated scans are easy to repeat, and the results can be shared within a team with ease.	
These scanners are quick and can test numerous applications efficiently.	
Open-source solutions exist.	
Automated scanners cover a wide range of different vulnerabilities that may be hard to manually search for.	

Disadvantage
People can often become reliant on these tools.
They are extremely "loud" and produce a lot of traffic and logging. This is not good if you are trying to bypass firewalls and the likes.
Open-source solutions are often basic and require expensive licenses to have useful features.
They often do not find every vulnerability on an application.


Manual scanning for vulnerabilities is often the weapon of choice by a penetration tester when testing individual applications or programs.
In fact, manual scanning will involve searching for the same vulnerabilities and uses similar techniques as automated scanning.
Ultimately, both techniques involve testing an application or program for vulnerabilities. These vulnerabilities include:

Security Misconfigurations	
 Security misconfigurations involve vulnerabilities that are due to developer oversight. For example, exposing server information in messages between the application and an attacker.

Broken Access Control	
 This vulnerability occurs when an attacker is able to access parts of an application that they are not supposed to be able to otherwise.

Insecure Deserialization	
 This is the insecure processing of data that is sent across an application. An attacker may be able to pass malicious code to the application, where it will then be executed.

Injection
 An Injection vulnerability exists when an attacker is able to input malicious data into an application. This is due to the failure of not ensuring (known as sanitising) input is not harmful.



Much like other services such as Exploit DB and NVD, Rapid7 is a vulnerability research database.

Security researchers store & share PoC’s (Proof of Concept) on GitHub, turning it into an exploit database in this context.
GitHub is extremely useful in finding rare or fresh exploits because anyone can create an account and upload – there is no formal verification process like there is with alternative exploit databases.
GitHub uses a tagging and keyword system, meaning that we can search GitHub by keywords such as “PoC”, “vulnerability”, and many more. 


Searchsploit is a tool that is available on popular pentesting distributions such as Kali Linux.
This tool is an offline copy of Exploit-DB, containing copies of exploits on your system. 


one of the most effective vulnerabilities that we can exploit is the ability to execute commands on the target that is running the vulnerable application or service.
A foothold is an access to the vulnerable machine’s console, where we can then begin to exploit other applications or machines on the network.
it is important to note that exploits rarely come out of the box and are ready to be used. 
They often require some configuration before they will work for our environment or target.













At it's core, Privilege Escalation usually involves going from a lower permission account to a higher permission one.
More technically, it's the exploitation of a vulnerability, design flaw, or configuration oversight in an operating system
or application to gain unauthorized access to resources that are usually restricted from the users.

Privilege escalation is crucial because it lets you gain system administrator levels of access, which allows you to perform actions such as:
   Resetting passwords
   Bypassing access controls to compromise protected data
   Editing software configurations
   Enabling persistence
   Changing the privilege of existing (or new) users
   Execute any administrative command
   



Enumeration is the first step you have to take once you gain access to any system
You may have accessed the system by exploiting a critical vulnerability that resulted in root-level access or just found a way to send commands using a low privileged account.
As you will see, enumeration is as important during the post-compromise phase as it is before.

The hostname command will return the hostname of the target machine.

uname -a
Will print system information giving us additional detail about the kernel used by the system. This will be useful when searching for any potential kernel vulnerabilities that could lead to privilege escalation.

/proc/version
The proc filesystem (procfs) provides information about the target system processes.

Systems can also be identified by looking at the /etc/issue file. This file usually contains some information about the operating system but can easily be customized or changed.

The ps command is an effective way to see the running processes on a Linux system.

The env command will show environmental variables.

The target system may be configured to allow users to run some (or all) commands with root privileges.
The sudo -l command can be used to list all commands your user can run using sudo.

The id command will provide a general overview of the user’s privilege level and group memberships.

Reading the /etc/passwd file can be an easy way to discover users on the system.
Another approach could be to grep for “home” as real users will most likely have their folders under the “home” directory.


Following an initial check for existing interfaces and network routes, it is worth looking into existing communications.
The netstat command can be used with several different options to gather information on existing connections.

The built-in “find” command is useful and worth keeping in your arsenal.
   find . -name flag1.txt: find the file named “flag1.txt” in the current directory
   find /home -name flag1.txt: find the file names “flag1.txt” in the /home directory
   find / -type d -name config: find the directory named config under “/”
   find / -type f -perm 0777: find files with the 777 permissions (files readable, writable, and executable by all users)
   find / -perm a=x: find executable files
   find /home -user frank: find all files for user “frank” under “/home”
   find / -mtime 10: find files that were modified in the last 10 days
   find / -atime 10: find files that were accessed in the last 10 day
   find / -cmin -60: find files changed within the last hour (60 minutes)
   find / -amin -60: find files accesses within the last hour (60 minutes)
   find / -size 50M: find files with a 50 MB size
   
Please spend some time getting comfortable with commands such as find, locate, grep, cut, sort, etc.


Several tools can help you save time during the enumeration process.
These tools should only be used to save time knowing they may miss some privilege escalation vectors.
The target system’s environment will influence the tool you will be able to use.
 LinPeas: https://github.com/carlospolop/privilege-escalation-awesome-scripts-suite/tree/master/linPEAS
 LinEnum: https://github.com/rebootuser/LinEnum
 LES (Linux Exploit Suggester): https://github.com/mzet-/linux-exploit-suggester
 Linux Smart Enumeration: https://github.com/diego-treitos/linux-smart-enumeration
 Linux Priv Checker: https://github.com/linted/linuxprivchecker
 
 
Unless a single vulnerability leads to a root shell, the privilege escalation process will rely on misconfigurations and lax permissions.

The Kernel exploit methodology is simple;
 Identify the kernel version
 Search and find an exploit code for the kernel version of the target system
 Run the exploit 
 
Research sources:
  Based on your findings, you can use Google to search for an existing exploit code.
  Sources such as https://www.linuxkernelcves.com/cves can also be useful.
  Another alternative would be to use a script like LES (Linux Exploit Suggester) but remember that these tools can generate false positives (report a kernel vulnerability that does not affect the target system) or false negatives (not report any kernel vulnerabilities although the kernel is vulnerable).
  
https://github.com/mzet-/linux-exploit-suggester

The sudo command, by default, allows you to run a program with root privileges.
Any user can check its current situation related to root privileges using the sudo -l command.
https://gtfobins.github.io/ is a valuable source that provides information on how any program, on which you may have sudo rights, can be used

Nano is owned by root, which probably means that we can read and edit files at a higher privilege level than our current user has.  









Simply put, privilege escalation consists of using given access to a host with "user A" and leveraging it to gain access to "user B" by abusing a weakness in the target system. 

Gaining access to different accounts can be as simple as finding credentials in text files or spreadsheets left unsecured by some careless user
but that won't always be the case.
Depending on the situation, we might need to abuse some of the following weaknesses:
  Misconfigurations on Windows services or scheduled tasks
  Excessive privileges assigned to our account
  Vulnerable software
  Missing Windows security patches
  
you will usually hear about some special built-in accounts used by the operating system in the context of privilege escalation:  

 SYSTEM / LocalSystem
  An account used by the operating system to perform internal tasks. It has full access to all files and resources available on the host with even higher privileges than administrators.
  
 Local Service
  Default account used to run Windows services with "minimum" privileges. It will use anonymous connections over the network.
 
 Network Service
  Default account used to run Windows services with "minimum" privileges. It will use the computer credentials to authenticate through the network.


When installing Windows on a large number of hosts, administrators may use Windows Deployment Services, which allows for a single operating system image to be deployed to several hosts through the network.
These kinds of installations are referred to as unattended installations as they don't require user interaction.

Such installations require the use of an administrator account to perform the initial setup, which might end up being stored in the machine in the following locations:
  C:\Unattend.xml
  C:\Windows\Panther\Unattend.xml
  C:\Windows\Panther\Unattend\Unattend.xml
  C:\Windows\system32\sysprep.inf
  C:\Windows\system32\sysprep\sysprep.xml
  
Whenever a user runs a command using Powershell, it gets stored into a file that keeps a memory of past commands.
This is useful for repeating commands you have used before quickly.
If a user runs a command that includes a password directly as part of the Powershell command line
%userprofile%\AppData\Roaming\Microsoft\Windows\PowerShell\PSReadline\ConsoleHost_history.txt (CMD)

Windows allows us to use other users' credentials. This function also gives the option to save these credentials on the system.
The command below will list saved credentials:
 cmdkey /list
 
 
Internet Information Services (IIS) is the default web server on Windows installations.  
The configuration of websites on IIS is stored in a file called web.config and can store passwords for databases or configured authentication mechanisms
Depending on the installed version of IIS, we can find web.config in one of the following locations:
  C:\inetpub\wwwroot\web.config
  C:\Windows\Microsoft.NET\Framework64\v4.0.30319\Config\web.config
  
  
While PuTTY won't allow users to store their SSH password, it will store proxy configurations that include cleartext authentication credentials.  
To retrieve the stored proxy credentials, you can search under the following registry key for ProxyPassword with the following command:
 reg query HKEY_CURRENT_USER\Software\SimonTatham\PuTTY\Sessions\ /f "Proxy" /s
Simon Tatham is the creator of PuTTY (and his name is part of the path), not the username for which we are retrieving the password

Just as putty stores credentials, any software that stores passwords, including browsers, email clients, FTP clients, SSH clients, VNC software and others, will have methods to recover any passwords the user has saved.


Some misconfigurations can allow you to obtain higher privileged user access and, in some cases, even administrator access.


Looking into scheduled tasks on the target system, you may see a scheduled task that either lost its binary or it's using a binary you can modify.
Scheduled tasks can be listed from the command line using the schtasks command without any options.


Windows services are managed by the Service Control Manager (SCM). 
The SCM is a process in charge of managing the state of services as needed, checking the current status of any given service and generally providing a way to configure services.


Services have a Discretionary Access Control List (DACL), which indicates who has permission to start, stop, pause, query status, query configuration, or reconfigure the service, amongst other privileges. 

The DACL can be seen from Process Hacker

All of the services configurations are stored on the registry under HKLM\SYSTEM\CurrentControlSet\Services\:


If the executable associated with a service has weak permissions that allow an attacker to modify or replace it, the attacker can gain the privileges of the service's account trivially.



Should you be interested in learning about additional techniques, the following resources are available:

PayloadsAllTheThings - Windows Privilege Escalation
Priv2Admin - Abusing Windows Privileges
RogueWinRM Exploit
Potatoes
Decoder's Blog
Token Kidnapping
Hacktricks - Windows Local Privilege Escalation






Just as we started programming and developing software, we were looking for ways to automate some of the tasks.
Today, automation is heavily ingrained in the Software Development Life Cycle (SDLC) and DevOps processes.
While this is incredibly good for production, allowing for faster development and deployment, it does, however, introduce new security risks.

source code and version control. This is the start of our pipeline. 
We need a location where we can store our code. 
Furthermore, we often want to keep several versions of our code since we are continuously making improvements and adding features.
We need to consider several things when deciding where to store our code:
  How can we perform access control for our source code?
  How can we make sure that changes made are tracked?
  Can we integrate our source code storage system with our development tools?
  Can we store and actively use multiple different versions of our source code?
  Should we host our source code internally, or can we use an external third party to host our code?
  

We need version control for two main reasons:

  We are often integrating new features in our software. Modern development approaches, such as Agile, means we are constantly updating our code. 
    To keep all of these updates in check, we need version control.
  An entire development team is working on the code, not just one developer. To ensure that we can integrate the changes from multiple developers, version control is required.
  

Version control allows us to keep multiple versions of the code.
The two most common source code storage and version control systems are Git and SubVersion (SVN)
  Git is a distributed source control tool, meaning that each contributor will have their own copy of the source code.
  On the other hand, SVN is a centralised source control tool, meaning the control of the repo is managed centrally.
  
Git is a distributed source control tool, meaning that each contributor will have their own copy of the source code. 
  On the other hand, SVN is a centralised source control tool, meaning the control of the repo is managed centrally.
  

GitHub is by far the largest provider of Internet hosting for software development and version control using Git.
you could also host your own git server using software such as Gitlab.

For SVN, the two most popular tools are TortoiseSVN and Apache SVN.

However, it should be noted that source code storage solutions such as Gitlab provide much more features than simple storage and version control. 
Today, these tools can be used for almost the entire pipeline


Our source code is often our secret sauce. As such, we want to make sure it is not exposed.
This is why authentication and access control for our source code is so important.  
We also want to make sure that changes and updates are adequately tracked, allowing us to always go back to a previous version if something happens.

We need to make sure not to store secrets, such as database connection strings and credentials, in our source code.
Since we keep all versions of our source code, even if we remove the secrets in a newer version, they will still be exposed in the previous versions.

GittyLeaks, which would scan through the commits for sensitive information.
Even if this information no longer exists in the current version, these tools can scan through all previous versions and uncover these secrets.


a lot of the code has already been written for us in the form of libraries and software development kits (SDKs). 
The management of these dependencies is a vital part of the pipeline.


External dependencies are publicly available libraries and SDKs.  
 These are hosted on external dependency managers such as PyPi for Python, NuGet for .NET, and Gems for Ruby libraries.
 
Internal dependencies are libraries and SDKs that an organisation develops and maintains internally.



There are different security concerns for internal and external dependencies:

Internal 
 Libraries can often become legacy software since they no longer receive updates or the original developer has left the company.
 The security of the package manager is our responsibility for internal libraries.
 A vulnerability in an internal library could affect several of our applications since it is used in all of them.


External 
 Since we do not have full control over the dependency, we must perform due diligence to ensure that the library is secure.
 If a package manager or content distribution network (CDN) is compromised, it could lead to a supply chain attack.
 External libraries can be researched by attackers to discover 0day vulnerabilities. 
  If such a vulnerability is found, it could lead to the compromise of several organisations at the same time.



A dependency manager, also called a package manager, is required to manage libraries and SDKs.
the primary security concern is that dependencies are code outside our control.
Especially in modern times, where so many different dependencies are used, it is incredibly hard to track dependencies.
If there are any vulnerabilities in these dependencies, it could lead to vulnerabilities in our application.



In the old days, testing was quite a tedious and manual process. 
A tester would have to manually run and document every test case and hope that the coverage was sufficient to ensure that the application or service works and will remain stable. 
However, in modern pipelines, automated testing can do a significant portion of this.

A unit test is a test case for a small part of the application or service.
The idea is to test the application in smaller parts to ensure that all the functionality works as it should.

 Test cases can be integrated into the Continuous Integration and Continuous Deployment (CI/CD) part of the pipeline,
 where the build will be stopped from progressing if these test cases fail. 
  However, unit testing is usually focused on functionality and not security.
  
  
Unit Testing
When talking about automated testing in a pipeline, this will be the first type of testing that most developers and software engineers are familiar with. 
A unit test is a test case for a small part of the application or service.
The idea is to test the application in smaller parts to ensure that all the functionality works as it should.
In modern pipelines, unit testing can be used as quality gates.
Test cases can be integrated into the Continuous Integration and Continuous Deployment (CI/CD) part of the pipeline, where the build will be stopped from progressing if these test cases fail. 
unit testing is usually focused on functionality and not security.


Integration Testing
integration testing focuses on how these small parts work together.
Similar to unit tests, testing will be performed for each of the integrations and can also be integrated into the CI/CD part of the pipeline.
A subset of integration testing is regression testing, which aims to ensure that new features do not adversely impact existing features and functionality. 



Security Testing

Static Application Security Testing (SAST)
  works by reviewing the source code of the application or service to identify sources of vulnerabilities. 
  SAST tools can be used to scan the source code for vulnerabilities.
  This can be integrated into the development process to already highlight potential issues to developers as they are writing code.
  We can also integrate this into the CI/CD process.
  Not as quality gates, but as security gates, preventing the pipeline from continuing if the SAST tool still detects vulnerabilities that have not been flagged as false positives.
 
 
Dynamic Application Security Testing (DAST)
  performs dynamic testing by executing the code.
  allows DAST tools to detect additional vulnerabilities that would not be possible with just a source code review.
  DAST tools can be integrated into the CI/CD pipeline as security gates.
  
  
SAST and DAST tools cannot fully replace manual testing, such as penetration tests.  
There have been significant advancements in automated testing and even in some cases, 
these techniques were combined with more modern approaches to create new testing techniques such as  
 Interactive Application Security Testing (IAST)
 Runtime Application Self-Protection (RASP).
 
 
GitHub and Gitlab have built-in SAST tooling. Tools such as Snyk and Sonarqube are also popular for SAST and DAST.


In modern pipelines, software isn't manually moved between different environments. 
an automated process can be followed to compile, build, integrate, and deploy new software features. This process is called CI/CD.
Continuous Integration and Continuous Delivery.

Since we are constantly building new features for our system or service, we need to ensure that these features will work with the current application.
Instead of waiting until the end of the development cycle when all features will be integrated, we can now continuously integrate new features and test them as they are being developed.

We can create what is called a CI/CD pipeline. 
These pipelines usually have the following distinct elements:
 Starting Trigger - The action that kicks off the pipeline process. For example, a push request is made to a specific branch.
 Building Actions - Actions taken to build both the project and the new feature.
 Testing Actions - Actions that will test the project to ensure that the new feature does not interfere with any of the current features of the application.
 Deployment Actions - Should a pipeline succeed, the deployment actions detail what should happen with the build. For example, it should then be pushed to the Testing Environment.
 Delivery Actions - As CI/CD processes have evolved, the focus is now no longer just on the deployment itself, but all aspects of the delivery of the solution. This includes actions such as monitoring the deployed solution.
 
 
CI/CD pipelines require build-infrastructure to execute the actions of these elements.  
We usually refer to this infrastructure as build orchestrators and agents.
A build orchestrator directs the various agents to perform the actions of the CI/CD pipelines as required.

These CI/CD pipelines are usually where the largest portion of automation can be found.
As such, this is usually the largest attack surface and the biggest chance for misconfigurations to creep in.

GitHub and Gitlab provide CI/CD pipeline capabilities and are quite popular to use.
GitHub provides build agents, whereas Gitlab provides a Gitlab runner application that can be installed on a host to make it a build agent.
For more complex builds, build orchestrator software such as Jenkins can be used.


Most pipelines have several environments. 
Each of these environments has a specific use case, and their security posture often differs. 

(imagem ambientes)

There are some other environments that you may hear about when talking about DevOps.

Green and Blue environments are used for a Blue/Green deployment strategy when pushing an update to PROD
Instead of having a single PROD instance, there are two.
The Blue environment is running the current application version, and the Green environment is running the newer version.
Using a proxy or a router, all traffic can then be switched to the Green environment when the team is ready.
However, the Blue environment is kept for some time, meaning that if there are any unforeseen issues with the new version, traffic can just be routed to the Blue environment again. 
 is faster than having to perform a roll-back of the actual PROD environment.
 


the goal of Canary environments is to smooth the PROD deployment process.
Again two environments are created, and users are gradually moved to the new environment.
For example, at the start, 10% of users can be migrated.
If the new environment remains stable, another 10% can be migrated until 100% of the users are in the new environment.


Instead of environments simply being computers, we can now have virtual computers created through tools such as Vagrant or Terraform. 
We could also move away from hosts entirely to things like containers using Docker or pods using Kubernetes.
These tools can make use of processes such as Infrastructure as Code (IaC) to even create software that can create and manage these environments.



the security considerations become more important the closer the environment is to PROD.
The underlying infrastructure of an application also forms part of the attack surface of the actual application.
Any vulnerabilities in this infrastructure could allow an attacker to take control of the host and the application. 
As such, the infrastructure must be hardened against attacks. 
 Removing unnecessary services
 Updating the host and applications
 Using a firewall to block unused ports











One of the critical tools in source code management is version control, which allows teams to collaborate, track changes, and maintain a history of their codebase.


in 2002, Linux kernel development introduced and used a Distributed Version Control System (DVCS) called BitKeep
in 2005 when Larry McVoy, BitKeeper's creator, developed strained relations with the Linux community, it resulted in the Linux community losing access to BitKeeper, causing significant disruptions in development.
Linus Torvalds, the creator of Linux, decided to create a new DVCS to replace BitKeeper. - He started working on Git in 2005 and released the first version of Git in April 2005
He designed the system so developers could work on a project from anywhere and easily merge changes made by multiple contributors.

Since its release, Git has become one of the world's most popular version control systems, used by millions of developers across various industries and projects.

Git offers the following benefits:
 Since it is open-source, you can contribute anytime without paying for it.
 Its performance is better than any other version control software since it focuses solely on the contents of files rather than their names.
 Through hashing, Git protects the code and changes history.
 Many organizations are using Git as their primary version control system.
 
Source code security is critical to software development, ensuring that code remains secure and protected from unauthorized access, tampering, or vulnerabilities.
One essential tool for managing source code security is version control, which allows teams to keep track of changes made to code over time, collaborate effectively, and maintain a history of edits and revisions. 

Version control uses a repository; consider it a database of changes and a working copy where developers make edits.
The working copy (sometimes called a checkout) is a personal copy of the project files, where developers can make arbitrary changes without affecting their teammates.
When satisfied with their improvements, they commit their changes to the repository. 

Version control can be centralized or distributed.
In centralized version control, there is a single repository, and updates are immediately visible to others upon committing.
In contrast, in distributed version control, each user has their repository and must push changes to a central repository for others to see.

A distributed version control system like Git or Mercurial offers faster performance, increased error resistance, and advanced capabilities.
Because they are more complex than centralized version control systems like Subversion, they can be tricky to learn.



Cloud-based version control is a modern approach to managing source code in software development, where the codebase and version history are stored in the cloud. 
It allows developers to collaborate on code from anywhere, providing a flexible and scalable solution for distributed teams.
Cloud-based version control platforms offer easy access, real-time collaboration, robust version history management, and seamless integration with other development tools. 
Overall, it provides a scalable, collaborative, and efficient solution for managing source code in modern software development workflows.


Github is cloud-based, and it helps developers store and manage source code from everywhere in the world. 
It is the oldest of the services.
Being founded in 2007 and developed using Ruby on Rails 

Continuous Integration and Continuous Deployment (CI/CD) is a method to introduce automation into the stages of app development or the Software Development Lifecycle. 
It introduces ongoing automation and continuous monitoring from the integration phases to delivery and deployment.
All these together are what you would call a CI/CD pipeline. 
Github launched Actions in 2018


GitLab is an open-core company that provides a DevOps software package that combines the ability to develop, secure, and operate software in a single application.
Founded in 2014, GitLab was created by Ukrainian developer Dmitriy Zaporozhets and Dutch developer Sytse Sijbrandij. 
GitLab eliminates the need for third-party integration as it includes CI/CD tools by default, making it a convenient all-in-one solution for software development.

GitLab’s CI/CD functionality allows developers to automate their software delivery pipeline, from building and testing to deploying and monitoring.
GitLab uses “runners” - agents that execute jobs - to enable continuous integration and continuous deployment.
With GitLab’s CI/CD features, developers can create custom workflows, define stages, and specify jobs to be executed in parallel or sequentially.


GitLab also emphasizes reliability by providing features such as a built-in container registry, built-in continuous deployment to Kubernetes, and GitLab Pages for hosting static websites.
Additionally, GitLab allows for creating multiple stable branches beyond the main branch, providing better version control and release management.


Maintaining good credential hygiene in CI/CD environments is crucial to protect against potential security risks. 
Various systems and individuals within the engineering ecosystem use credentials, such as secrets and tokens, to deploy and access resources and other highly privileged actions. 


Recommendations:
 Ensure that credentials follow the principle of least privilege from code to deployment.
 Avoid sharing the same credentials across multiple contexts to maintain accountability and simplify privilege management.
 Use temporary credentials whenever possible, and establish procedures to rotate static credentials and detect stale credentials periodically.
 Ensure credentials are only used under predefined conditions, such as limiting usage to a specific IP address or identity.
 Detect secrets pushed to and stored in code repositories using Integrated Development Environments (IDE) plugins, automatic scanning, and periodic repository commit scans.
 Use built-in vendor options or third-party tools to prevent secrets from being printed to console outputs during builds and ensure existing outputs do not contain secrets.
 Verify that secrets are removed from artifacts, such as container image layers and binaries.
 
You can manage environment variables securely by following these best practices:
 Avoid hardcoding sensitive information in code; use environment variables instead.
 Regularly review and rotate credentials stored in environment variables.
 Limit access to environment variables to authorized personnel only.
 Environment variables should be set according to the principle of least privilege.
 Implement monitoring and auditing mechanisms to track changes to environment variables.
 If implementing a secrets manager solution (explained in the next task), review its encryption mechanisms and if it's a good fit for your development environments.
 



To fetch a project from GitLab, Git uses a command called clone. 

Branching is a common feature in most Version Control Systems (VCS) that allows you to diverge from the main line of development and work on changes without affecting the main line.

